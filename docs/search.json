[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Pierzchajlo",
    "section": "",
    "text": "MSc Neuroscience\nCurrently a PhD Candidate in Psychology at Stockholm University\n \n  \n   \n  \n    \n     E-mail\n  \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Stephen Pierzchajlo",
    "section": "About",
    "text": "About\nI am an experimental psychologist and cognitive neuroscientist with a passion for data analysis and statistics.\nLearn more"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Stephen Pierzchajlo",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nJournal Of Neuroscience: Bayesian Reanalysis\n\n\n\n\n\n\n\n\n\n\n\n\n\nJournal Of Neuroscience: Bayesian Reanalysis\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Stephen Pierzchajlo",
    "section": "Blog posts",
    "text": "Blog posts\n\n\n\n\n\nWhy You Should Pre-specify Exploratory Analyses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMore posts"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Stephen Pierzchajlo",
    "section": "CV",
    "text": "CV\nWrite Same Thing About Myself As I"
  },
  {
    "objectID": "index.html#learn-more",
    "href": "index.html#learn-more",
    "title": "Stephen Pierzchajlo",
    "section": "Learn more",
    "text": "Learn more"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "The data I am analysing comes from a paper I recently published in the Journal Of Neuroscience (https://www.jneurosci.org/content/44/22/e1232232024). The paper involves 3 experiments, of which I am only analysing data from the first. I have chosen this study because I actually wanted to do a Bayesian analysis originally, and wanted to do a random effects model as well. We opted for a Frequentist analysis and a 3-way ANOVA. Here I will perform the same analyses using a Bayesian random-effects model.\n\n\nPeople are notoriously bad at identifying odors. Herrick (1993) noted that the olfactory cortex of most species is large, relative to other areas, and it is only in humans that the olfactory epithelium becomes dwarfed by the neocortex. This is likely do to evolution favoring cognitive capabilities, along with senses more important to our lives, like vision. Herrick hypothesized that, because the human olfactory cortex still contains dense interconnectivity with other sensory modalities, that maybe olfaction relies on other senses to help with olfactory identification and localization. Additionally, a recent study found that when people describe different types of sensory stimuli, all but olfaction are described abstractly (i.e. bright). Olfactory stimuli alone Are described as object-based (i.e. lemon).\n\n\n\nAfter removing some participants for poor performance, this dataset includes 63 participants (40 female; age range: 18-65; mean age: 32 years) total.\n\n\n\nWe developed a behavioural task where people listened to a predictive cue, were presented with a target, and then had to determine whether the cue and target matched or did not match. The cue was delivered by a voice, and the target was either visual or olfactory. The cue could also be object-based (i.e. lemon) or category-based (i.e. fruit). The targets consisted of 4 stimuli: lavender, lilac, lemon, pear. These could be presented both visually and in an olfactory manner. We hypothesized that, if olfaction requires information from another sensory modality, that their would be a large disparity in reaction time between matching (congruent) and non-matching (incongruent) cues and targets. We further hypothesized that this disparity would not be seen when the targets were visual. We also hypothesized that, in olfaction, this disparity would be larger when the cues were object-based, rather than category based.\n\n\n\nTheir are several predictions that are important for this assignment. First, we predicted much slower responses to olfactory targets. We also predicted participants would be slower at responding when cues and targets did not match. We further predicted that this slowness in responding to cues and targets that were incongruent would be larger when the targets were olfactory. Finally, we predicted that, in olfaction, people would respond more slowly to incongruent cues/targets whe nthe cues were object-based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#short-introduction",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#short-introduction",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "People are notoriously bad at identifying odors. Herrick (1993) noted that the olfactory cortex of most species is large, relative to other areas, and it is only in humans that the olfactory epithelium becomes dwarfed by the neocortex. This is likely do to evolution favoring cognitive capabilities, along with senses more important to our lives, like vision. Herrick hypothesized that, because the human olfactory cortex still contains dense interconnectivity with other sensory modalities, that maybe olfaction relies on other senses to help with olfactory identification and localization. Additionally, a recent study found that when people describe different types of sensory stimuli, all but olfaction are described abstractly (i.e. bright). Olfactory stimuli alone Are described as object-based (i.e. lemon)."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#participant-information",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#participant-information",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "After removing some participants for poor performance, this dataset includes 63 participants (40 female; age range: 18-65; mean age: 32 years) total."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#hypotheses",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#hypotheses",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "We developed a behavioural task where people listened to a predictive cue, were presented with a target, and then had to determine whether the cue and target matched or did not match. The cue was delivered by a voice, and the target was either visual or olfactory. The cue could also be object-based (i.e. lemon) or category-based (i.e. fruit). The targets consisted of 4 stimuli: lavender, lilac, lemon, pear. These could be presented both visually and in an olfactory manner. We hypothesized that, if olfaction requires information from another sensory modality, that their would be a large disparity in reaction time between matching (congruent) and non-matching (incongruent) cues and targets. We further hypothesized that this disparity would not be seen when the targets were visual. We also hypothesized that, in olfaction, this disparity would be larger when the cues were object-based, rather than category based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#predictions",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#predictions",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "Their are several predictions that are important for this assignment. First, we predicted much slower responses to olfactory targets. We also predicted participants would be slower at responding when cues and targets did not match. We further predicted that this slowness in responding to cues and targets that were incongruent would be larger when the targets were olfactory. Finally, we predicted that, in olfaction, people would respond more slowly to incongruent cues/targets whe nthe cues were object-based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#load-global-environment",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#load-global-environment",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Global Environment",
    "text": "Load Global Environment\n\n# load variables that are saved outside R\nload(\"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/workspace/assignmentworkspace.RData\")"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#load-libraries",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#load-libraries",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n# load libraries\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidybayes) \nlibrary(ggpubr)\nlibrary(gridExtra)"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#set-ggplot-theme-and-color-scheme",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#set-ggplot-theme-and-color-scheme",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Set ggplot Theme And Color Scheme",
    "text": "Set ggplot Theme And Color Scheme\n\n# set ggplot theme\ntheme_set(theme_minimal())\n# set ggplot colors\ncolors &lt;- c(\"#d1e1ec\", \"#b3cde0\", \"#6497b1\", \"#005b96\", \"#03396c\", \"#011f4b\")"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#example-dataframe",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#example-dataframe",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Example Dataframe",
    "text": "Example Dataframe\nBelow is an example for how one participants’ data might look. These data are fake, but provide an overview for how the data are organized.\n\n# Fill dataframe with fake values for one participant\nfaketable &lt;- data.frame(\n  row = c(1, 2, 3, 4, \"...\", 44, 45, 46, 47, \"...\", 82, 83, 84, 85, \"...\", 127, 128, 129, 130),\n  subject = c(rep(1, times = 4), \"...\", rep(1, times = 4), \"...\", rep(1, times = 4), \"...\", rep(1, times = 4)),\n  modality = c(rep(\"visual\", times = 4), \"...\", rep(\"visual\", times = 4), \"...\", rep(\"olfactory\", times = 4), \"...\", rep(\"olfactory\", times = 4)),\n  congruency = c(rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\",rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\", rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\",rep(c(\"congruent\", \"incongruent\"), times = 2)),\n  cue_type = c(rep(\"object\", time = 4), \"...\", rep(\"category\", times = 4), \"...\", rep(\"object\", time = 4), \"...\", rep(\"category\", times = 4)),\n  auditory_cue = c(\"lavender\", \"lilac\", \"lemon\", \"pear\", \"...\", \"flower\", \"flower\", \"fruit\", \"fruit\", \"...\", \"lavender\", \"lilac\", \"lemon\", \"pear\", \"...\", \"flower\", \"flower\", \"fruit\", \"fruit\"),\n  target = c(\"lavender\", \"lavender\", \"lemon\", \"lemon\", \"...\", \"lavender\", \"pear\", \"lemon\", \"lilac\", \"...\", \"lavender\", \"lavender\", \"lemon\", \"lemon\", \"...\", \"lavender\", \"pear\", \"lemon\", \"lilac\"),\n  reaction_time = c(564, 740, 602, 557, \"...\", 471, 649, 668, 519, \"...\", 1121, 1576, 1844, 1343, \"...\", 1876, 1265, 1721, 1846)\n  )\n# display fake dataframe with nice kable styling\nfaketable %&gt;%\n  kbl(caption = \"Example dataframe\") %&gt;%\n  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\nExample dataframe\n\n\nrow\nsubject\nmodality\ncongruency\ncue_type\nauditory_cue\ntarget\nreaction_time\n\n\n\n\n1\n1\nvisual\ncongruent\nobject\nlavender\nlavender\n564\n\n\n2\n1\nvisual\nincongruent\nobject\nlilac\nlavender\n740\n\n\n3\n1\nvisual\ncongruent\nobject\nlemon\nlemon\n602\n\n\n4\n1\nvisual\nincongruent\nobject\npear\nlemon\n557\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44\n1\nvisual\ncongruent\ncategory\nflower\nlavender\n471\n\n\n45\n1\nvisual\nincongruent\ncategory\nflower\npear\n649\n\n\n46\n1\nvisual\ncongruent\ncategory\nfruit\nlemon\n668\n\n\n47\n1\nvisual\nincongruent\ncategory\nfruit\nlilac\n519\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n82\n1\nolfactory\ncongruent\nobject\nlavender\nlavender\n1121\n\n\n83\n1\nolfactory\nincongruent\nobject\nlilac\nlavender\n1576\n\n\n84\n1\nolfactory\ncongruent\nobject\nlemon\nlemon\n1844\n\n\n85\n1\nolfactory\nincongruent\nobject\npear\nlemon\n1343\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127\n1\nolfactory\ncongruent\ncategory\nflower\nlavender\n1876\n\n\n128\n1\nolfactory\nincongruent\ncategory\nflower\npear\n1265\n\n\n129\n1\nolfactory\ncongruent\ncategory\nfruit\nlemon\n1721\n\n\n130\n1\nolfactory\nincongruent\ncategory\nfruit\nlilac\n1846"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#load-data",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#load-data",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Data",
    "text": "Load Data\nThe data I am using is a single .csv file from our previous experiment. These data were pre-processed in a separate R script that took each individual participants’ data and combined them into a single dataframe. The data are not aggregated, so they still represent the participants’ raw data. However, 5 participants were removed for having very low accuracy in the task. Additionally, we were only interested in trials where participants’ correctly determined whether the cue and target matched/did not match. Thus, these data only represent “correct” trials. Additionally, all participants’ in this dataset had at least 80% accuracy across the study.\n\n# load data\ndf &lt;- read.csv(file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/data/object_category.csv\", header = TRUE)\n# glimpse first few rows\nhead(df)\n\n  participant modality  congruency cue_type      cue   target reaction_time\n1         101   visual incongruent   object     Pear Lavender     1933.3277\n2         101   visual   congruent   object     Pear     Pear      400.3761\n3         101   visual   congruent   object    Lemon    Lemon      500.3686\n4         101   visual incongruent   object    Lilac     Pear      833.7295\n5         101   visual   congruent   object    Lilac    Lilac      533.7733\n6         101   visual   congruent   object Lavender Lavender      883.7940\n\n\nSome columns need to be changed to factors for the analysis.\n\n# Columns to convert to factors\ncols_to_factor &lt;- c(\"participant\", \"modality\", \"congruency\", \"cue_type\", \"cue\", \"target\")\n# Convert specified columns to factors\ndf[cols_to_factor] &lt;- lapply(df[cols_to_factor], as.factor)\n\nFirst, I wanted to plot the distribution of the dependent variable (reaction time) for both visual and olfactory target trials. This is because reaction time is often skewed. If the distributions are indeed skewed, I may want to model them using an Exgaussian distribution.\n\n# Add mean lines\nggplot(df, aes(x=reaction_time, fill = modality)) +\n  geom_histogram(bins = 100)+\n  theme(legend.position=\"top\") +\n  scale_fill_manual(values = c(colors[2], colors[5]))\n\n\n\n\n\n\n\n\nThey definitely appear skewed. Thus, I will take that into account with my model. However, I want to try both an Exgaussian and Gaussian model. I will then see how well each model recovers the distribution of the dependent variable using a Posterior Predictive Check (PPC)."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#gaussian-model",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#gaussian-model",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Gaussian model",
    "text": "Gaussian model\n\nPrior Predictive Check.\nI want to use informative priors for my model as I have already conducted several other similar studies and have several ideas about how reaction times will differ between different conditions. My original plan was to conduct a Prior Predictive Check to see how my informative priors would generate reaction time data. However, this proved too difficult and time consuming. The Prior Predictive Check gave estimates of some pararmeters in the millions, while also having very low effective sample sizes (ess). Some of the ess values were as low as 20, even with 4 chains running for 10,000 iterations. I think the model is simply to complex for the number of iterations I would need to run for stable estimates. Therefore, I will be going ahead without a Prior Predictive Check.\n\n\nInformative Priors\nI decided to use informative priors as I have run several studies looking at modality and congruency regarding vision and olfaction. Those have been outlined in the Notation section above. I did not put informative priors on the interaction terms however, because it is very hard to predict exactly how they will turn out.\n\n\nFitting Gaussian Model in brm\nFirst, I estimated the 3-way factorial model using a Gaussian distribution.\n\n# run Gaussian model in brm\ngaussian_model &lt;- brm(\n  reaction_time ~ modality*congruency*cue_type + (target|participant),\n  data = df,\n  family = gaussian(),\n  prior = c(prior(normal(1200, 200), class = Intercept),\n            prior(normal(500, 100), class = b, coef = \"modalityvisual\"),\n            prior(normal(200, 50), class = b, coef = \"congruencyincongruent\"),\n            prior(normal(100, 50), class = b, coef = \"cue_typeobject\"),\n            prior(cauchy(0, 2), class = sd),\n            prior(cauchy(0, 2), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter  = 6000,\n  warmup = 2000,\n  file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/models/gaussian_model\"\n  )"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#exgaussian-model",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#exgaussian-model",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Exgaussian Model",
    "text": "Exgaussian Model\nNext, I run an Exgaussian model. Everything is identical to the Gaussian model otherwise.\n\n# run Exgaussian model in brm\nexgaussian_model &lt;- brm(\n  reaction_time ~ modality*congruency*cue_type + (target|participant),\n  data = df,\n  family = exgaussian(),\n  prior = c(prior(normal(1200, 200), class = Intercept),\n            prior(normal(500, 100), class = b, coef = \"modalityvisual\"),\n            prior(normal(200, 50), class = b, coef = \"congruencyincongruent\"),\n            prior(normal(100, 50), class = b, coef = \"cue_typeobject\"),\n            prior(cauchy(0, 2), class = sd),\n            prior(cauchy(0, 2), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter  = 6000,\n  warmup = 2000, \n  file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/models/exgaussian_model\",\n  )\n\nUsing informative priors with the Exgaussian model led to sever errors, with R-hat values over 3 and ess values as low as 5. Not shown here, I ran another Exgaussian model where I used the default priors brm gives you. This model did well, so I will use it from here."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#model-comparison",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#model-comparison",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Model Comparison",
    "text": "Model Comparison\nMy plan was to first compare these models using the loo function. However, due to their complexity, I would need to redo both models and save the parameters as the models go. I have done this before and know it would take at least twice as long to run (sometimes longer). Given that the Exgaussian model took more than a day to complete, I just don’t have time. Instead I will let the PPC below guide my model selection."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#mcmc-traceplots",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#mcmc-traceplots",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "MCMC Traceplots",
    "text": "MCMC Traceplots\n\nmcmc_trace(gaussian_model, pars = c(\"b_Intercept\", \"b_modalityvisual\", \"b_congruencyincongruent\", \"b_cue_typeobject\", \"b_modalityvisual:congruencyincongruent\", \"b_modalityvisual:cue_typeobject\", \"b_congruencyincongruent:cue_typeobject\", \"b_modalityvisual:congruencyincongruent:cue_typeobject\", \"sigma\"), \n           facet_args = list(ncol = 2, strip.position = \"left\"))\n\n\n\n\n\n\n\n\nAll chains had Rhat values of 1, so I know they converged. This just gives a bit more visual evidence of that."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#fixed-effects",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#fixed-effects",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nHere I am taking all draws from all markov chains for all parameters and plotting a histogram of them. I will also take the model estimates and credible intervals and plot those under the histogram.\n\n# make posterior draws dataframe\ndraws_posterior &lt;- gaussian_model |&gt;\n  # select coefficients\n  spread_draws(b_Intercept, b_modalityvisual, b_congruencyincongruent, b_cue_typeobject, `b_modalityvisual:congruencyincongruent`, `b_modalityvisual:congruencyincongruent:cue_typeobject`) |&gt;\n  # new column called \"posterior\"\n  mutate(distribution = \"posterior\")\n\n\nMain Effects\nBelow I plot the posterior distributions for the main fixed effects.\n\n# graph of modality\nmodality_main_graph &lt;- ggplot(draws_posterior, aes(x = b_modalityvisual)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[2, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[2, 3], y = 0, xend = fixef(gaussian_model)[2, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Modality\") +\n  labs(x = \"Vision - Olfaction\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# graph of congruency\ncongruency_main_graph &lt;- ggplot(draws_posterior, aes(x = b_congruencyincongruent)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[3, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[3, 3], y = 0, xend = fixef(gaussian_model)[3, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Congruency\") +\n  labs(x = \"Incongruent - Congruent\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# graph of cue-type\ncuetype_main_graph &lt;- ggplot(draws_posterior, aes(x = b_cue_typeobject)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[4, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[4, 3], y = 0, xend = fixef(gaussian_model)[4, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Cue Type\") +\n  labs(x = \"Object-Cue - Category-Cue\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# arrange graphs beside one another\nggarrange(modality_main_graph, congruency_main_graph, cuetype_main_graph, ncol = 3)\n\n\n\n\n\n\n\n\nFirst, visual trials are 1176ms faster than olfactory trials. Because olfactory trials are the intercept (1764ms), visual trials are estimated to be approximately 588ms. This is a very big difference, but not surprising as it takes longer to process an odor compared to a visual stimulus. Next, incongruent trials are 109ms slower than congruent trials. So people tend to respond more slowly when the auditory cue and target do not match. Finally, people are 118ms faster at responding to targets when the cue is object-based rather than category-based.\nCentral to our main hypothesis, we were interested in whether the congruency effect (people responding slower when the cue and target did not match) was larger for olfactory target trials compared to visual target trials.\n\n# graph of modality x congruency posterior\ncongruency_modality_graph &lt;- ggplot(draws_posterior, aes(x = `b_modalityvisual:congruencyincongruent`)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[5, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[5, 3], y = 0, xend = fixef(gaussian_model)[5, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Interaction: Modality x Congruency\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\ncongruency_modality_cuetype_graph &lt;- ggplot(draws_posterior, aes(x = `b_modalityvisual:congruencyincongruent:cue_typeobject`)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[8, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[8, 3], y = 0, xend = fixef(gaussian_model)[8, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Interaction: Modality x Congruency x Cue-Type\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# arrange graphs beside one another\nggarrange(congruency_modality_graph, congruency_modality_cuetype_graph, ncol = 2)\n\n\n\n\n\n\n\n\nThis can be interpreted like this: people respond more slowly to incongruent cue target presentations, but this difference is greater when the target is olfactory compared to when it is visual. We could also say they are 53ms slower at responding to incongruent olfactory targets compared to incongruent visual targets. In addition, the 95% credible intervals do not overlap with zero (-92.02, -15.15). All of the 95% most probable reaction times for this interaction are negative, so I think we can say this difference is likely negative too. For comparison, this interaction was statistically significant in the original paper (p = 0.02). I actually think a p-value that close to the threshold is not amazingly convincing, so it kind of matches the interpretation of the credible interval here. The plot on the right represents the 3-way interaction. This lets us see whether the disparity in congruent and incongruent reaction times, which is larger in olfaction, occurs when object-cues preceed the olfactory target. The model suggests this is not the case.\nThere are other interactions as well, but they were not central to our main hypotheses so I won’t go into them here. In addition, I’m not a fan of 3-way interactions as they are hard to interpret and usually very underpowered to detect, so I don’t see much value in looking at the 3-way interaction here."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#modality-random-effects",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.knit.html#modality-random-effects",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Modality random effects",
    "text": "Modality random effects\nBelow is some pre-processing for displaying main random-effects estimates.\n\n# temporary dataframe to be filled in loop\ntemp_df &lt;- data.frame(\"participant\" = factor(),\n                      \"empirical_olfactory_rt\" = numeric(),\n                      \"estimated_olfactory_rt\" = numeric(),\n                      \"empicial_visual_rt\" = numeric(),\n                      \"estimated_visual_rt\" = numeric())\n# for each unique participant:\nfor (i in unique(df$participant)) { \n  # ith participants' reaction time random effect for olfaction\n  olfaction_rt_i &lt;- fixef(gaussian_model)[1]  + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4]\n  # ith participants' reaction time random effect for vision\n  visual_rt_i &lt;- fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]) + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4]\n  # dataframe of fixed and random modality effect estimates for ith person\n  participant_df &lt;- data.frame(\"participant\" = i,\n                      \"empirical_olfactory_rt\" = fixef(gaussian_model)[1],\n                      \"estimated_olfactory_rt\" = olfaction_rt_i,\n                      \"empicial_visual_rt\" = fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]),\n                      \"estimated_visual_rt\" = fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]) + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4])\n  # here all participant fixed and random modality effect estimates are combined in a single dataframe\n  temp_df &lt;- rbind(temp_df, participant_df)\n}\n\n\n# reshape from wide to long\nmodality_df &lt;- temp_df %&gt;% pivot_longer(cols = c('empirical_olfactory_rt', 'estimated_olfactory_rt', 'empicial_visual_rt', 'estimated_visual_rt'),\n                    names_to = 'coefficient',\n                    values_to = 'rt')\n# create new column\nmodality_df$group &lt;- factor(ifelse(modality_df$coefficient == \"empirical_olfactory_rt\", \"empirical\",\n                            ifelse(modality_df$coefficient == \"empicial_visual_rt\", \"empirical\", \"estimated\")))\n# create new column\nmodality_df$group2 &lt;- factor(ifelse(modality_df$coefficient == \"empirical_olfactory_rt\", \"olfactory\",\n                            ifelse(modality_df$coefficient == \"estimated_olfactory_rt\", \"olfactory\", \"visual\")))\n# rename columns\ncolnames(modality_df) &lt;- c(\"participant\", \"coefficient\", \"rt\", \"estimate\", \"modality\")\n\nmodality_df_estimates &lt;- modality_df[modality_df$estimate == \"estimated\", ]\nmodality_df_estimates &lt;- aggregate(modality_df$rt, list(modality_df$participant, modality_df$modality), mean)\ncolnames(modality_df_estimates) &lt;- c(\"participant\", \"modality\", \"rt\")\n\n# Calculate mean rt for each modality\nmean_values &lt;- modality_df_estimates %&gt;%\n  group_by(modality) %&gt;%\n  summarize(mean_rt = mean(rt))\n\nmean_values &lt;- data.frame(\"participant\" = c(101, 101),\n                          \"modality\" = c(\"olfactory\", \"visual\"),\n                          \"rt\" = c(1837, 612))\nmean_values$participant &lt;- as.factor(mean_values$participant)\nmean_values$modality &lt;- as.factor(mean_values$modality)\n\n\n# graph random effects for modality\nggplot(modality_df_estimates, aes(x = modality, y = rt, group = participant)) +\n  geom_point(color = \"grey\") +\n  geom_line(color = \"grey\")  +\n  geom_point(data = mean_values, aes(x = modality, y = rt), color = \"black\", size = 3) +\n  geom_line(data = mean_values, aes(x = modality, y = rt, group = 1), color = \"black\", size = 1.5) +\n  ggtitle(\"Modality Random Effects\")\n\n\n\n\n\n\n\n\nAn alternate way to view this is to put each participants’ random effect estimate on a plot with the fixed effect estimate, and plot each of these on individual panels. Maybe not as intuitive as the previous plot, but it still makes sense to look at it like this.\n\n# empty list where each runner's graph will be saved.\ntemp_list &lt;- list()  \n# loop through runners, grab their specific intercepts/slope from previous dataframe, and save graph to list\nfor (i in unique(modality_df$participant)) {\n  # runner-specific graph\n  temp_graph &lt;- ggplot(modality_df[modality_df$participant == i, ], aes(x = modality, y = rt, color = estimate, group = estimate)) +\n  geom_point() +\n  geom_line() +\n    ylim(0, 2800) +\n    ggtitle(paste0(\"Subject \", i)) +\n    theme(legend.position = \"none\")\n  # save ith runner graph to temp_list\n  temp_list[[i]] &lt;- temp_graph\n}\n\narg_list &lt;- c(temp_list, list(nrow = 13, ncol = 5))\n# Use do.call to pass all elements of graph_list to ggarrange\narranged_plots &lt;- do.call(\"ggarrange\", arg_list)\n# Print or display the arranged plots\nprint(arranged_plots)"
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/posts/1-eyeball-test/1-eyeball-test.html",
    "href": "content/posts/1-eyeball-test/1-eyeball-test.html",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "",
    "text": "# load libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(lsr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "content/posts/1-eyeball-test/1-eyeball-test.html#how-does-a-simulation-work",
    "href": "content/posts/1-eyeball-test/1-eyeball-test.html#how-does-a-simulation-work",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "How Does a Simulation Work?",
    "text": "How Does a Simulation Work?\nThis simulation will assume the null hypthesis is true. Therefore, there won’t be any difference between the groups at the population level. There may, however, be difference at the sample level. In fact, there almost always will be. Because I prefer linear regression, I will simulate data from a linear model:\n\\(Y_{i} \\mid \\beta_{0}, \\beta_{1}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0} + \\beta_{1}X_{i}\\)\nHowever, because the slope will always be cancelled out by zero, this model can be simplified to this:\n\\(Y_{i} \\mid \\beta_{0}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0}\\)\nFirst, I make an indicator for the independent variable. One group will have a zero, and the other a one. This also means the sample size is going to be n = 100.\n\nx &lt;- rep(c(0, 1), each = 50)\nx\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nTo make the dependent variable, one would need to add and multiply the model coefficients with the independent variable(s). That is what I will do here. No measurement is perfect though, so every sample will be measured with some error. I am simulating this error with the rnorm() function. Because there is no difference at the population level between the two groups, both groups are sampled from a normal distribution with a mean of 350 and a standard deviation of 30. To calculate the difference between groups, one would add a slope to the intercept. Here, the slope is zero and cancels out the influence of the independent variable.\n\nset.seed(1234)\ny &lt;- rnorm(100, mean = 350, sd = 30) + 0*x\ny\n\n  [1] 313.7880 358.3229 382.5332 279.6291 362.8737 365.1817 332.7578 333.6010\n  [9] 333.0664 323.2989 335.6842 320.0484 326.7124 351.9338 378.7848 346.6914\n [17] 334.6697 322.6641 324.8848 422.4751 354.0226 335.2794 336.7836 363.7877\n [25] 329.1884 306.5539 367.2427 319.2903 349.5459 321.9215 383.0689 335.7322\n [33] 328.7168 334.9623 301.1272 314.9714 284.5988 309.7702 341.1712 336.0231\n [41] 393.4849 317.9407 324.3391 341.5813 320.1698 320.9446 316.7805 312.4404\n [49] 334.2852 335.0945 295.8191 332.5377 316.7333 319.5511 345.1307 366.8917\n [57] 399.4345 326.7994 398.1773 315.2657 369.6977 426.4697 348.9572 329.9110\n [65] 349.7719 403.3125 315.8418 391.0348 389.8869 360.0942 350.2068 336.3359\n [73] 339.0043 369.4486 412.1081 345.3980 308.2790 328.2925 357.7479 340.4882\n [81] 344.6663 344.9002 308.8309 344.7864 375.5070 370.9283 366.4999 337.9180\n [89] 344.2522 314.1642 348.4052 357.6559 401.1789 380.0454 335.1325 360.6665\n [97] 315.9618 376.3461 379.1875 413.6335\n\n\nNow the data can be put in a dataframe\n\nsim_df &lt;- data.frame(x, y)\nhead(sim_df)\n\n  x        y\n1 0 313.7880\n2 0 358.3229\n3 0 382.5332\n4 0 279.6291\n5 0 362.8737\n6 0 365.1817\n\n\nFinally, a linear regression can be performed on the sample. Remember, their is no difference between the two groups at the pupulation level, but there almost certainlt will be differences at the sample level (due to sampling error).\n\nmodel &lt;-  lm(y ~ x, data = sim_df)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = sim_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.367 -17.301  -3.815  15.830  86.067 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  336.408      4.090  82.242  &lt; 2e-16 ***\nx             17.777      5.785   3.073  0.00274 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.92 on 98 degrees of freedom\nMultiple R-squared:  0.0879,    Adjusted R-squared:  0.07859 \nF-statistic: 9.444 on 1 and 98 DF,  p-value: 0.002743\n\n\nAbove I can see the slope coefficient (x) is actually statistically significant. This is likely due to the dact that the Intercept estimate is so low. Both groups were sampled from the same distribution centered at 350. THe Intercept is 336, and the slope is 18. This means the mean of the second group is 336 + 18 = 354. In frequentist statistics, probabilities are conceptualized in reference to long-term frequencies. For instance, we know a fair coin has a 50% probability of coming up heads because if we flipped that coin a theoretically infinite number of times, approximately 50% of those flips would land heads. P-values can be thought of the same way: If you were to run the same experiment an infinite number of times, fixing the sample size and collecting a new sample with each experiment, how many p-values would be below your threshold (in most cases in science, the threshold is 0.05)? When the null hypothesis is true, only 5% of experiments will yield a p-value below 0.05."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "The data I am analysing comes from a paper I recently published in the Journal Of Neuroscience (https://www.jneurosci.org/content/44/22/e1232232024). The paper involves 3 experiments, of which I am only analysing data from the first. I have chosen this study because I actually wanted to do a Bayesian analysis originally, and wanted to do a random effects model as well. We opted for a Frequentist analysis and a 3-way ANOVA. Here I will perform the same analyses using a Bayesian random-effects model.\n\n\nPeople are notoriously bad at identifying odors. Herrick (1993) noted that the olfactory cortex of most species is large, relative to other areas, and it is only in humans that the olfactory epithelium becomes dwarfed by the neocortex. This is likely do to evolution favoring cognitive capabilities, along with senses more important to our lives, like vision. Herrick hypothesized that, because the human olfactory cortex still contains dense interconnectivity with other sensory modalities, that maybe olfaction relies on other senses to help with olfactory identification and localization. Additionally, a recent study found that when people describe different types of sensory stimuli, all but olfaction are described abstractly (i.e. bright). Olfactory stimuli alone Are described as object-based (i.e. lemon).\n\n\n\nAfter removing some participants for poor performance, this dataset includes 63 participants (40 female; age range: 18-65; mean age: 32 years) total.\n\n\n\nWe developed a behavioural task where people listened to a predictive cue, were presented with a target, and then had to determine whether the cue and target matched or did not match. The cue was delivered by a voice, and the target was either visual or olfactory. The cue could also be object-based (i.e. lemon) or category-based (i.e. fruit). The targets consisted of 4 stimuli: lavender, lilac, lemon, pear. These could be presented both visually and in an olfactory manner. We hypothesized that, if olfaction requires information from another sensory modality, that their would be a large disparity in reaction time between matching (congruent) and non-matching (incongruent) cues and targets. We further hypothesized that this disparity would not be seen when the targets were visual. We also hypothesized that, in olfaction, this disparity would be larger when the cues were object-based, rather than category based.\n\n\n\nTheir are several predictions that are important for this assignment. First, we predicted much slower responses to olfactory targets. We also predicted participants would be slower at responding when cues and targets did not match. We further predicted that this slowness in responding to cues and targets that were incongruent would be larger when the targets were olfactory. Finally, we predicted that, in olfaction, people would respond more slowly to incongruent cues/targets whe nthe cues were object-based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#short-introduction",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#short-introduction",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "People are notoriously bad at identifying odors. Herrick (1993) noted that the olfactory cortex of most species is large, relative to other areas, and it is only in humans that the olfactory epithelium becomes dwarfed by the neocortex. This is likely do to evolution favoring cognitive capabilities, along with senses more important to our lives, like vision. Herrick hypothesized that, because the human olfactory cortex still contains dense interconnectivity with other sensory modalities, that maybe olfaction relies on other senses to help with olfactory identification and localization. Additionally, a recent study found that when people describe different types of sensory stimuli, all but olfaction are described abstractly (i.e. bright). Olfactory stimuli alone Are described as object-based (i.e. lemon)."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#participant-information",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#participant-information",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "After removing some participants for poor performance, this dataset includes 63 participants (40 female; age range: 18-65; mean age: 32 years) total."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#hypotheses",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#hypotheses",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "We developed a behavioural task where people listened to a predictive cue, were presented with a target, and then had to determine whether the cue and target matched or did not match. The cue was delivered by a voice, and the target was either visual or olfactory. The cue could also be object-based (i.e. lemon) or category-based (i.e. fruit). The targets consisted of 4 stimuli: lavender, lilac, lemon, pear. These could be presented both visually and in an olfactory manner. We hypothesized that, if olfaction requires information from another sensory modality, that their would be a large disparity in reaction time between matching (congruent) and non-matching (incongruent) cues and targets. We further hypothesized that this disparity would not be seen when the targets were visual. We also hypothesized that, in olfaction, this disparity would be larger when the cues were object-based, rather than category based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#predictions",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#predictions",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "Their are several predictions that are important for this assignment. First, we predicted much slower responses to olfactory targets. We also predicted participants would be slower at responding when cues and targets did not match. We further predicted that this slowness in responding to cues and targets that were incongruent would be larger when the targets were olfactory. Finally, we predicted that, in olfaction, people would respond more slowly to incongruent cues/targets whe nthe cues were object-based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-global-environment",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-global-environment",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Global Environment",
    "text": "Load Global Environment\n\n# load variables that are saved outside R\nload(\"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/workspace/assignmentworkspace.RData\")"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-libraries",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-libraries",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n# load libraries\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidybayes) \nlibrary(ggpubr)\nlibrary(gridExtra)"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#set-ggplot-theme-and-color-scheme",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#set-ggplot-theme-and-color-scheme",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Set ggplot Theme And Color Scheme",
    "text": "Set ggplot Theme And Color Scheme\n\n# set ggplot theme\ntheme_set(theme_minimal())\n# set ggplot colors\ncolors &lt;- c(\"#d1e1ec\", \"#b3cde0\", \"#6497b1\", \"#005b96\", \"#03396c\", \"#011f4b\")"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#example-dataframe",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#example-dataframe",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Example Dataframe",
    "text": "Example Dataframe\nBelow is an example for how one participants’ data might look. These data are fake, but provide an overview for how the data are organized.\n\n# Fill dataframe with fake values for one participant\nfaketable &lt;- data.frame(\n  row = c(1, 2, 3, 4, \"...\", 44, 45, 46, 47, \"...\", 82, 83, 84, 85, \"...\", 127, 128, 129, 130),\n  subject = c(rep(1, times = 4), \"...\", rep(1, times = 4), \"...\", rep(1, times = 4), \"...\", rep(1, times = 4)),\n  modality = c(rep(\"visual\", times = 4), \"...\", rep(\"visual\", times = 4), \"...\", rep(\"olfactory\", times = 4), \"...\", rep(\"olfactory\", times = 4)),\n  congruency = c(rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\",rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\", rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\",rep(c(\"congruent\", \"incongruent\"), times = 2)),\n  cue_type = c(rep(\"object\", time = 4), \"...\", rep(\"category\", times = 4), \"...\", rep(\"object\", time = 4), \"...\", rep(\"category\", times = 4)),\n  auditory_cue = c(\"lavender\", \"lilac\", \"lemon\", \"pear\", \"...\", \"flower\", \"flower\", \"fruit\", \"fruit\", \"...\", \"lavender\", \"lilac\", \"lemon\", \"pear\", \"...\", \"flower\", \"flower\", \"fruit\", \"fruit\"),\n  target = c(\"lavender\", \"lavender\", \"lemon\", \"lemon\", \"...\", \"lavender\", \"pear\", \"lemon\", \"lilac\", \"...\", \"lavender\", \"lavender\", \"lemon\", \"lemon\", \"...\", \"lavender\", \"pear\", \"lemon\", \"lilac\"),\n  reaction_time = c(564, 740, 602, 557, \"...\", 471, 649, 668, 519, \"...\", 1121, 1576, 1844, 1343, \"...\", 1876, 1265, 1721, 1846)\n  )\n# display fake dataframe with nice kable styling\nfaketable %&gt;%\n  kbl(caption = \"Example dataframe\") %&gt;%\n  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\nExample dataframe\n\n\nrow\nsubject\nmodality\ncongruency\ncue_type\nauditory_cue\ntarget\nreaction_time\n\n\n\n\n1\n1\nvisual\ncongruent\nobject\nlavender\nlavender\n564\n\n\n2\n1\nvisual\nincongruent\nobject\nlilac\nlavender\n740\n\n\n3\n1\nvisual\ncongruent\nobject\nlemon\nlemon\n602\n\n\n4\n1\nvisual\nincongruent\nobject\npear\nlemon\n557\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44\n1\nvisual\ncongruent\ncategory\nflower\nlavender\n471\n\n\n45\n1\nvisual\nincongruent\ncategory\nflower\npear\n649\n\n\n46\n1\nvisual\ncongruent\ncategory\nfruit\nlemon\n668\n\n\n47\n1\nvisual\nincongruent\ncategory\nfruit\nlilac\n519\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n82\n1\nolfactory\ncongruent\nobject\nlavender\nlavender\n1121\n\n\n83\n1\nolfactory\nincongruent\nobject\nlilac\nlavender\n1576\n\n\n84\n1\nolfactory\ncongruent\nobject\nlemon\nlemon\n1844\n\n\n85\n1\nolfactory\nincongruent\nobject\npear\nlemon\n1343\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127\n1\nolfactory\ncongruent\ncategory\nflower\nlavender\n1876\n\n\n128\n1\nolfactory\nincongruent\ncategory\nflower\npear\n1265\n\n\n129\n1\nolfactory\ncongruent\ncategory\nfruit\nlemon\n1721\n\n\n130\n1\nolfactory\nincongruent\ncategory\nfruit\nlilac\n1846"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-data",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-data",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Data",
    "text": "Load Data\nThe data I am using is a single .csv file from our previous experiment. These data were pre-processed in a separate R script that took each individual participants’ data and combined them into a single dataframe. The data are not aggregated, so they still represent the participants’ raw data. However, 5 participants were removed for having very low accuracy in the task. Additionally, we were only interested in trials where participants’ correctly determined whether the cue and target matched/did not match. Thus, these data only represent “correct” trials. Additionally, all participants’ in this dataset had at least 80% accuracy across the study.\n\n# load data\ndf &lt;- read.csv(file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/data/object_category.csv\", header = TRUE)\n# glimpse first few rows\nhead(df)\n\n  participant modality  congruency cue_type      cue   target reaction_time\n1         101   visual incongruent   object     Pear Lavender     1933.3277\n2         101   visual   congruent   object     Pear     Pear      400.3761\n3         101   visual   congruent   object    Lemon    Lemon      500.3686\n4         101   visual incongruent   object    Lilac     Pear      833.7295\n5         101   visual   congruent   object    Lilac    Lilac      533.7733\n6         101   visual   congruent   object Lavender Lavender      883.7940\n\n\nSome columns need to be changed to factors for the analysis.\n\n# Columns to convert to factors\ncols_to_factor &lt;- c(\"participant\", \"modality\", \"congruency\", \"cue_type\", \"cue\", \"target\")\n# Convert specified columns to factors\ndf[cols_to_factor] &lt;- lapply(df[cols_to_factor], as.factor)\n\nFirst, I wanted to plot the distribution of the dependent variable (reaction time) for both visual and olfactory target trials. This is because reaction time is often skewed. If the distributions are indeed skewed, I may want to model them using an Exgaussian distribution.\n\n# Add mean lines\nggplot(df, aes(x=reaction_time, fill = modality)) +\n  geom_histogram(bins = 100)+\n  theme(legend.position=\"top\") +\n  scale_fill_manual(values = c(colors[2], colors[5]))\n\n\n\n\n\n\n\n\nThey definitely appear skewed. Thus, I will take that into account with my model. However, I want to try both an Exgaussian and Gaussian model. I will then see how well each model recovers the distribution of the dependent variable using a Posterior Predictive Check (PPC)."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#gaussian-model",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#gaussian-model",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Gaussian model",
    "text": "Gaussian model\n\nPrior Predictive Check.\nI want to use informative priors for my model as I have already conducted several other similar studies and have several ideas about how reaction times will differ between different conditions. My original plan was to conduct a Prior Predictive Check to see how my informative priors would generate reaction time data. However, this proved too difficult and time consuming. The Prior Predictive Check gave estimates of some pararmeters in the millions, while also having very low effective sample sizes (ess). Some of the ess values were as low as 20, even with 4 chains running for 10,000 iterations. I think the model is simply to complex for the number of iterations I would need to run for stable estimates. Therefore, I will be going ahead without a Prior Predictive Check.\n\n\nInformative Priors\nI decided to use informative priors as I have run several studies looking at modality and congruency regarding vision and olfaction. Those have been outlined in the Notation section above. I did not put informative priors on the interaction terms however, because it is very hard to predict exactly how they will turn out.\n\n\nFitting Gaussian Model in brm\nFirst, I estimated the 3-way factorial model using a Gaussian distribution.\n\n# run Gaussian model in brm\ngaussian_model &lt;- brm(\n  reaction_time ~ modality*congruency*cue_type + (target|participant),\n  data = df,\n  family = gaussian(),\n  prior = c(prior(normal(1200, 200), class = Intercept),\n            prior(normal(500, 100), class = b, coef = \"modalityvisual\"),\n            prior(normal(200, 50), class = b, coef = \"congruencyincongruent\"),\n            prior(normal(100, 50), class = b, coef = \"cue_typeobject\"),\n            prior(cauchy(0, 2), class = sd),\n            prior(cauchy(0, 2), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter  = 6000,\n  warmup = 2000,\n  file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/models/gaussian_model\"\n  )"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#exgaussian-model",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#exgaussian-model",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Exgaussian Model",
    "text": "Exgaussian Model\nNext, I run an Exgaussian model. Everything is identical to the Gaussian model otherwise.\n\n# run Exgaussian model in brm\nexgaussian_model &lt;- brm(\n  reaction_time ~ modality*congruency*cue_type + (target|participant),\n  data = df,\n  family = exgaussian(),\n  prior = c(prior(normal(1200, 200), class = Intercept),\n            prior(normal(500, 100), class = b, coef = \"modalityvisual\"),\n            prior(normal(200, 50), class = b, coef = \"congruencyincongruent\"),\n            prior(normal(100, 50), class = b, coef = \"cue_typeobject\"),\n            prior(cauchy(0, 2), class = sd),\n            prior(cauchy(0, 2), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter  = 6000,\n  warmup = 2000, \n  file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/models/exgaussian_model\",\n  )\n\nUsing informative priors with the Exgaussian model led to sever errors, with R-hat values over 3 and ess values as low as 5. Not shown here, I ran another Exgaussian model where I used the default priors brm gives you. This model did well, so I will use it from here."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#model-comparison",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#model-comparison",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Model Comparison",
    "text": "Model Comparison\nMy plan was to first compare these models using the loo function. However, due to their complexity, I would need to redo both models and save the parameters as the models go. I have done this before and know it would take at least twice as long to run (sometimes longer). Given that the Exgaussian model took more than a day to complete, I just don’t have time. Instead I will let the PPC below guide my model selection."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#mcmc-traceplots",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#mcmc-traceplots",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "MCMC Traceplots",
    "text": "MCMC Traceplots\n\nmcmc_trace(gaussian_model, pars = c(\"b_Intercept\", \"b_modalityvisual\", \"b_congruencyincongruent\", \"b_cue_typeobject\", \"b_modalityvisual:congruencyincongruent\", \"b_modalityvisual:cue_typeobject\", \"b_congruencyincongruent:cue_typeobject\", \"b_modalityvisual:congruencyincongruent:cue_typeobject\", \"sigma\"), \n           facet_args = list(ncol = 2, strip.position = \"left\"))\n\n\n\n\n\n\n\n\nAll chains had Rhat values of 1, so I know they converged. This just gives a bit more visual evidence of that."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#fixed-effects",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#fixed-effects",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nHere I am taking all draws from all markov chains for all parameters and plotting a histogram of them. I will also take the model estimates and credible intervals and plot those under the histogram.\n\n# make posterior draws dataframe\ndraws_posterior &lt;- gaussian_model |&gt;\n  # select coefficients\n  spread_draws(b_Intercept, b_modalityvisual, b_congruencyincongruent, b_cue_typeobject, `b_modalityvisual:congruencyincongruent`, `b_modalityvisual:congruencyincongruent:cue_typeobject`) |&gt;\n  # new column called \"posterior\"\n  mutate(distribution = \"posterior\")\n\n\nMain Effects\nBelow I plot the posterior distributions for the main fixed effects.\n\n# graph of modality\nmodality_main_graph &lt;- ggplot(draws_posterior, aes(x = b_modalityvisual)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[2, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[2, 3], y = 0, xend = fixef(gaussian_model)[2, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Modality\") +\n  labs(x = \"Vision - Olfaction\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# graph of congruency\ncongruency_main_graph &lt;- ggplot(draws_posterior, aes(x = b_congruencyincongruent)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[3, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[3, 3], y = 0, xend = fixef(gaussian_model)[3, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Congruency\") +\n  labs(x = \"Incongruent - Congruent\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# graph of cue-type\ncuetype_main_graph &lt;- ggplot(draws_posterior, aes(x = b_cue_typeobject)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[4, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[4, 3], y = 0, xend = fixef(gaussian_model)[4, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Cue Type\") +\n  labs(x = \"Object-Cue - Category-Cue\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# arrange graphs beside one another\nggarrange(modality_main_graph, congruency_main_graph, cuetype_main_graph, ncol = 3)\n\n\n\n\n\n\n\n\nFirst, visual trials are 1176ms faster than olfactory trials. Because olfactory trials are the intercept (1764ms), visual trials are estimated to be approximately 588ms. This is a very big difference, but not surprising as it takes longer to process an odor compared to a visual stimulus. Next, incongruent trials are 109ms slower than congruent trials. So people tend to respond more slowly when the auditory cue and target do not match. Finally, people are 118ms faster at responding to targets when the cue is object-based rather than category-based.\nCentral to our main hypothesis, we were interested in whether the congruency effect (people responding slower when the cue and target did not match) was larger for olfactory target trials compared to visual target trials.\n\n# graph of modality x congruency posterior\ncongruency_modality_graph &lt;- ggplot(draws_posterior, aes(x = `b_modalityvisual:congruencyincongruent`)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[5, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[5, 3], y = 0, xend = fixef(gaussian_model)[5, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Interaction: Modality x Congruency\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\ncongruency_modality_cuetype_graph &lt;- ggplot(draws_posterior, aes(x = `b_modalityvisual:congruencyincongruent:cue_typeobject`)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[8, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[8, 3], y = 0, xend = fixef(gaussian_model)[8, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Interaction: Modality x Congruency x Cue-Type\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# arrange graphs beside one another\nggarrange(congruency_modality_graph, congruency_modality_cuetype_graph, ncol = 2)\n\n\n\n\n\n\n\n\nThis can be interpreted like this: people respond more slowly to incongruent cue target presentations, but this difference is greater when the target is olfactory compared to when it is visual. We could also say they are 53ms slower at responding to incongruent olfactory targets compared to incongruent visual targets. In addition, the 95% credible intervals do not overlap with zero (-92.02, -15.15). All of the 95% most probable reaction times for this interaction are negative, so I think we can say this difference is likely negative too. For comparison, this interaction was statistically significant in the original paper (p = 0.02). I actually think a p-value that close to the threshold is not amazingly convincing, so it kind of matches the interpretation of the credible interval here. The plot on the right represents the 3-way interaction. This lets us see whether the disparity in congruent and incongruent reaction times, which is larger in olfaction, occurs when object-cues preceed the olfactory target. The model suggests this is not the case.\nThere are other interactions as well, but they were not central to our main hypotheses so I won’t go into them here. In addition, I’m not a fan of 3-way interactions as they are hard to interpret and usually very underpowered to detect, so I don’t see much value in looking at the 3-way interaction here."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#modality-random-effects",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#modality-random-effects",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Modality random effects",
    "text": "Modality random effects\nBelow is some pre-processing for displaying main random-effects estimates.\n\n# temporary dataframe to be filled in loop\ntemp_df &lt;- data.frame(\"participant\" = factor(),\n                      \"empirical_olfactory_rt\" = numeric(),\n                      \"estimated_olfactory_rt\" = numeric(),\n                      \"empicial_visual_rt\" = numeric(),\n                      \"estimated_visual_rt\" = numeric())\n# for each unique participant:\nfor (i in unique(df$participant)) { \n  # ith participants' reaction time random effect for olfaction\n  olfaction_rt_i &lt;- fixef(gaussian_model)[1]  + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4]\n  # ith participants' reaction time random effect for vision\n  visual_rt_i &lt;- fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]) + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4]\n  # dataframe of fixed and random modality effect estimates for ith person\n  participant_df &lt;- data.frame(\"participant\" = i,\n                      \"empirical_olfactory_rt\" = fixef(gaussian_model)[1],\n                      \"estimated_olfactory_rt\" = olfaction_rt_i,\n                      \"empicial_visual_rt\" = fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]),\n                      \"estimated_visual_rt\" = fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]) + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4])\n  # here all participant fixed and random modality effect estimates are combined in a single dataframe\n  temp_df &lt;- rbind(temp_df, participant_df)\n}\n\n\n# reshape from wide to long\nmodality_df &lt;- temp_df %&gt;% pivot_longer(cols = c('empirical_olfactory_rt', 'estimated_olfactory_rt', 'empicial_visual_rt', 'estimated_visual_rt'),\n                    names_to = 'coefficient',\n                    values_to = 'rt')\n# create new column\nmodality_df$group &lt;- factor(ifelse(modality_df$coefficient == \"empirical_olfactory_rt\", \"empirical\",\n                            ifelse(modality_df$coefficient == \"empicial_visual_rt\", \"empirical\", \"estimated\")))\n# create new column\nmodality_df$group2 &lt;- factor(ifelse(modality_df$coefficient == \"empirical_olfactory_rt\", \"olfactory\",\n                            ifelse(modality_df$coefficient == \"estimated_olfactory_rt\", \"olfactory\", \"visual\")))\n# rename columns\ncolnames(modality_df) &lt;- c(\"participant\", \"coefficient\", \"rt\", \"estimate\", \"modality\")\n\nmodality_df_estimates &lt;- modality_df[modality_df$estimate == \"estimated\", ]\nmodality_df_estimates &lt;- aggregate(modality_df$rt, list(modality_df$participant, modality_df$modality), mean)\ncolnames(modality_df_estimates) &lt;- c(\"participant\", \"modality\", \"rt\")\n\n# Calculate mean rt for each modality\nmean_values &lt;- modality_df_estimates %&gt;%\n  group_by(modality) %&gt;%\n  summarize(mean_rt = mean(rt))\n\nmean_values &lt;- data.frame(\"participant\" = c(101, 101),\n                          \"modality\" = c(\"olfactory\", \"visual\"),\n                          \"rt\" = c(1837, 612))\nmean_values$participant &lt;- as.factor(mean_values$participant)\nmean_values$modality &lt;- as.factor(mean_values$modality)\n\n\n# graph random effects for modality\nggplot(modality_df_estimates, aes(x = modality, y = rt, group = participant)) +\n  geom_point(color = \"grey\") +\n  geom_line(color = \"grey\")  +\n  geom_point(data = mean_values, aes(x = modality, y = rt), color = \"black\", size = 3) +\n  geom_line(data = mean_values, aes(x = modality, y = rt, group = 1), color = \"black\", size = 1.5) +\n  ggtitle(\"Modality Random Effects\")\n\n\n\n\n\n\n\n\nAn alternate way to view this is to put each participants’ random effect estimate on a plot with the fixed effect estimate, and plot each of these on individual panels. Maybe not as intuitive as the previous plot, but it still makes sense to look at it like this.\n\n# empty list where each runner's graph will be saved.\ntemp_list &lt;- list()  \n# loop through runners, grab their specific intercepts/slope from previous dataframe, and save graph to list\nfor (i in unique(modality_df$participant)) {\n  # runner-specific graph\n  temp_graph &lt;- ggplot(modality_df[modality_df$participant == i, ], aes(x = modality, y = rt, color = estimate, group = estimate)) +\n  geom_point() +\n  geom_line() +\n    ylim(0, 2800) +\n    ggtitle(paste0(\"Subject \", i)) +\n    theme(legend.position = \"none\")\n  # save ith runner graph to temp_list\n  temp_list[[i]] &lt;- temp_graph\n}\n\narg_list &lt;- c(temp_list, list(nrow = 13, ncol = 5))\n# Use do.call to pass all elements of graph_list to ggarrange\narranged_plots &lt;- do.call(\"ggarrange\", arg_list)\n# Print or display the arranged plots\nprint(arranged_plots)"
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "April4.html",
    "href": "April4.html",
    "title": "April 4 Homework",
    "section": "",
    "text": "# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(broom.mixed) \nlibrary(rethinking)\nlibrary(ggpubr)\nlibrary(lme4)\nlibrary(DiagrammeR)"
  },
  {
    "objectID": "April4.html#a",
    "href": "April4.html#a",
    "title": "April 4 Homework",
    "section": "a)",
    "text": "a)\nTo explore the complete pooled behavior of the data points, construct and discuss a plot of Reaction by Days. In doing so, ignore the subjects and include the observed trend in the relationship.\n\nggplot(sleepstudy, aes(y = Reaction, x = Days)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nThis plots Day as a function of reaction time. Here we don’t account for the individual, so we are violating the assumption that each observation is independently distributed. The line I have fit should have a slope and intercept that match the OLS estimates. However, each day contains multiple observations from the same individual (thus violating independence). This also inflates my degrees of freedom. If I were calculating a p-value, it would be much lower than if I averaged each person’s reaction time per day. All of this considered, there is information about each participant we are likely missing. Their may be idiosyncrasies here that are lost because we are treating all observations as independent."
  },
  {
    "objectID": "April4.html#b",
    "href": "April4.html#b",
    "title": "April 4 Homework",
    "section": "b)",
    "text": "b)\nDraw a diagram in the spirit of Figure 15.8 that captures the complete pooling framework.\n\nDiagrammeR::grViz(\"digraph {\nPopulation -&gt; 'Day 0' -&gt; 'rt_1'\nPopulation -&gt; 'Day 1' -&gt; 'rt_2'\nPopulation -&gt; 'Day 2' -&gt; 'rt_3'\nPopulation -&gt; 'Day 3' -&gt; 'rt_4'\nPopulation -&gt; 'Day 4' -&gt; 'rt_5'\nPopulation -&gt; 'Day 5' -&gt; 'rt_6'\nPopulation -&gt; 'Day 6' -&gt; 'rt_7'\nPopulation -&gt; 'Day 7' -&gt; 'rt_8'\nPopulation -&gt; 'Day 8' -&gt; 'rt_9'\nPopulation -&gt; 'Day 9' -&gt; 'rt_10'\n}\")\n\n\n\n\n\nHere we don’t account for people, so each reaction time should just be clustered by day."
  },
  {
    "objectID": "April4.html#c",
    "href": "April4.html#c",
    "title": "April 4 Homework",
    "section": "c)",
    "text": "c)\nUsing careful notation, write out the structure for the complete pooled model of Y by X.\n\\(Y_{i} \\mid \\beta_{0}, \\beta_{1}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0} + \\beta_{1}X_{i}\\)\nAbove, the complete pooled model is saying that each Reaction Time (Yi), conditional on an intercept (B0, reaction time at Day 0), a slope (change in reaction time each day), and a common standard deviation is identically ad independently distributed. Each observation is a conditional mean (ui) such that it is distributed via a Gaussian distribution where the mean depends on the unique day."
  },
  {
    "objectID": "April4.html#a-1",
    "href": "April4.html#a-1",
    "title": "April 4 Homework",
    "section": "a)",
    "text": "a)\nTo explore the no pooled behavior of the data points, construct and discuss separate scatterplots of Reaction by Days for each Subject, including subject-specific trend lines.\n\n# renumber participants so that looping is easier.\nsleepstudy$numeric &lt;- as.numeric(sleepstudy$Subject)\n# empty dataframe to store unique intercept and slope for each runner.\ncoef_df &lt;- data.frame(\"Subject\" = \"\",\n                      \"intercept\" = \"\",\n                      \"Day\" = \"\")\nfor (i in unique(sleepstudy$numeric)) {\n  temp &lt;- stan_glm(\n    Reaction ~ Days, \n    data = sleepstudy[sleepstudy$numeric == i,], family = gaussian, \n    #prior_intercept = normal(50, 2.5, autoscale = TRUE),\n    prior_aux = exponential(1, autoscale = TRUE),\n    chains = 4, iter = 5000*2, seed = 84735, refresh = 0)\n  temp_df &lt;- data.frame(\"Subject\" = i,\n                        \"intercept\" = temp$coefficients[1],\n                        \"Day\" = temp$coefficients[2])\n  # add runner i's intercept and slope to the ith row of larger dataframe\n  coef_df &lt;- rbind(coef_df, temp_df)\n}\n# coef_df saved all columns as character. Here I resave them as factors/numeric\nbr_no_pool_coef_df &lt;- data.frame(\"Subject\" = as.factor(coef_df$Subject),\n                                 \"intercept\" = as.numeric(coef_df$intercept),\n                                 \"Day\" = as.numeric(coef_df$Day))\n# remove first row (is empty)\nbr_no_pool_coef_df &lt;-  br_no_pool_coef_df[-1, ]\n# reset rowname index to start at 1\nrow.names(br_no_pool_coef_df) &lt;- NULL\n\n\nNo pooling graph: stan_glm\n\n# empty list where each runner's graph will be saved.\ntemp_list &lt;- list()  \n# loop through runners, grab their specific intercepts/slope from previous dataframe, and save graph to list\nfor (i in unique(sleepstudy$numeric)) {\n  # runner-specific graph\n  temp_graph &lt;- ggplot(sleepstudy[sleepstudy$numeric == i, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    geom_abline(aes_string(intercept = br_no_pool_coef_df[i, 2], slope = br_no_pool_coef_df[i, 3]), color = \"blue\") +\n    ggtitle(paste0(\"Subject \", i)) +\n    ylim(190, 470)\n  # save ith runner graph to temp_list\n  temp_list[[i]] &lt;- temp_graph\n}\n# Use do.call to pass all elements of graph_list to ggarrange\narranged_plots &lt;- do.call(\"ggarrange\", temp_list)\n# Print or display the arranged plots\nprint(arranged_plots)\n\n\n\n\nEach person has a different intercept and slope, and non of these are informed by any of the other participants. This makes it a useless model for prediction in the sense that it isn’t clear how to use any of these estimates when predicting a new participant’s reaction time."
  },
  {
    "objectID": "April4.html#b-1",
    "href": "April4.html#b-1",
    "title": "April 4 Homework",
    "section": "b)",
    "text": "b)\nDraw a diagram in the spirit of Figure 15.6 that captures the no pooling framework.\n\nDiagrammeR::grViz(\"digraph {\nPopulation -&gt; 'Participant 1' -&gt; 'Day 01' -&gt; 'rt_01'\n'Participant 1' -&gt; 'Day 11' -&gt; 'rt_11'\nPopulation -&gt; 'Participant 2' -&gt; 'Day 02' -&gt; 'rt_02'\n'Participant 2' -&gt; 'Day 12' -&gt; 'rt_12'\n}\")\n\n\n\n\n\nI wasn’t sure if there was a single right way to do this. Basically, each participant has a reaction time for each day. I have shown the first two days for the first 2 participants. Everything is separate, meaning all coefficients will be estimated seperately."
  },
  {
    "objectID": "April4.html#c-1",
    "href": "April4.html#c-1",
    "title": "April 4 Homework",
    "section": "c)",
    "text": "c)\nUsing careful notation, write out the structure for the no pooled model of Y by X.\n\\(Y_{i} \\mid \\beta_{0i}, \\beta_{1i}, \\sigma_{i} \\sim N (\\mu_{i}, \\sigma_{i}^{2})\\) with \\(\\mu_{i} = \\beta_{0i} + \\beta_{1i}X_{i}\\)\nI believe this is right. Each person will have their own coefficient estimates, so the model must index the beta coefficients as well. I wasn’t sure about the standard deviation, but I decided to index it too given each participant will have a different spread of their reaction times. But if that is wrong, then I would update the equation so it does not index the sigma."
  },
  {
    "objectID": "April4.html#complete-pooled-trendline",
    "href": "April4.html#complete-pooled-trendline",
    "title": "April 4 Homework",
    "section": "Complete-pooled Trendline",
    "text": "Complete-pooled Trendline\n\n# complete pool\nggarrange(\n  ggplot(sleepstudy[sleepstudy$Subject == 308, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Subject 308\") +\n    ylim(min(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335]), max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = m.15.8_completepool$coefficients[1], slope = m.15.8_completepool$coefficients[2]), color = \"blue\"),\n  ggplot(sleepstudy[sleepstudy$Subject == 335, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Subject 335\") +\n    ylim(min(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335]), max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = m.15.8_completepool$coefficients[1], slope = m.15.8_completepool$coefficients[2]), color = \"blue\"),\n  ggplot(sleepstudy[sleepstudy$Subject == 308 | sleepstudy$Subject == 335, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Both Subjects\") +\n    ylim(min(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335]), max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = m.15.8_completepool$coefficients[1], slope = m.15.8_completepool$coefficients[2]), color = \"blue\"),\n  ncol = 3\n)\n\n\n\n\nWhen we completely pool the data, then the trend line goes exactly through the middle of the pooled data (right graph). However, if we put this trend line through participant 308 and participant 335 individually, the trend line doesn’t fit very well. For participant 335, it is even in the wrong direction."
  },
  {
    "objectID": "April4.html#no-pooled-trendline",
    "href": "April4.html#no-pooled-trendline",
    "title": "April 4 Homework",
    "section": "No-pooled Trendline",
    "text": "No-pooled Trendline\n\n# complete pool\nggarrange(\n  ggplot(sleepstudy[sleepstudy$Subject == 308, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Subject 308\") +\n    ylim(min(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335]), max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = no_pool_308$coefficients[1], slope = no_pool_308$coefficients[2]), color = \"blue\"),\n  ggplot(sleepstudy[sleepstudy$Subject == 335, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Subject 335\") +\n    ylim(min(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335]), max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = no_pool_335$coefficients[1], slope = no_pool_335$coefficients[2]), color = \"red\"),\n  ggplot(sleepstudy[sleepstudy$Subject == 308 | sleepstudy$Subject == 335, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Both Subjects\") +\n    ylim(min(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335]), max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = no_pool_308$coefficients[1], slope = no_pool_308$coefficients[2]), color = \"blue\") +\n    geom_abline(aes(intercept = no_pool_335$coefficients[1], slope = no_pool_335$coefficients[2]), color = \"red\"),\n  ncol = 3\n)\n\n\n\n\nNow we have a really well fitting trend line for each individual participant. However, when we look at the pooled data (right graph) we don’t have any group level estimate, making prediction of new data impossible."
  },
  {
    "objectID": "April4.html#partial-pooled-trendline",
    "href": "April4.html#partial-pooled-trendline",
    "title": "April 4 Homework",
    "section": "Partial-pooled Trendline",
    "text": "Partial-pooled Trendline\n\nggarrange(\n  ggplot(sleepstudy[sleepstudy$Subject == 308, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Subject 308\") +\n    ylim(200, max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = m.15.8_partial_pool$coefficients[1] + ranef(m.15.8_partial_pool)$Subject[1,1], slope = m.15.8_partial_pool$coefficients[2]), color = \"blue\"),\n  ggplot(sleepstudy[sleepstudy$Subject == 335, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Subject 335\") +\n    ylim(200, max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = m.15.8_partial_pool$coefficients[1] + ranef(m.15.8_partial_pool)$Subject[2,1], slope = m.15.8_partial_pool$coefficients[2]), color = \"blue\"),\n  ggplot(sleepstudy[sleepstudy$Subject == 308 | sleepstudy$Subject == 335, ], aes(x = Days, y = Reaction)) + \n    geom_point() +\n    ggtitle(\"Both Subjects\") +\n    ylim(200, max(sleepstudy$Reaction[sleepstudy$Subject == 308 | sleepstudy$Subject == 335])) +\n    geom_abline(aes(intercept = m.15.8_partial_pool$coefficients[1], slope = m.15.8_partial_pool$coefficients[2]), color = \"blue\"),\n  ncol = 3\n)\n\n\n\n\nAll three panels have different intercepts. The slopes are all the same as this is an intercept only model. But now, we can use the trendline on the right graph to make predictions about new data, and we have also accounted for individual differences (first two panels)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Pierzchajlo",
    "section": "",
    "text": "MSc Neuroscience\nCurrently a PhD Candidate in Psychology at Stockholm University\n \n  \n   \n  \n    \n     E-mail\n  \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Stephen Pierzchajlo",
    "section": "About",
    "text": "About\nWrite Something About Myself Here.\nLearn more"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Stephen Pierzchajlo",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nWhy You Should Pre-specify Exploratory Analyses\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Stephen Pierzchajlo",
    "section": "Blog posts",
    "text": "Blog posts\n\n\n\n\n\nWhy You Should Pre-specify Exploratory Analyses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nMore posts"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Stephen Pierzchajlo",
    "section": "CV",
    "text": "CV\nWrite Same Thing About Myself As I"
  },
  {
    "objectID": "index.html#learn-more",
    "href": "index.html#learn-more",
    "title": "Stephen Pierzchajlo",
    "section": "Learn more",
    "text": "Learn more"
  },
  {
    "objectID": "eyeball_test.html",
    "href": "eyeball_test.html",
    "title": "Eyeball Test",
    "section": "",
    "text": "# load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lsr)\nlibrary(ggpubr)\n\n\n# model parameters\nintercept &lt;- 350\nslope = 0\nsimulations &lt;- 20000\nsample_size &lt;- 100\n\n# empty dataframe to fill in the loop\ndf &lt;- data.frame(\"p\" = numeric(),\n                 \"r\" = numeric(),\n                 \"cohen_d\" = numeric())\n\n# simulate power\nfor (i in 1:simulations) {\n  x &lt;- rep(c(0, 1), each = sample_size/2)\n  y &lt;- intercept + slope*x + rnorm(sample_size, mean = 10, sd = 5)\n  sim_df &lt;- data.frame(x, y)\n  model &lt;-  lm(y ~ x, data = sim_df)\n  p_value &lt;- summary(model)$coefficients[2,4]\n  r &lt;- sqrt(summary(model)$r.squared)\n  cohen_d &lt;- abs((summary(model)$coefficients[2, 1])/(sqrt(100)*summary(model)$coefficients[2, 2]))\n  temp &lt;- data.frame(\"p\" = p_value,\n                     \"r\" = r,\n                     \"cohen_d\" = cohen_d)\n  df &lt;- rbind(temp, df)\n}\n# calculate power from simulation\ntype_I_error_rate &lt;- (length(df[df$p &lt;= 0.05, 1]))/simulations * 100\nprint(paste0(\"Type-I Error Rate: \", round(type_I_error_rate, 1)))\n\n[1] \"Type-I Error Rate: 5.1\"\n\n\n\n# graph simulation results\nggplot(df, aes(x = p)) +\n  geom_histogram(bins = 100 , fill = \"#6497b1\") +\n  geom_vline(xintercept = 0.05, linetype=\"dashed\", color = \"black\", size = 1) +\n  theme_minimal() +\n  ggtitle(paste0(\"p &lt; 0.05 = \", round(type_I_error_rate, 1), \"%\", \" , Effect Size = \", slope, \", n = \", sample_size))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nggplot(df, aes(x = r, y = p)) +\n  geom_point() +\n  geom_hline(yintercept = 0.05, linetype=\"dashed\", color = \"blue\", size = 1)\n\n\n\n\n\nggplot(df, aes(x = cohen_d, y = p)) +\n  geom_point() +\n  geom_hline(yintercept = 0.05, linetype=\"dashed\", color = \"blue\", size = 1)\n\n\n\n\n\nggplot(df, aes(x = cohen_d, y = r)) +\n  geom_point() +\n  geom_hline(yintercept = 0.05, linetype=\"dashed\", color = \"blue\", size = 1)\n\n\n\n\n\ndf_eyeball &lt;- df[df$cohen_d &gt;= 0.1, ]\ntype_I_error_rate &lt;- (length(df_eyeball[df_eyeball$p &lt;= 0.05, 1]))/nrow(df_eyeball) * 100\n\nggplot(df_eyeball, aes(x = p)) +\n  geom_histogram(bins = 200) +\n  xlim(0, 1) +\n  geom_vline(xintercept = 0.05, linetype=\"dashed\", \n             color = \"blue\", size=1) +\n  ggtitle(paste0(\"Error Rate = \", round(type_I_error_rate,1), \"%\", \" , True Effect Size = \", slope, \", n = \", sample_size))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\ntemp_list &lt;- list()\n#df_eyeball &lt;- df[df$cohen_d &gt;= 0.1, ]\n#type_I_error_rate &lt;- (length(df_eyeball[df_eyeball$p &lt;= 0.05, 1]))/nrow(df_eyeball) * 100\n\nfor (i in c(0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2)) {\n  df_eyeball &lt;- df[df$cohen_d &gt;= i, ]\n  type_I_error_rate &lt;- (length(df_eyeball[df_eyeball$p &lt;= 0.05, 1]))/nrow(df_eyeball) * 100\n  temp_graph &lt;- ggplot(df_eyeball, aes(x = p)) +\n    geom_histogram(bins = 100 , fill = \"#6497b1\") +\n    xlim(0, 1) +\n    geom_vline(xintercept = 0.05, linetype=\"dashed\", color = \"black\", size = 1) +\n    theme_minimal() +\n    ggtitle(paste0(\"p &lt; 0.05 = \", round(type_I_error_rate, 1), \"%\"))\n  temp_list[[as.character(i)]] &lt;- temp_graph\n}\n\n# Define the number of columns and rows\nnum_columns &lt;- 4\nnum_rows &lt;- 3\n\n# Use do.call to pass all elements of temp_list to ggarrange with specified columns and rows\narranged_plots &lt;- do.call(ggarrange, c(temp_list, ncol = num_columns, nrow = num_rows))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n# Print or display the arranged plots\nprint(arranged_plots)\n\n\n\n\n\n# simulate eyeball test: Continuous x\n# model parameters\nintercept &lt;- 350\nslope = 0\nsimulations &lt;- 10000\nsample_size &lt;- 1000\n\n# empty dataframe to fill in the loop\ndf &lt;- data.frame(\"p\" = numeric(),\n                 \"r\" = numeric(),\n                 \"cohen_d\" = numeric())\ndf_eyeball &lt;- data.frame(\"p\" = numeric(),\n                         \"r\" = numeric(),\n                         \"cohen_d\" = numeric())\n\n# simulate power\nfor (i in 1:simulations) {\n  x &lt;- rep(c(0, 1), each = sample_size/2)\n  y &lt;- intercept + slope*x + rnorm(sample_size, mean = 50, sd = 15)\n  sim_df &lt;- data.frame(x, y)\n  model &lt;-  lm(y ~ x, data = sim_df)\n  p_value &lt;- summary(model)$coefficients[2, 4]\n  r &lt;- sqrt(summary(model)$r.squared)\n  cohen_d &lt;- abs((summary(model)$coefficients[2, 1])/(sqrt(100)*summary(model)$coefficients[2, 2]))\n  temp &lt;- data.frame(\"p\" = p_value,\n                     \"r\" = r,\n                     \"cohen_d\" = cohen_d)\n  df &lt;- rbind(temp, df)\n  if (cohen_d &gt;= 0.18) {\n    temp_eyeball &lt;- data.frame(\"p\" = p_value,\n                               \"r\" = r,\n                               \"cohen_d\" = cohen_d)\n    df_eyeball &lt;- rbind(temp_eyeball, df_eyeball)\n  }\n}\nmean(df$cohen_d)\ntype_I_error_rate &lt;- (length(df[df$p &lt;= 0.05, 1]))/simulations * 100\ntype_I_error_rate_eyeball &lt;- (length(df_eyeball[df_eyeball$p &lt;= 0.05, 1]))/nrow(df_eyeball) * 100\nprint(paste0(\"Type-I Error Rate: \", round(type_I_error_rate, 1), \"%\"))\nprint(paste0(\"Type-I Error Rate For Eyeball Test: \", round(type_I_error_rate_eyeball, 1), \"%\"))\n\n\ndf_eyeball &lt;- df[df$cohen_d &gt;= 0.1, ]\nggplot(df, aes(x = p)) +\n  geom_histogram(bins = 200) +\n  xlim(0, 1) +\n  geom_vline(xintercept = 0.05, linetype=\"dashed\", \n             color = \"blue\", size=1) +\n  ggtitle(paste0(\"Error Rate = \", round(type_I_error_rate_eyeball,1), \"%\", \" , True Effect Size = \", slope, \", n = \", sample_size))\n\n\nggplot(df_eyeball, aes(x = p)) +\n  geom_histogram(bins = 200) +\n  xlim(0, 1) +\n  geom_vline(xintercept = 0.05, linetype=\"dashed\", \n             color = \"blue\", size=1) +\n  ggtitle(paste0(\"Error Rate = \", round(type_I_error_rate_eyeball,1), \"%\", \" , True Effect Size = \", slope, \", n = \", sample_size))\n\n\nggplot(df_eyeball, aes(x = p, y = r)) +\n  geom_point() +\n  geom_vline(xintercept = 0.05, linetype=\"dashed\", color = \"blue\", size = 1)"
  },
  {
    "objectID": "posts/1-You-are-doing-exploratory-analysis-wrong/eyeball_test.html",
    "href": "posts/1-You-are-doing-exploratory-analysis-wrong/eyeball_test.html",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "",
    "text": "# load libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(lsr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "posts/1-You-are-doing-exploratory-analysis-wrong/eyeball_test.html#how-does-a-simulation-work",
    "href": "posts/1-You-are-doing-exploratory-analysis-wrong/eyeball_test.html#how-does-a-simulation-work",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "How Does a Simulation Work?",
    "text": "How Does a Simulation Work?\nThis simulation will assume the null hypthesis is true. Therefore, there won’t be any difference between the groups at the population level. There may, however, be difference at the sample level. In fact, there almost always will be. Because I prefer linear regression, I will simulate data from a linear model:\n\\(Y_{i} \\mid \\beta_{0}, \\beta_{1}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0} + \\beta_{1}X_{i}\\)\nHowever, because the slope will always be cancelled out by zero, this model can be simplified to this:\n\\(Y_{i} \\mid \\beta_{0}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0}\\)\nFirst, I make an indicator for the independent variable. One group will have a zero, and the other a one. This also means the sample size is going to be n = 100.\n\nx &lt;- rep(c(0, 1), each = 50)\nx\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nTo make the dependent variable, one would need to add and multiply the model coefficients with the independent variable(s). That is what I will do here. No measurement is perfect though, so every sample will be measured with some error. I am simulating this error with the rnorm() function. Because there is no difference at the population level between the two groups, both groups are sampled from a normal distribution with a mean of 350 and a standard deviation of 30. To calculate the difference between groups, one would add a slope to the intercept. Here, the slope is zero and cancels out the influence of the independent variable.\n\nset.seed(1234)\ny &lt;- rnorm(100, mean = 350, sd = 30) + 0*x\ny\n\n  [1] 313.7880 358.3229 382.5332 279.6291 362.8737 365.1817 332.7578 333.6010\n  [9] 333.0664 323.2989 335.6842 320.0484 326.7124 351.9338 378.7848 346.6914\n [17] 334.6697 322.6641 324.8848 422.4751 354.0226 335.2794 336.7836 363.7877\n [25] 329.1884 306.5539 367.2427 319.2903 349.5459 321.9215 383.0689 335.7322\n [33] 328.7168 334.9623 301.1272 314.9714 284.5988 309.7702 341.1712 336.0231\n [41] 393.4849 317.9407 324.3391 341.5813 320.1698 320.9446 316.7805 312.4404\n [49] 334.2852 335.0945 295.8191 332.5377 316.7333 319.5511 345.1307 366.8917\n [57] 399.4345 326.7994 398.1773 315.2657 369.6977 426.4697 348.9572 329.9110\n [65] 349.7719 403.3125 315.8418 391.0348 389.8869 360.0942 350.2068 336.3359\n [73] 339.0043 369.4486 412.1081 345.3980 308.2790 328.2925 357.7479 340.4882\n [81] 344.6663 344.9002 308.8309 344.7864 375.5070 370.9283 366.4999 337.9180\n [89] 344.2522 314.1642 348.4052 357.6559 401.1789 380.0454 335.1325 360.6665\n [97] 315.9618 376.3461 379.1875 413.6335\n\n\nNow the data can be put in a dataframe\n\nsim_df &lt;- data.frame(x, y)\nhead(sim_df)\n\n  x        y\n1 0 313.7880\n2 0 358.3229\n3 0 382.5332\n4 0 279.6291\n5 0 362.8737\n6 0 365.1817\n\n\nFinally, a linear regression can be performed on the sample. Remember, their is no difference between the two groups at the pupulation level, but there almost certainlt will be differences at the sample level (due to sampling error).\n\nmodel &lt;-  lm(y ~ x, data = sim_df)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = sim_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.367 -17.301  -3.815  15.830  86.067 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  336.408      4.090  82.242  &lt; 2e-16 ***\nx             17.777      5.785   3.073  0.00274 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.92 on 98 degrees of freedom\nMultiple R-squared:  0.0879,    Adjusted R-squared:  0.07859 \nF-statistic: 9.444 on 1 and 98 DF,  p-value: 0.002743\n\n\nAbove I can see the slope coefficient (x) is actually statistically significant. This is likely due to the dact that the Intercept estimate is so low. Both groups were sampled from the same distribution centered at 350. THe Intercept is 336, and the slope is 18. This means the mean of the second group is 336 + 18 = 354. In frequentist statistics, probabilities are conceptualized in reference to long-term frequencies. For instance, we know a fair coin has a 50% probability of coming up heads because if we flipped that coin a theoretically infinite number of times, approximately 50% of those flips would land heads. P-values can be thought of the same way: If you were to run the same experiment an infinite number of times, fixing the sample size and collecting a new sample with each experiment, how many p-values would be below your threshold (in most cases in science, the threshold is 0.05)? When the null hypothesis is true, only 5% of experiments will yield a p-value below 0.05."
  },
  {
    "objectID": "content/posts/1-eyeball-test/1-eyeball-test.html",
    "href": "content/posts/1-eyeball-test/1-eyeball-test.html",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "",
    "text": "# load libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(lsr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "content/posts/1-eyeball-test/1-eyeball-test.html#how-does-a-simulation-work",
    "href": "content/posts/1-eyeball-test/1-eyeball-test.html#how-does-a-simulation-work",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "How Does a Simulation Work?",
    "text": "How Does a Simulation Work?\nThis simulation will assume the null hypthesis is true. Therefore, there won’t be any difference between the groups at the population level. There may, however, be difference at the sample level. In fact, there almost always will be. Because I prefer linear regression, I will simulate data from a linear model:\n\\(Y_{i} \\mid \\beta_{0}, \\beta_{1}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0} + \\beta_{1}X_{i}\\)\nHowever, because the slope will always be cancelled out by zero, this model can be simplified to this:\n\\(Y_{i} \\mid \\beta_{0}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0}\\)\nFirst, I make an indicator for the independent variable. One group will have a zero, and the other a one. This also means the sample size is going to be n = 100.\n\nx &lt;- rep(c(0, 1), each = 50)\nx\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nTo make the dependent variable, one would need to add and multiply the model coefficients with the independent variable(s). That is what I will do here. No measurement is perfect though, so every sample will be measured with some error. I am simulating this error with the rnorm() function. Because there is no difference at the population level between the two groups, both groups are sampled from a normal distribution with a mean of 350 and a standard deviation of 30. To calculate the difference between groups, one would add a slope to the intercept. Here, the slope is zero and cancels out the influence of the independent variable.\n\nset.seed(1234)\ny &lt;- rnorm(100, mean = 350, sd = 30) + 0*x\ny\n\n  [1] 313.7880 358.3229 382.5332 279.6291 362.8737 365.1817 332.7578 333.6010\n  [9] 333.0664 323.2989 335.6842 320.0484 326.7124 351.9338 378.7848 346.6914\n [17] 334.6697 322.6641 324.8848 422.4751 354.0226 335.2794 336.7836 363.7877\n [25] 329.1884 306.5539 367.2427 319.2903 349.5459 321.9215 383.0689 335.7322\n [33] 328.7168 334.9623 301.1272 314.9714 284.5988 309.7702 341.1712 336.0231\n [41] 393.4849 317.9407 324.3391 341.5813 320.1698 320.9446 316.7805 312.4404\n [49] 334.2852 335.0945 295.8191 332.5377 316.7333 319.5511 345.1307 366.8917\n [57] 399.4345 326.7994 398.1773 315.2657 369.6977 426.4697 348.9572 329.9110\n [65] 349.7719 403.3125 315.8418 391.0348 389.8869 360.0942 350.2068 336.3359\n [73] 339.0043 369.4486 412.1081 345.3980 308.2790 328.2925 357.7479 340.4882\n [81] 344.6663 344.9002 308.8309 344.7864 375.5070 370.9283 366.4999 337.9180\n [89] 344.2522 314.1642 348.4052 357.6559 401.1789 380.0454 335.1325 360.6665\n [97] 315.9618 376.3461 379.1875 413.6335\n\n\nNow the data can be put in a dataframe\n\nsim_df &lt;- data.frame(x, y)\nhead(sim_df)\n\n  x        y\n1 0 313.7880\n2 0 358.3229\n3 0 382.5332\n4 0 279.6291\n5 0 362.8737\n6 0 365.1817\n\n\nFinally, a linear regression can be performed on the sample. Remember, their is no difference between the two groups at the pupulation level, but there almost certainlt will be differences at the sample level (due to sampling error).\n\nmodel &lt;-  lm(y ~ x, data = sim_df)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = sim_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.367 -17.301  -3.815  15.830  86.067 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  336.408      4.090  82.242  &lt; 2e-16 ***\nx             17.777      5.785   3.073  0.00274 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.92 on 98 degrees of freedom\nMultiple R-squared:  0.0879,    Adjusted R-squared:  0.07859 \nF-statistic: 9.444 on 1 and 98 DF,  p-value: 0.002743\n\n\nAbove I can see the slope coefficient (x) is actually statistically significant. This is likely due to the dact that the Intercept estimate is so low. Both groups were sampled from the same distribution centered at 350. THe Intercept is 336, and the slope is 18. This means the mean of the second group is 336 + 18 = 354. In frequentist statistics, probabilities are conceptualized in reference to long-term frequencies. For instance, we know a fair coin has a 50% probability of coming up heads because if we flipped that coin a theoretically infinite number of times, approximately 50% of those flips would land heads. P-values can be thought of the same way: If you were to run the same experiment an infinite number of times, fixing the sample size and collecting a new sample with each experiment, how many p-values would be below your threshold (in most cases in science, the threshold is 0.05)? When the null hypothesis is true, only 5% of experiments will yield a p-value below 0.05."
  }
]
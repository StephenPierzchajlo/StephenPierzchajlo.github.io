[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Pierzchajlo",
    "section": "",
    "text": "MSc Neuroscience\nCurrently a PhD Candidate in Psychology at Stockholm University\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     GitHub"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Stephen Pierzchajlo",
    "section": "About",
    "text": "About\nI am cognitive neuroscientist (Msc) and experimental psychologist (Phd) with a passion for data analysis and statistics."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Stephen Pierzchajlo",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nCensus Income Data Set\n\n\n\n\n\n\n\n\n\n\n\n\n\nJournal Of Neuroscience: Bayesian Reanalysis\n\n\n\n\n\n\n\n\n\n\n\n\n\nThreat Imminence And Everyday Altruism During The COVID-19 Pandemic\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Stephen Pierzchajlo",
    "section": "Blog posts",
    "text": "Blog posts\n\n\n\n\n\nWhy You Should Pre-specify Exploratory Analyses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#cv",
    "href": "index.html#cv",
    "title": "Stephen Pierzchajlo",
    "section": "CV",
    "text": "CV\nWrite Same Thing About Myself As I"
  },
  {
    "objectID": "index.html#learn-more",
    "href": "index.html#learn-more",
    "title": "Stephen Pierzchajlo",
    "section": "Learn more",
    "text": "Learn more"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "",
    "text": "Is increased COVID-19 threat over time associated with increased altruism?"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#data-collection",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#data-collection",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Data Collection",
    "text": "Data Collection\nData for this analysis came from a study conducted online between March 24th and April 14th of 2020. Each week, 150 different people from the United States were surveyed about a range of self-reported behaviours and psychological states. Therefore, a total of 600 people were included in the study, collected over 4 data acquisition periods. For the purposes of this analysis, only the measurements directly associated with the Bayesian model will be discussed. The dependent measure for this study was each participants’ self-reported altruism (SRA) score. The main predictors we collected and had hypotheses related to were: COVID-19 Risk Perception (RP), Percieved Stress Score (PSS), Anxiety, and Depression. We also collected each participants’ age, gender, employment status, and financial situation. These latter 4 predictors had no prior hypothesis tied to them, but we thought each could contribute to predicting SRA. Later, each predictor will be very briefly described as it relates to the hypothesis."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#covid-19-data-aquisition",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#covid-19-data-aquisition",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "COVID-19 Data Aquisition",
    "text": "COVID-19 Data Aquisition\nTo get a sense of where the United States was in terms of overall COVID-19 cases during our data acquisition period, I obtained COVID-19 related data from github for those 4 weeks. I then created a dataframe that contains only the weeks for which the data for the experiment took place. The code chunk below puts this dataframe together.\n\n# Load libraries\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(gghighlight)\nlibrary(plyr)\nlibrary(readr)\nlibrary(plotly)\nlibrary(ggpubr)\nlibrary(dplyr)\nlibrary(bayesplot)\nlibrary(rethinking)\nlibrary(ggmcmc)\nlibrary(brms)\nlibrary(broom)\nlibrary(ggrepel)\n\n# Here, we load .csv files from github. These are updated daily, so everytime this analysis is run, it uses the\n# most up to date information.\n\n# Total Global Cases Data\nCOVID_US_Wide&lt;-read_csv(url(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"))\n\n# Total Global Deaths Data\nCOVID_US_Deaths_Wide &lt;-read_csv(url(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"))\n\n\n# Total global cases data in long format.\nCOVID_US_Long_Date &lt;- gather(COVID_US_Wide, date, cases, 12:ncol(COVID_US_Wide), factor_key=TRUE)\n\n# Total global deaths data in long format\nCOVID_US_Deaths_Long_Date &lt;- gather(COVID_US_Deaths_Wide, date, cases, 13:ncol(COVID_US_Deaths_Wide),\n                                    factor_key=TRUE)\n\n\n# Filter States as the  cases dataset also includes groups (American Samoa) and boats (Diamond Cruise ship).\nCovid_US_Filter &lt;- COVID_US_Long_Date %&gt;% dplyr::filter(\n  `Province_State` %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\",\n                          \"Colorado\", \"Connecticut\", \"Delaware\", \"District of Columbia\", \"Florida\", \"Georgia\",\n                          \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\",\n                          \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\",\n                          \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\",\n                          \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\",\n                          \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",\n                          \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\",\n                          \"West Virginia\", \"Wisconsin\", \"Wyoming\"))\n\n# Filter data to only include dates starting from March 1st.\nCovid_US_Filter &lt;- Covid_US_Filter[126558:nrow(Covid_US_Filter), ]\n\n# Further reduce columns to only include states, dates, and cases.\nCovid_US_Filter &lt;- ddply(Covid_US_Filter, c(\"`Province_State`\", \"date\"), summarise,\n                            cases = sum(cases))\n\n# Filter States as the deaths dataset also includes groups (American Samoa) and boats (Diamond Cruise ship).\nCovid_US_Deaths_Filter &lt;- COVID_US_Deaths_Long_Date %&gt;% dplyr::filter(\n  `Province_State` %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\",\n                          \"Connecticut\", \"Delaware\", \"District of Columbia\", \"Florida\", \"Georgia\", \"Hawaii\",\n                          \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\",\n                          \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\",\n                          \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\",\n                          \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\",\n                          \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\",\n                          \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\",\n                          \"Wisconsin\", \"Wyoming\"))\n\n# Filter States as the dataset also includes groups (American Samoa) and boats (Diamond Cruise ship).\nCovid_US_Deaths_Filter &lt;- Covid_US_Deaths_Filter[126558:nrow(Covid_US_Deaths_Filter), ]\n\n# Filter States as the dataset also includes groups (American Samoa) and boats (Diamond Cruise ship).\nCovid_US_Deaths_Filter &lt;- ddply(Covid_US_Deaths_Filter, c(\"`Province_State`\", \"date\"), summarise,\n                            deaths = sum(cases))\n\n# Bind cases and deaths dataframes.\nCovid_Cases_Deaths &lt;- cbind(Covid_US_Filter, Covid_US_Deaths_Filter)\n\n# Change column names.\ncolnames(Covid_Cases_Deaths) &lt;- c(\"State\", \"Date\", \"Total_State_Cases\", \"Null1\", \"Null2\", \"Total_State_Deaths\")\n\n# Subset combined dataframe to reduce redundant features.\nCovid_Cases_Deaths &lt;- subset(Covid_Cases_Deaths, select = c(\"State\", \"Date\", \"Total_State_Cases\", \"Total_State_Deaths\"))\n\n# Create column indicating each week of Joana's data collection.\nCovid_Cases_Deaths$Week &lt;- ifelse(Covid_Cases_Deaths$Date == \"3/23/20\", \"Week 1\", \n                                  ifelse(Covid_Cases_Deaths$Date == \"3/30/20\", \"Week 2\",\n                                  ifelse(Covid_Cases_Deaths$Date == \"4/6/20\", \"Week 3\",\n                                  ifelse( Covid_Cases_Deaths$Date == \"4/13/20\", \"Week 4\", \"NA\"))))\n\n# Remove certain rows so the dataset only contains days from the weeks Joana collected data.\nCovid_Cases_Deaths &lt;- Covid_Cases_Deaths[!(Covid_Cases_Deaths$Week==\"NA\"), ]\n\n# New dataframe, summarising cases by week of data collection.\nCovid_Cases &lt;- ddply(Covid_Cases_Deaths, c(\"State\", \"Week\"), summarise,\n               N    = length(Total_State_Cases),\n               Total_State_Cases = sum(Total_State_Cases))\n\n# New dataframe, summarising deaths by week of data collection.\nCovid_Deaths &lt;- ddply(Covid_Cases_Deaths, c(\"State\", \"Week\"), summarise,\n               N    = length(Total_State_Deaths),\n               Total_State_Deaths = sum(Total_State_Deaths))\n\n# Dataframe combining summarised cases and deaths dataframes.\nTotal_Cases_And_Deaths &lt;- cbind(Covid_Cases, Covid_Deaths)\n\n# Change column names.\ncolnames(Total_Cases_And_Deaths) &lt;- c(\"State\", \"Week\", \"N\", \"Total_State_Cases\", \"Null1\", \"Mull2\", \"Null3\", \"Total_State_Deaths\")\n\n# Filter out redundant columns.\nTotal_Cases_And_Deaths &lt;- subset(Total_Cases_And_Deaths, select = c(\"State\", \"Week\", \"N\", \"Total_State_Cases\", \"Total_State_Deaths\"))\n\nTotal_Cases_And_Deaths$Total_US_Cases &lt;- ifelse(Total_Cases_And_Deaths$Week == \"Week 1\", sum(Total_Cases_And_Deaths$Total_State_Cases[Total_Cases_And_Deaths$Week == \"Week 1\"]),\n                                         ifelse(Total_Cases_And_Deaths$Week == \"Week 2\", sum(Total_Cases_And_Deaths$Total_State_Cases[Total_Cases_And_Deaths$Week == \"Week 2\"]),\n                                         ifelse(Total_Cases_And_Deaths$Week == \"Week 3\", sum(Total_Cases_And_Deaths$Total_State_Cases[Total_Cases_And_Deaths$Week == \"Week 3\"]),\n                                         ifelse(Total_Cases_And_Deaths$Week == \"Week 4\", sum(Total_Cases_And_Deaths$Total_State_Cases[Total_Cases_And_Deaths$Week == \"Week 4\"]), \"None\"))))\n\n# Total\nTotal_Cases_And_Deaths$Total_US_Cases &lt;- as.double(Total_Cases_And_Deaths$Total_US_Cases)\n\n\n\nTotal_Cases_And_Deaths$Total_US_Deaths &lt;- ifelse(Total_Cases_And_Deaths$Week == \"Week 1\",     sum(Total_Cases_And_Deaths$Total_State_Deaths[Total_Cases_And_Deaths$Week == \"Week 1\"]),\n                                         ifelse(Total_Cases_And_Deaths$Week == \"Week 2\", sum(Total_Cases_And_Deaths$Total_State_Deaths[Total_Cases_And_Deaths$Week == \"Week 2\"]),\n                                         ifelse(Total_Cases_And_Deaths$Week == \"Week 3\", sum(Total_Cases_And_Deaths$Total_State_Deaths[Total_Cases_And_Deaths$Week == \"Week 3\"]),\n                                         ifelse(Total_Cases_And_Deaths$Week == \"Week 4\", sum(Total_Cases_And_Deaths$Total_State_Deaths[Total_Cases_And_Deaths$Week == \"Week 4\"]), \"None\"))))\n\n# Total\nTotal_Cases_And_Deaths$Total_US_Deaths &lt;- as.double(Total_Cases_And_Deaths$Total_US_Deaths)\n\n# Add state abbreviations.\nTotal_Cases_And_Deaths$State_Abb &lt;- case_when(\nTotal_Cases_And_Deaths$State == \"Alabama\" ~ \"AL\",\nTotal_Cases_And_Deaths$State == \"Alaska\" ~ \"AK\",\nTotal_Cases_And_Deaths$State == \"Arizona\" ~ \"AZ\",\nTotal_Cases_And_Deaths$State == \"Arkansas\" ~ \"AR\",\nTotal_Cases_And_Deaths$State == \"California\" ~ \"CA\",\nTotal_Cases_And_Deaths$State == \"Colorado\" ~ \"CO\",\nTotal_Cases_And_Deaths$State == \"Connecticut\" ~ \"CT\",\nTotal_Cases_And_Deaths$State == \"Delaware\" ~ \"DE\",\nTotal_Cases_And_Deaths$State == \"District of Columbia\" ~ \"DC\",      \nTotal_Cases_And_Deaths$State == \"Florida\" ~ \"FL\",\nTotal_Cases_And_Deaths$State == \"Georgia\" ~ \"GA\",\nTotal_Cases_And_Deaths$State == \"Hawaii\" ~ \"HI\",\nTotal_Cases_And_Deaths$State == \"Idaho\" ~ \"ID\",\nTotal_Cases_And_Deaths$State == \"Illinois\" ~ \"IL\",\nTotal_Cases_And_Deaths$State == \"Indiana\" ~ \"IN\",\nTotal_Cases_And_Deaths$State == \"Iowa\" ~ \"IA\",\nTotal_Cases_And_Deaths$State == \"Kansas\" ~ \"KA\",\nTotal_Cases_And_Deaths$State == \"Kentucky\" ~ \"KY\",\nTotal_Cases_And_Deaths$State == \"Louisiana\" ~ \"LA\",\nTotal_Cases_And_Deaths$State == \"Maine\" ~ \"ME\",\nTotal_Cases_And_Deaths$State == \"Maryland\" ~ \"MD\",\nTotal_Cases_And_Deaths$State == \"Massachusetts\" ~ \"MA\",\nTotal_Cases_And_Deaths$State == \"Michigan\" ~ \"MI\",\nTotal_Cases_And_Deaths$State == \"Minnesota\" ~ \"MN\",\nTotal_Cases_And_Deaths$State == \"Mississippi\" ~ \"MS\",\nTotal_Cases_And_Deaths$State == \"Missouri\" ~ \"MO\",\nTotal_Cases_And_Deaths$State == \"Montana\" ~ \"MT\",\nTotal_Cases_And_Deaths$State == \"Nebraska\" ~ \"NE\",\nTotal_Cases_And_Deaths$State == \"Nevada\" ~ \"NV\",\nTotal_Cases_And_Deaths$State == \"New Hampshire\" ~ \"NH\",\nTotal_Cases_And_Deaths$State == \"New Jersey\" ~ \"NJ\",\nTotal_Cases_And_Deaths$State == \"New Mexico\" ~ \"NM\",\nTotal_Cases_And_Deaths$State == \"New York\" ~ \"NY\",\nTotal_Cases_And_Deaths$State == \"North Carolina\" ~ \"NC\",\nTotal_Cases_And_Deaths$State == \"North Dakota\" ~ \"ND\",\nTotal_Cases_And_Deaths$State == \"Ohio\" ~ \"OH\",\nTotal_Cases_And_Deaths$State == \"Oklahoma\" ~ \"OK\",\nTotal_Cases_And_Deaths$State == \"Oregon\" ~ \"OR\",\nTotal_Cases_And_Deaths$State == \"Pennsylvania\" ~ \"PA\",\nTotal_Cases_And_Deaths$State == \"Rhode Island\" ~ \"RI\",\nTotal_Cases_And_Deaths$State == \"South Carolina\" ~ \"SC\",\nTotal_Cases_And_Deaths$State == \"South Dakota\" ~ \"SD\",\nTotal_Cases_And_Deaths$State == \"Tennessee\" ~ \"TN\",\nTotal_Cases_And_Deaths$State == \"Texas\" ~ \"TX\",\nTotal_Cases_And_Deaths$State == \"Utah\" ~ \"UT\",\nTotal_Cases_And_Deaths$State == \"Vermont\" ~ \"VT\",\nTotal_Cases_And_Deaths$State == \"Virginia\" ~ \"VA\",\nTotal_Cases_And_Deaths$State == \"Washington\" ~ \"WA\",\nTotal_Cases_And_Deaths$State == \"West Virginia\" ~ \"WV\",\nTotal_Cases_And_Deaths$State == \"Wisconsin\" ~ \"WI\",\nTotal_Cases_And_Deaths$State == \"Wyoming\" ~ \"WY\",\n    TRUE ~ as.character(Total_Cases_And_Deaths$State)\n)\n\n# Add column with per capity cases (per 10,000 citizens).\nTotal_Cases_And_Deaths$Cases_Per_10000 &lt;- case_when(\nTotal_Cases_And_Deaths$State == \"Alabama\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4908621) * 10000,\nTotal_Cases_And_Deaths$State == \"Alaska\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/734002) * 10000,\nTotal_Cases_And_Deaths$State == \"Arizona\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/7378494) * 10000,\nTotal_Cases_And_Deaths$State == \"Arkansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3038999) * 10000,\nTotal_Cases_And_Deaths$State == \"California\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/39937489) * 10000,\nTotal_Cases_And_Deaths$State == \"Colorado\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5845526) * 10000,\nTotal_Cases_And_Deaths$State == \"Connecticut\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3563077) * 10000,\nTotal_Cases_And_Deaths$State == \"Delaware\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/982895) * 10000,\nTotal_Cases_And_Deaths$State == \"District of Columbia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/    720687) * 10000,\nTotal_Cases_And_Deaths$State == \"Florida\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/21992985) * 10000,\nTotal_Cases_And_Deaths$State == \"Georgia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/10736059) * 10000,\nTotal_Cases_And_Deaths$State == \"Hawaii\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1412687) * 10000,\nTotal_Cases_And_Deaths$State == \"Idaho\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1826156) * 10000,\nTotal_Cases_And_Deaths$State == \"Illinois\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/12659682) * 10000,\nTotal_Cases_And_Deaths$State == \"Indiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6745354) * 10000,\nTotal_Cases_And_Deaths$State == \"Iowa\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3179849) * 10000,\nTotal_Cases_And_Deaths$State == \"Kansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/2910357) * 10000,\nTotal_Cases_And_Deaths$State == \"Kentucky\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4499692) * 10000,\nTotal_Cases_And_Deaths$State == \"Louisiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4645184) * 10000,\nTotal_Cases_And_Deaths$State == \"Maine\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1345790) * 10000,\nTotal_Cases_And_Deaths$State == \"Maryland\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6083116) * 10000,\nTotal_Cases_And_Deaths$State == \"Massachusetts\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6976597) * 10000,\nTotal_Cases_And_Deaths$State == \"Michigan\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/10045029) * 10000,\nTotal_Cases_And_Deaths$State == \"Minnesota\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5700671) * 10000,\nTotal_Cases_And_Deaths$State == \"Mississippi\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/2989260) * 10000,\nTotal_Cases_And_Deaths$State == \"Missouri\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6169270) * 10000,\nTotal_Cases_And_Deaths$State == \"Montana\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1086759) * 10000,\nTotal_Cases_And_Deaths$State == \"Nebraska\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1952570) * 10000,\nTotal_Cases_And_Deaths$State == \"Nevada\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3139658) * 10000,\nTotal_Cases_And_Deaths$State == \"New Hampshire\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1371246) * 10000,\nTotal_Cases_And_Deaths$State == \"New Jersey\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/8936574) * 10000,\nTotal_Cases_And_Deaths$State == \"New Mexico\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/2096640) * 10000,\nTotal_Cases_And_Deaths$State == \"New York\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/19440469) * 10000,\nTotal_Cases_And_Deaths$State == \"North Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/10611862) * 10000,\nTotal_Cases_And_Deaths$State == \"North Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/761723) * 10000,\nTotal_Cases_And_Deaths$State == \"Ohio\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/11747694) * 10000,\nTotal_Cases_And_Deaths$State == \"Oklahoma\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3954821) * 10000,\nTotal_Cases_And_Deaths$State == \"Oregon\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4301089) * 10000,\nTotal_Cases_And_Deaths$State == \"Pennsylvania\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/12820878) * 10000,\nTotal_Cases_And_Deaths$State == \"Rhode Island\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1056161) * 10000,\nTotal_Cases_And_Deaths$State == \"South Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5210095) * 10000,\nTotal_Cases_And_Deaths$State == \"South Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/903027) * 10000,\nTotal_Cases_And_Deaths$State == \"Tennessee\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6897576) * 10000,\nTotal_Cases_And_Deaths$State == \"Texas\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/29472295) * 10000,\nTotal_Cases_And_Deaths$State == \"Utah\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3282115) * 10000,\nTotal_Cases_And_Deaths$State == \"Vermont\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/628061) * 10000,\nTotal_Cases_And_Deaths$State == \"Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/8626207) * 10000,\nTotal_Cases_And_Deaths$State == \"Washington\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/7797095) * 10000,\nTotal_Cases_And_Deaths$State == \"West Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1778070) * 10000,\nTotal_Cases_And_Deaths$State == \"Wisconsin\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5851754) * 10000,\nTotal_Cases_And_Deaths$State == \"Wyoming\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/567025) * 10000,\n    TRUE ~ as.double(Total_Cases_And_Deaths$State))\n\n# Add column with per capity cases (per 100,000 citizens).\nTotal_Cases_And_Deaths$Cases_Per_100000 &lt;- case_when(\nTotal_Cases_And_Deaths$State == \"Alabama\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4908621) * 100000,\nTotal_Cases_And_Deaths$State == \"Alaska\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/734002) * 100000,\nTotal_Cases_And_Deaths$State == \"Arizona\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/7378494) * 100000,\nTotal_Cases_And_Deaths$State == \"Arkansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3038999) * 100000,\nTotal_Cases_And_Deaths$State == \"California\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/39937489) * 100000,\nTotal_Cases_And_Deaths$State == \"Colorado\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5845526) * 100000,\nTotal_Cases_And_Deaths$State == \"Connecticut\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3563077) * 100000,\nTotal_Cases_And_Deaths$State == \"Delaware\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/982895) * 100000,\nTotal_Cases_And_Deaths$State == \"District of Columbia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/    720687) * 100000,\nTotal_Cases_And_Deaths$State == \"Florida\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/21992985) * 100000,\nTotal_Cases_And_Deaths$State == \"Georgia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/10736059) * 100000,\nTotal_Cases_And_Deaths$State == \"Hawaii\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1412687) * 100000,\nTotal_Cases_And_Deaths$State == \"Idaho\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1826156) * 100000,\nTotal_Cases_And_Deaths$State == \"Illinois\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/12659682) * 100000,\nTotal_Cases_And_Deaths$State == \"Indiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6745354) * 100000,\nTotal_Cases_And_Deaths$State == \"Iowa\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3179849) * 100000,\nTotal_Cases_And_Deaths$State == \"Kansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/2910357) * 100000,\nTotal_Cases_And_Deaths$State == \"Kentucky\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4499692) * 100000,\nTotal_Cases_And_Deaths$State == \"Louisiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4645184) * 100000,\nTotal_Cases_And_Deaths$State == \"Maine\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1345790) * 100000,\nTotal_Cases_And_Deaths$State == \"Maryland\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6083116) * 100000,\nTotal_Cases_And_Deaths$State == \"Massachusetts\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6976597) * 100000,\nTotal_Cases_And_Deaths$State == \"Michigan\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/10045029) * 100000,\nTotal_Cases_And_Deaths$State == \"Minnesota\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5700671) * 100000,\nTotal_Cases_And_Deaths$State == \"Mississippi\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/2989260) * 100000,\nTotal_Cases_And_Deaths$State == \"Missouri\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6169270) * 100000,\nTotal_Cases_And_Deaths$State == \"Montana\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1086759) * 100000,\nTotal_Cases_And_Deaths$State == \"Nebraska\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1952570) * 100000,\nTotal_Cases_And_Deaths$State == \"Nevada\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3139658) * 100000,\nTotal_Cases_And_Deaths$State == \"New Hampshire\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1371246) * 100000,\nTotal_Cases_And_Deaths$State == \"New Jersey\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/8936574) * 100000,\nTotal_Cases_And_Deaths$State == \"New Mexico\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/2096640) * 100000,\nTotal_Cases_And_Deaths$State == \"New York\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/19440469) * 100000,\nTotal_Cases_And_Deaths$State == \"North Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/10611862) * 100000,\nTotal_Cases_And_Deaths$State == \"North Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/761723) * 100000,\nTotal_Cases_And_Deaths$State == \"Ohio\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/11747694) * 100000,\nTotal_Cases_And_Deaths$State == \"Oklahoma\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/3954821) * 100000,\nTotal_Cases_And_Deaths$State == \"Oregon\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/4301089) * 100000,\nTotal_Cases_And_Deaths$State == \"Pennsylvania\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/12820878) * 100000,\nTotal_Cases_And_Deaths$State == \"Rhode Island\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1056161) * 100000,\nTotal_Cases_And_Deaths$State == \"South Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5210095) * 100000,\nTotal_Cases_And_Deaths$State == \"South Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/903027) * 100000,\nTotal_Cases_And_Deaths$State == \"Tennessee\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/6897576) * 100000,\nTotal_Cases_And_Deaths$State == \"Texas\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/29472295) * 100000,\nTotal_Cases_And_Deaths$State == \"Utah\" ~ \n  (Total_Cases_And_Deaths$Total_State_Cases/3282115) * 100000,\nTotal_Cases_And_Deaths$State == \"Vermont\" ~ \n  (Total_Cases_And_Deaths$Total_State_Cases/628061) * 100000,\nTotal_Cases_And_Deaths$State == \"Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/8626207) * 100000,\nTotal_Cases_And_Deaths$State == \"Washington\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/7797095) * 100000,\nTotal_Cases_And_Deaths$State == \"West Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/1778070) * 100000,\nTotal_Cases_And_Deaths$State == \"Wisconsin\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/5851754) * 100000,\nTotal_Cases_And_Deaths$State == \"Wyoming\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/567025) * 100000,\n    TRUE ~ as.double(Total_Cases_And_Deaths$State))\n\n# Add column with per capity deaths (per 10,000 citizens).\nTotal_Cases_And_Deaths$Deaths_Per_10000 &lt;- case_when(\nTotal_Cases_And_Deaths$State == \"Alabama\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4908621) * 10000,\nTotal_Cases_And_Deaths$State == \"Alaska\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/734002) * 10000,\nTotal_Cases_And_Deaths$State == \"Arizona\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/7378494) * 10000,\nTotal_Cases_And_Deaths$State == \"Arkansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3038999) * 10000,\nTotal_Cases_And_Deaths$State == \"California\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/39937489) * 10000,\nTotal_Cases_And_Deaths$State == \"Colorado\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/5845526) * 10000,\nTotal_Cases_And_Deaths$State == \"Connecticut\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3563077) * 10000,\nTotal_Cases_And_Deaths$State == \"Delaware\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/982895) * 10000,\nTotal_Cases_And_Deaths$State == \"District of Columbia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/    720687) * 10000,\nTotal_Cases_And_Deaths$State == \"Florida\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/21992985) * 10000,\nTotal_Cases_And_Deaths$State == \"Georgia\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/10736059) * 10000,\nTotal_Cases_And_Deaths$State == \"Hawaii\" ~ \n  (Total_Cases_And_Deaths$Total_State_Deaths/1412687) * 10000,\nTotal_Cases_And_Deaths$State == \"Idaho\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1826156) * 10000,\nTotal_Cases_And_Deaths$State == \"Illinois\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/12659682) * 10000,\nTotal_Cases_And_Deaths$State == \"Indiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6745354) * 10000,\nTotal_Cases_And_Deaths$State == \"Iowa\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3179849) * 10000,\nTotal_Cases_And_Deaths$State == \"Kansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/2910357) * 10000,\nTotal_Cases_And_Deaths$State == \"Kentucky\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4499692) * 10000,\nTotal_Cases_And_Deaths$State == \"Louisiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4645184) * 10000,\nTotal_Cases_And_Deaths$State == \"Maine\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1345790) * 10000,\nTotal_Cases_And_Deaths$State == \"Maryland\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6083116) * 10000,\nTotal_Cases_And_Deaths$State == \"Massachusetts\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6976597) * 10000,\nTotal_Cases_And_Deaths$State == \"Michigan\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/10045029) * 10000,\nTotal_Cases_And_Deaths$State == \"Minnesota\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/5700671) * 10000,\nTotal_Cases_And_Deaths$State == \"Mississippi\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/2989260) * 10000,\nTotal_Cases_And_Deaths$State == \"Missouri\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6169270) * 10000,\nTotal_Cases_And_Deaths$State == \"Montana\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1086759) * 10000,\nTotal_Cases_And_Deaths$State == \"Nebraska\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1952570) * 10000,\nTotal_Cases_And_Deaths$State == \"Nevada\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3139658) * 10000,\nTotal_Cases_And_Deaths$State == \"New Hampshire\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1371246) * 10000,\nTotal_Cases_And_Deaths$State == \"New Jersey\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/8936574) * 10000,\nTotal_Cases_And_Deaths$State == \"New Mexico\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/2096640) * 10000,\nTotal_Cases_And_Deaths$State == \"New York\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/19440469) * 10000,\nTotal_Cases_And_Deaths$State == \"North Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/10611862) * 10000,\nTotal_Cases_And_Deaths$State == \"North Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/761723) * 10000,\nTotal_Cases_And_Deaths$State == \"Ohio\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/11747694) * 10000,\nTotal_Cases_And_Deaths$State == \"Oklahoma\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3954821) * 10000,\nTotal_Cases_And_Deaths$State == \"Oregon\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4301089) * 10000,\nTotal_Cases_And_Deaths$State == \"Pennsylvania\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/12820878) * 10000,\nTotal_Cases_And_Deaths$State == \"Rhode Island\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1056161) * 10000,\nTotal_Cases_And_Deaths$State == \"South Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/5210095) * 10000,\nTotal_Cases_And_Deaths$State == \"South Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/903027) * 10000,\nTotal_Cases_And_Deaths$State == \"Tennessee\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6897576) * 10000,\nTotal_Cases_And_Deaths$State == \"Texas\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/29472295) * 10000,\nTotal_Cases_And_Deaths$State == \"Utah\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3282115) * 10000,\nTotal_Cases_And_Deaths$State == \"Vermont\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/628061) * 10000,\nTotal_Cases_And_Deaths$State == \"Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/8626207) * 10000,\nTotal_Cases_And_Deaths$State == \"Washington\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/7797095) * 10000,\nTotal_Cases_And_Deaths$State == \"West Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1778070) * 10000,\nTotal_Cases_And_Deaths$State == \"Wisconsin\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/5851754) * 10000,\nTotal_Cases_And_Deaths$State == \"Wyoming\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/567025) * 10000,\n    TRUE ~ as.double(Total_Cases_And_Deaths$State))\n\n# Add column with per capity deaths (per 100,000 citizens).\nTotal_Cases_And_Deaths$Deaths_Per_100000 &lt;- case_when(\nTotal_Cases_And_Deaths$State == \"Alabama\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4908621) * 100000,\nTotal_Cases_And_Deaths$State == \"Alaska\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/734002) * 100000,\nTotal_Cases_And_Deaths$State == \"Arizona\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/7378494) * 100000,\nTotal_Cases_And_Deaths$State == \"Arkansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3038999) * 100000,\nTotal_Cases_And_Deaths$State == \"California\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/39937489) * 100000,\nTotal_Cases_And_Deaths$State == \"Colorado\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/5845526) * 100000,\nTotal_Cases_And_Deaths$State == \"Connecticut\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3563077) * 100000,\nTotal_Cases_And_Deaths$State == \"District of Columbia\" ~\n  (Total_Cases_And_Deaths$Total_State_Cases/    720687) * 100000,\nTotal_Cases_And_Deaths$State == \"Delaware\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/982895) * 100000,\nTotal_Cases_And_Deaths$State == \"Florida\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/21992985) * 100000,\nTotal_Cases_And_Deaths$State == \"Georgia\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/10736059) * 100000,\nTotal_Cases_And_Deaths$State == \"Hawaii\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1412687) * 100000,\nTotal_Cases_And_Deaths$State == \"Idaho\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1826156) * 100000,\nTotal_Cases_And_Deaths$State == \"Illinois\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/12659682) * 100000,\nTotal_Cases_And_Deaths$State == \"Indiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6745354) * 100000,\nTotal_Cases_And_Deaths$State == \"Iowa\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3179849) * 100000,\nTotal_Cases_And_Deaths$State == \"Kansas\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/2910357) * 100000,\nTotal_Cases_And_Deaths$State == \"Kentucky\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4499692) * 100000,\nTotal_Cases_And_Deaths$State == \"Louisiana\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4645184) * 100000,\nTotal_Cases_And_Deaths$State == \"Maine\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1345790) * 100000,\nTotal_Cases_And_Deaths$State == \"Maryland\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6083116) * 100000,\nTotal_Cases_And_Deaths$State == \"Massachusetts\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6976597) * 100000,\nTotal_Cases_And_Deaths$State == \"Michigan\" ~ \n  (Total_Cases_And_Deaths$Total_State_Deaths/10045029) * 100000,\nTotal_Cases_And_Deaths$State == \"Minnesota\" ~ \n  (Total_Cases_And_Deaths$Total_State_Deaths/5700671) * 100000,\nTotal_Cases_And_Deaths$State == \"Mississippi\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/2989260) * 100000,\nTotal_Cases_And_Deaths$State == \"Missouri\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6169270) * 100000,\nTotal_Cases_And_Deaths$State == \"Montana\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1086759) * 100000,\nTotal_Cases_And_Deaths$State == \"Nebraska\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1952570) * 100000,\nTotal_Cases_And_Deaths$State == \"Nevada\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3139658) * 100000,\nTotal_Cases_And_Deaths$State == \"New Hampshire\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1371246) * 100000,\nTotal_Cases_And_Deaths$State == \"New Jersey\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/8936574) * 100000,\nTotal_Cases_And_Deaths$State == \"New Mexico\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/2096640) * 100000,\nTotal_Cases_And_Deaths$State == \"New York\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/19440469) * 100000,\nTotal_Cases_And_Deaths$State == \"North Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/10611862) * 100000,\nTotal_Cases_And_Deaths$State == \"North Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/761723) * 100000,\nTotal_Cases_And_Deaths$State == \"Ohio\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/11747694) * 100000,\nTotal_Cases_And_Deaths$State == \"Oklahoma\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3954821) * 100000,\nTotal_Cases_And_Deaths$State == \"Oregon\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/4301089) * 100000,\nTotal_Cases_And_Deaths$State == \"Pennsylvania\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/12820878) * 100000,\nTotal_Cases_And_Deaths$State == \"Rhode Island\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1056161) * 100000,\nTotal_Cases_And_Deaths$State == \"South Carolina\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/5210095) * 100000,\nTotal_Cases_And_Deaths$State == \"South Dakota\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/903027) * 100000,\nTotal_Cases_And_Deaths$State == \"Tennessee\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/6897576) * 100000,\nTotal_Cases_And_Deaths$State == \"Texas\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/29472295) * 100000,\nTotal_Cases_And_Deaths$State == \"Utah\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/3282115) * 100000,\nTotal_Cases_And_Deaths$State == \"Vermont\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/628061) * 100000,\nTotal_Cases_And_Deaths$State == \"Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/8626207) * 100000,\nTotal_Cases_And_Deaths$State == \"Washington\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/7797095) * 100000,\nTotal_Cases_And_Deaths$State == \"West Virginia\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/1778070) * 100000,\nTotal_Cases_And_Deaths$State == \"Wisconsin\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/5851754) * 100000,\nTotal_Cases_And_Deaths$State == \"Wyoming\" ~\n  (Total_Cases_And_Deaths$Total_State_Deaths/567025) * 100000,\n    TRUE ~ as.double(Total_Cases_And_Deaths$State))\n\n# Remove Full State name column.\nTotal_Cases_And_Deaths2 &lt;- select(Total_Cases_And_Deaths, -(State))\n\n# Add State Abbreviations as State instead.\nTotal_Cases_And_Deaths2$State &lt;- Total_Cases_And_Deaths2$State_Abb\n\n# Remove redundant State_Abb column.\nTotal_Cases_And_Deaths2 &lt;- select(Total_Cases_And_Deaths2, -(State_Abb))\n\nhead(Total_Cases_And_Deaths2)\n\n    Week N Total_State_Cases Total_State_Deaths Total_US_Cases Total_US_Deaths\n1 Week 1 1               224                  0          45952             786\n2 Week 2 1              1001                 10         165282            4270\n3 Week 3 1              2079                 49         384281           14959\n4 Week 4 1              3870                 99         597539           30151\n5 Week 1 1                39                  0          45952             786\n6 Week 2 1               119                  3         165282            4270\n  Cases_Per_10000 Cases_Per_100000 Deaths_Per_10000 Deaths_Per_100000 State\n1       0.4563400         4.563400       0.00000000         0.0000000    AL\n2       2.0392693        20.392693       0.02037232         0.2037232    AL\n3       4.2354054        42.354054       0.09982437         0.9982437    AL\n4       7.8840880        78.840880       0.20168597         2.0168597    AL\n5       0.5313337         5.313337       0.00000000         0.0000000    AK\n6       1.6212490        16.212490       0.04087182         0.4087182    AK\n\n\nThe dataframe above shows just the first 6 rows. Each row contains the total COVID-19 cases and deaths (and some per capita information) in a specific State during one of the four Weeks we collected data. Later, I will plot some of this information for interests sake."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#survey-data-pre-processing",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#survey-data-pre-processing",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Survey Data Pre-processing",
    "text": "Survey Data Pre-processing\nBelow I read in the raw survey data from our study, and then combined/removed some data. The steps taken and data removed can be looked at more closely in the code chunk below. Some of the data are recoded as well and consists of variables that are used in the study but not used in this assignment.\nPlease note that the data are available on the osf page for this study: https://osf.io/8w6x2/\nThe file on the osf page is called “CVA_full.csv”.\n\n## Load dataset\nCVA &lt;- read.csv(\"C:/Users/STPI0560/Desktop/R Projects/Covid-19-Altruism-Bayesian-Analysis/data/raw/cva.csv\")\n\n#Remove one participant with repeated data\nCVA &lt;- filter(CVA, !(SubID == \"294b\"))\n\n## Questionnaire scoring \n\n# Self-reported altruism scale (SRA) Scoring\nCVA$SRA &lt;- (CVA$SRA1+CVA$SRA2+CVA$SRA3+CVA$SRA4+CVA$SRA5+CVA$SRA6+CVA$SRA7+CVA$SRA8+CVA$SRA9+CVA$SRA10+CVA$SRA11+CVA$SRA12+CVA$SRA13+CVA$SRA14+CVA$SRA15+CVA$SRA16+CVA$SRA17+CVA$SRA18+CVA$SRA19+CVA$SRA20)\n\n# Donations Scoring\nCVA$donations &lt;- CVA$SRA4+CVA$SRA5+CVA$SRA6+CVA$SRA7+CVA$SRA8\n\n# Risk Perception (RP) Scoring\nCVA$RP &lt;- CVA$RP1+CVA$RP2+CVA$RP3+CVA$RP4+CVA$RP5+CVA$RP6+CVA$RP7+CVA$RP8+CVA$RP9+CVA$RP10\n\n# Prosocial behavioral intentions scoring\nCVA$PBI &lt;- (CVA$PSI1+CVA$PSI2+CVA$PSI3+CVA$PSI4)/4\nCVA$PBI_S &lt;- (CVA$PSI2+CVA$PSI4)/2\nCVA$PBI_R &lt;- (CVA$PSI1+CVA$PSI3)/2\n\n## Perceived Stress scale Recoding\n# Reverse scored items (4, 5, 7, 8) for:\n\n# PS4\nCVA$PS4R &lt;- ifelse(CVA$PS4 == \"0\", 4,\n                    ifelse(CVA$PS4 == \"1\", 3,\n                           ifelse(CVA$PS4 == \"2\", 2,\n                                  ifelse(CVA$PS4 == \"3\", 1, 0))))\n\n#PS5\nCVA$PS5R &lt;- ifelse(CVA$PS5 == \"0\", 4,\n                    ifelse(CVA$PS5 == \"1\", 3,\n                           ifelse(CVA$PS5 == \"2\", 2,\n                                  ifelse(CVA$PS5 == \"3\", 1, 0))))\n# PS7 \nCVA$PS7R &lt;- ifelse(CVA$PS7 == \"0\", 4,\n                    ifelse(CVA$PS7 == \"1\", 3,\n                           ifelse(CVA$PS7 == \"2\", 2,\n                                  ifelse(CVA$PS7 == \"3\", 1, 0)))) \n# PS8\nCVA$PS8R &lt;- ifelse(CVA$PS8 == \"0\", 4,\n                    ifelse(CVA$PS8 == \"1\", 3,\n                           ifelse(CVA$PS8 == \"2\", 2,\n                                  ifelse(CVA$PS8 == \"3\", 1, 0)))) \n\n#Calculate PSS score\nCVA$PSS &lt;- CVA$PS1 + CVA$PS2 + CVA$PS3+ CVA$PS4R+ CVA$PS5R+ CVA$PS6+ CVA$PS7R+ CVA$PS8R+ CVA$PS9+ CVA$PS10\n\n \n## DASS-21\n# Depression: 3, 5, 10, 13, 16, 17, 21; Anxiety: 2, 4, 7, 9, 15, 19, 20; Stress: 1, 6, 8, 11, 12, 14, 18\nCVA$Depression &lt;- (CVA$D3 + CVA$D5+ CVA$D10+ CVA$D13+ CVA$D16+ CVA$D17+ CVA$D21)*2\nCVA$Anxiety &lt;- (CVA$D2 + CVA$D4+ CVA$D7+ CVA$D9+ CVA$D15+ CVA$D19+ CVA$D20)*2\nCVA$Stress &lt;- (CVA$D1 + CVA$D6+ CVA$D8+ CVA$D11+ CVA$D12+ CVA$D14+ CVA$D18)*2\n\n## Convert Timepoint to Week\nCVA$Week &lt;- ifelse(CVA$Timepoint == \"1\", \"Week 1\", \n                 ifelse(CVA$Timepoint == \"2\", \"Week 2\", \n                        ifelse(CVA$Timepoint == \"3\", \"Week 3\", \"Week 4\")))\n\n## Recode categorical variables\n# Employment Recoding\nCVA$Employment &lt;- ifelse(CVA$Emp == \"1\", \"Employed\",\n                         ifelse(CVA$Emp == \"2\", \"Unemployed\", \"Student\"))\n\n# Gender Recoding\nCVA$Gender2 &lt;- ifelse(CVA$Gender == \"1\", \"Male\", \n                      ifelse(CVA$Gender == \"2\", \"Female\", \"Other\"))\n\n# Education Recoding\nCVA$Education &lt;- ifelse(CVA$Edu == \"1\", \"High school or below\", \n                      ifelse(CVA$Gender == \"2\", \"University degree\", \"Graduate degree\"))\n\n## Remove unnecessary variables before joining with Covid-19 data\nCVA_new &lt;- select(CVA, -c(Timepoint, SRA1, SRA2, SRA3, SRA4, SRA5, SRA6, SRA7, SRA8, SRA9, SRA10, SRA11, SRA12, SRA13, SRA14, SRA15, SRA16,SRA17, SRA18, SRA19, SRA20, Emp, Edu, Gender, D1, D2, D3, D4, D5, D6, D7, D8, D9, D10, D11, D12, D13, D14, D15, D16, D17, D18, D19, D20, D21, PS1, PS2, PS3, PS4, PS5, PS6, PS7, PS8, PS9, PS10, PS4R, PS5R, PS7R, PS8R, PSI1, PSI2, PSI3, PSI4, RP1, RP2, RP3, RP4, RP5, RP6, RP7, RP8, RP9, RP10, VAR029, VAR033, VAR039, VAR050, VAR072, VAR083, VAR089, VAR094))\n\nNext, the COVID-19 data and survey data were combined to form a single dataframe. A bit later, I will plot both total cases per State, as well as self-reported altruism score per State over the course of each Week that data acquisition took place.\n\n# Join COVID survey and COVID U.S.A data.\nCVA_full &lt;- full_join(CVA_new, Total_Cases_And_Deaths2, by=NULL)\n\n# Remove NA Subjects\nCVA_full &lt;- filter(CVA_full, !(SubID == \"NA\"))\n\n# Display dataframe form analysis\nhead(CVA_full)\n\n  SubID  ID               ProlificID Age SEL Country State relative friend\n1   287 139 5d288c5cc3c61e001781c77d  19   2       2    AK       NA     NA\n2   384  85 5c802d06948f9c00017b3eb2  35   4       2    AK        3      3\n3    87 196 58af423e6590840001566cc7  32   3       2    AL       NA     NA\n4   198  48 5e72665f88152e1aecef02ff  20   4       2    AL       NA     NA\n5   240  90 5cbdc86714b3cb0001f035bb  38   2       2    AL       NA     NA\n6   333  34 5dab9bb23dd8f50015d256d5  46   1       2    AL        4      4\n  acquaint stranger ingroup outgroup ownn othern political identify help\n1       NA       NA      NA       NA    3      3         2        2    3\n2        1        1       2        1    1      3         1        1    1\n3       NA       NA      NA       NA    3      4         1        3    3\n4       NA       NA      NA       NA    2      3         1        3    1\n5       NA       NA      NA       NA    3      3         3        3    3\n6        4        4       4        4    3      3         3        3    3\n  socialm personalcom mainnews indnews personalexp alc1 alc2 alc3 control SRA\n1       4           3        5       1           1    4    5    3       4  47\n2       1           4        3       1           3    3    2    1       4  56\n3       5           5        5       3           1    4    3    3       4  45\n4       4           2        5       3           1    1    1    1       4  36\n5       3           3        1       1           1    1    1    1       4  38\n6       3           3        4       4           2    2    2    1       4  77\n  donations RP  PBI PBI_S PBI_R PSS Depression Anxiety Stress   Week Employment\n1        12 40 5.75     6   5.5  29         16       8     18 Week 2    Student\n2        14 49 5.75     5   6.5  25         10       2     16 Week 3 Unemployed\n3        10 28 7.00     7   7.0  12          0       0      2 Week 1   Employed\n4         9 58 4.75     4   5.5  28         14       0      8 Week 2    Student\n5        10 41 7.00     7   7.0  21         14       4     10 Week 2   Employed\n6        17 48 7.00     7   7.0  34         24      14     34 Week 3 Unemployed\n  Gender2            Education N Total_State_Cases Total_State_Deaths\n1  Female High school or below 1               119                  3\n2    Male      Graduate degree 1               193                  6\n3    Male High school or below 1               224                  0\n4  Female High school or below 1              1001                 10\n5  Female High school or below 1              1001                 10\n6  Female High school or below 1              2079                 49\n  Total_US_Cases Total_US_Deaths Cases_Per_10000 Cases_Per_100000\n1         165282            4270        1.621249         16.21249\n2         384281           14959        2.629421         26.29421\n3          45952             786        0.456340          4.56340\n4         165282            4270        2.039269         20.39269\n5         165282            4270        2.039269         20.39269\n6         384281           14959        4.235405         42.35405\n  Deaths_Per_10000 Deaths_Per_100000\n1       0.04087182         0.4087182\n2       0.08174365         0.8174365\n3       0.00000000         0.0000000\n4       0.02037232         0.2037232\n5       0.02037232         0.2037232\n6       0.09982437         0.9982437\n\n\nThe dataframe consists of 601 columns, and is what I will use for my Bayesian analysis. Each row is data collected from one participant, and also contains data about the number of cases and deaths for the week the data was collected for the State each participant comes from. There are many additional variables as well that are not important for the current analysis."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#measurement-scales",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#measurement-scales",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Measurement Scales",
    "text": "Measurement Scales\nTheir were 5 major scales that make up the dependent and independent measures used in the survey study, and they will briefly be discussed here. The Self-Reported Altruism Scale (SRA; Rushton et al., 1981) asks participants to rate the frequency with which they engaged in various altruistic acts. COVID-19 Risk Perception (RP) was assessed using a modified scale from Wise et al. (2020) and asks participants about their percieved risk of COVID-19 related infection and related personal/financial worries. Defensive emotions were indexed via the Percieved Stress Scale (PSS-10; Cohen et al., 1983) and the Depression Anxiety Stress Scale (DASS-21; Lovibond & Lovibond, 1995). The PSS-10 asks participants about experiences of stress related to unpredictable and uncontrollable events in their lives. The DASS-21 contains an anxiety scale related to acute anxiety/panic and autonomic arrousal. The DASS-21 also contains questons related to experienced depressive episodes, and these depression-related questions are also included as a seperate predictor in the model.\nBelow, I created some graphs to help visualise the distribution of scores for each measure. These are simply to help visualise how participants responded, and contain the responses from all 600 individuals.\n\n# Filter questionnaires only.\nCVA_Subset &lt;- CVA[, c(\"PS1\", \"PS2\", \"PS3\", \"PS4\", \"PS5\", \"PS6\", \"PS7\", \"PS8\", \"PS9\", \"PS10\", \"D1\", \"D2\", \"D4\", \"D6\", \"D7\", \"D8\", \"D9\", \"D11\", \"D12\", \"D14\", \"D15\", \"D18\", \"D19\", \"D20\", \"RP1\", \"RP2\", \"RP3\", \"RP4\", \"RP5\", \"RP6\", \"RP7\", \"RP8\", \"RP9\", \"RP10\", \"SRA1\", \"SRA2\", \"SRA3\", \"SRA4\", \"SRA5\", \"SRA6\", \"SRA7\", \"SRA8\", \"SRA9\", \"SRA10\", \"SRA11\", \"SRA12\", \"SRA13\", \"SRA14\", \"SRA15\", \"SRA16\", \"SRA17\", \"SRA18\", \"SRA19\", \"SRA20\")]\n\n#Reverse scored item PS4.\nCVA_Subset$PS4 &lt;- ifelse(CVA_Subset$PS4 == \"0\", 4,\n                    ifelse(CVA_Subset$PS4 == \"1\", 3,\n                           ifelse(CVA_Subset$PS4 == \"2\", 2,\n                                  ifelse(CVA_Subset$PS4 == \"3\", 1, 0))))\n\n#Reverse scored item PS5.\nCVA_Subset$PS5 &lt;- ifelse(CVA_Subset$PS5 == \"0\", 4,\n                    ifelse(CVA_Subset$PS5 == \"1\", 3,\n                           ifelse(CVA_Subset$PS5 == \"2\", 2,\n                                  ifelse(CVA_Subset$PS5 == \"3\", 1, 0))))\n \n#Reverse scored item PS7.\nCVA_Subset$PS7 &lt;- ifelse(CVA_Subset$PS7 == \"0\", 4,\n                    ifelse(CVA_Subset$PS7 == \"1\", 3,\n                           ifelse(CVA_Subset$PS7 == \"2\", 2,\n                                  ifelse(CVA_Subset$PS7 == \"3\", 1, 0)))) \n\n#Reverse scored item PS8.\nCVA_Subset$PS8 &lt;- ifelse(CVA_Subset$PS8 == \"0\", 4,\n                    ifelse(CVA_Subset$PS8 == \"1\", 3,\n                           ifelse(CVA_Subset$PS8 == \"2\", 2,\n                                  ifelse(CVA_Subset$PS8 == \"3\", 1, 0))))\n\nCVA_PS &lt;- CVA_Subset[, 1:10]\nCVA_D &lt;- CVA_Subset[, 11:24]\nCVA_RP &lt;- CVA_Subset[, 25:34]\nCVA_SRA &lt;- CVA_Subset[, 35:54]\n\nCVA_PS_Long &lt;- gather(CVA_PS, key = PS, value = \"Measure\", PS1:PS10)\nCVA_D_Long &lt;- gather(CVA_D, key = D, value = \"Measure\", D1:D20)\nCVA_RP_Long &lt;- gather(CVA_RP, key = RP, value = \"Measure\", RP1:RP10)\nCVA_SRA_Long &lt;- gather(CVA_SRA, key = SRA, value = \"Measure\", SRA1:SRA20)\n\n# PSS graphing dataframe.\nCVA_PS_Long$Question &lt;- case_when(\nCVA_PS_Long$PS == \"PS1\" ~ \"How often have you been upset because of something that happened unexpectedly?\",\nCVA_PS_Long$PS == \"PS2\" ~ \"how often have you felt you were unable to control the important things in your life?\",\nCVA_PS_Long$PS == \"PS3\" ~ \"how often have you felt nervous and stressed?\",\nCVA_PS_Long$PS == \"PS4\" ~ \"how often have you felt confident about your ability to handle your personal problems?\",\nCVA_PS_Long$PS == \"PS5\" ~ \"how often have you felt that things were going your way?\",\nCVA_PS_Long$PS == \"PS6\" ~ \"how often have you found that you could not cope with all the things you had to do?\",\nCVA_PS_Long$PS == \"PS7\" ~ \"how often have you been able to control irritations in your life?\",\nCVA_PS_Long$PS == \"PS8\" ~ \"how often have you felt that you were on top of things?\",\nCVA_PS_Long$PS == \"PS9\" ~ \"how often have you been angered because of things that were outside your control?\",\nCVA_PS_Long$PS == \"PS10\" ~ \"how often have you felt difficulties were pilling up so high that you could not overcome them?\",\nTRUE ~ as.character(CVA_PS_Long$PS)\n)\n\nCVA_PS_Long$Answer &lt;- case_when(\nCVA_PS_Long$Measure == 0 ~ \"1. Never\",\nCVA_PS_Long$Measure == 1 ~ \"2. Almost Never\",\nCVA_PS_Long$Measure == 2 ~ \"3. Sometimes\",\nCVA_PS_Long$Measure == 3 ~ \"4. Fairly Often\",\nCVA_PS_Long$Measure == 4 ~ \"5. Very Often\",\nTRUE ~ as.character(CVA_PS_Long$Measure)\n)\n\n# Depression graphing dataframe.\nCVA_D_Long$Question &lt;- case_when(\nCVA_D_Long$D == \"D1\" ~ \"I found it hard to wind down\",\nCVA_D_Long$D == \"D2\" ~ \"I was aware of dryness of my mouth\",\nCVA_D_Long$D == \"D4\" ~ \"I experienced breathing difficulty\",\nCVA_D_Long$D == \"D6\" ~ \"I tended to overreact to situations\",\nCVA_D_Long$D == \"D7\" ~ \"I experienced trembling\",\nCVA_D_Long$D == \"D8\" ~ \"I felt I was using a lot of nervous energy\",\nCVA_D_Long$D == \"D9\" ~ \"I was worried about situations where I might panic and make a fool of myself\",\nCVA_D_Long$D == \"D11\" ~ \"I found myself getting agitated\",\nCVA_D_Long$D == \"D12\" ~ \"I found it difficult to relax\",\nCVA_D_Long$D == \"D14\" ~ \"I was intolerant of anything that kept me from getting on with what I was doing\",\nCVA_D_Long$D == \"D15\" ~ \"I felt I was close to panic\",\nCVA_D_Long$D == \"D18\" ~ \"I felt I was rather touchy\",\nCVA_D_Long$D == \"D19\" ~ \"I was aware of the action of my heart in the absence of physical exertion\",\nCVA_D_Long$D == \"D20\" ~ \"I felt scared without any good reason\",\nTRUE ~ as.character(CVA_D_Long$D)\n)\n\nCVA_D_Long$Answer &lt;- case_when(\nCVA_D_Long$Measure == 0 ~ \"1. Did not apply to me at all\",\nCVA_D_Long$Measure == 1 ~ \"2. Applied to me to some degree\",\nCVA_D_Long$Measure == 2 ~ \"3. Applied to me to a great degree\",\nCVA_D_Long$Measure == 3 ~ \"4. Applied to me most of the time\",\nTRUE ~ as.character(CVA_D_Long$Measure)\n)\n\n# Risk Perception graphing dataframe.\nCVA_RP_Long$Question &lt;- case_when(\nCVA_RP_Long$RP == \"RP1\" ~ \"How likely do you think you are to catch the virus?\",\nCVA_RP_Long$RP == \"RP2\" ~ \"How badly do you think your health will be affected if you do catch the virus?\",\nCVA_RP_Long$RP == \"RP3\" ~ \"How badly do you think you will be affected economically if you specifically catch the virus?\",\nCVA_RP_Long$RP == \"RP4\" ~ \"How badly do you think you will be affected by the global effects of the virus?\",\nCVA_RP_Long$RP == \"RP5\" ~ \"How likely do you think it is that a loved one will become infected?\",\nCVA_RP_Long$RP == \"RP6\" ~ \"How likely do you think the average person in your neighbourhood is to become infected?\",\nCVA_RP_Long$RP == \"RP7\" ~ \" How likely do you think the average person in your state is to become infected?\",\nCVA_RP_Long$RP == \"RP8\" ~ \"How likely do you think the average person in your country is to become infected?\",\nCVA_RP_Long$RP == \"RP9\" ~ \"If you do contract the virus, how likely do you think it is that you will pass it on to someone else?\",\nCVA_RP_Long$RP == \"RP10\" ~ \"If you do contract the virus and pass it on to someone else, how badly do you think they would be affected?\",\nTRUE ~ as.character(CVA_RP_Long$RP)\n)\n\nCVA_RP_Long$Answer &lt;- case_when(\nCVA_RP_Long$Measure == 1 ~ \"1 - Min\",\nCVA_RP_Long$Measure == 2 ~ \"2\",\nCVA_RP_Long$Measure == 3 ~ \"3\",\nCVA_RP_Long$Measure == 4 ~ \"4\",\nCVA_RP_Long$Measure == 5 ~ \"5\",\nCVA_RP_Long$Measure == 6 ~ \"6\",\nCVA_RP_Long$Measure == 7 ~ \"7 - Max\",\nTRUE ~ as.character(CVA_RP_Long$Measure)\n)\n\n# Self-Reported Altruism graphing dataframe.\nCVA_SRA_Long$Question &lt;- case_when(\nCVA_SRA_Long$SRA == \"SRA1\" ~ \"I have helped push a stranger’s car out of the snow\",\nCVA_SRA_Long$SRA == \"SRA2\" ~ \"I have given directions to a stranger\",\nCVA_SRA_Long$SRA == \"SRA3\" ~ \"I have made change for a stranger\",\nCVA_SRA_Long$SRA == \"SRA4\" ~ \"I have given money to a charity\",\nCVA_SRA_Long$SRA == \"SRA5\" ~ \"I have given money to a stranger who needed it\",\nCVA_SRA_Long$SRA == \"SRA6\" ~ \"I have donated goods or clothes to a charity\",\nCVA_SRA_Long$SRA == \"SRA7\" ~ \"I have done volunteer work for a charity\",\nCVA_SRA_Long$SRA == \"SRA8\" ~ \"I have donated blood\",\nCVA_SRA_Long$SRA == \"SRA9\" ~ \"I have helped carry a stranger’s belongings\",\nCVA_SRA_Long$SRA == \"SRA10\" ~ \"I have delayed an elevator and held the door open for a stranger\",\nCVA_SRA_Long$SRA == \"SRA11\" ~ \"I have allowed someone to go ahead of me in a lineup\",\nCVA_SRA_Long$SRA == \"SRA12\" ~ \"I have given a stranger a lift in my car\",\nCVA_SRA_Long$SRA == \"SRA13\" ~ \"I have pointed out a clerk’s error in undercharging me for an item\",\nCVA_SRA_Long$SRA == \"SRA14\" ~ \"I have let a neighbour whom I didn’t know too well borrow an item of some value to me\",\nCVA_SRA_Long$SRA == \"SRA15\" ~ \"I have bought ‘charity” Christmas cards deliberately because I knew it was a good cause.\",\nCVA_SRA_Long$SRA == \"SRA16\" ~ \"I have helped a classmate who I did not know that well with a homework assignment when my knowledge was greater\",\nCVA_SRA_Long$SRA == \"SRA17\" ~ \"I have before being asked, voluntarily looked after a neighbour’s pets or children without being paid for it\",\nCVA_SRA_Long$SRA == \"SRA18\" ~ \"I have offered to help a handicapped or elderly stranger across a street\",\nCVA_SRA_Long$SRA == \"SRA19\" ~ \"I have offered my seat on a bus or train to a stranger who was standing\",\nCVA_SRA_Long$SRA == \"SRA20\" ~ \"I have helped an acquaintance to move households\",\nTRUE ~ as.character(CVA_SRA_Long$SRA)\n)\n\nCVA_SRA_Long$Answer &lt;- case_when(\nCVA_SRA_Long$Measure == 1 ~ \"1. Never\",\nCVA_SRA_Long$Measure == 2 ~ \"2. Once\",\nCVA_SRA_Long$Measure == 3 ~ \"3. More Than Once\",\nCVA_SRA_Long$Measure == 4 ~ \"4. Often\",\nCVA_SRA_Long$Measure == 5 ~ \"5. Very Often\",\nTRUE ~ as.character(CVA_SRA_Long$Measure)\n)\n\n\nggplot(CVA_PS_Long, aes(x = PS, fill = Answer, na.rm = TRUE)) +\n  geom_bar(position = position_fill(reverse = TRUE), na.rm = TRUE) +\n  theme(text = element_text(size=11)) +\n  coord_flip() + \n  xlab(\"\") +\n  ylab(\"Number of responses\")+\n  scale_fill_brewer(type = \"div\")+ \n  labs(fill = \"Answer\") +\n  ggtitle(\"Perceived Stress Scale\")\n\n\n\n\n\n\n\n\n\nggplot(CVA_D_Long, aes(x = D, fill = Answer, na.rm = TRUE)) +\n  geom_bar(position = position_fill(reverse = TRUE), na.rm = TRUE) +\n  theme(text = element_text(size=11)) +\n  coord_flip() + \n  xlab(\"\") +\n  ylab(\"Number of responses\")+\n  scale_fill_brewer(type = \"div\")+ \n  labs(fill = \"Answer\") +\n  ggtitle(\"Depression, Anxiety, and Stress Scale\")\n\n\n\n\n\n\n\n\nThis graph contains scores for both acute anxiety related questions, and depression related questions.\n\nggplot(CVA_RP_Long, aes(x = RP, fill = Answer, na.rm = TRUE)) +\n  geom_bar(position = position_fill(reverse = TRUE), na.rm = TRUE) +\n  theme(text = element_text(size=11)) +\n  coord_flip() + \n  xlab(\"\") +\n  ylab(\"Number of responses\")+\n  scale_fill_brewer(type = \"div\")+ \n  labs(fill = \"Answer\") +\n  ggtitle(\"COVID-19 Risk Perception Questionnaire\")\n\n\n\n\n\n\n\n\n\nggplot(CVA_SRA_Long, aes(x = SRA, fill = Answer, na.rm = TRUE)) +\n  geom_bar(position = position_fill(reverse = TRUE), na.rm = TRUE) +\n  theme(text = element_text(size=11)) +\n  coord_flip() + \n  xlab(\"\") +\n  ylab(\"Number of responses\")+\n  scale_fill_brewer(type = \"div\")+ \n  labs(fill = \"Answer\") +\n  ggtitle(\"Self-Reported Altruism Scale\")"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#distribution-of-main-variables",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#distribution-of-main-variables",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Distribution of main variables",
    "text": "Distribution of main variables\nI have included histograms of the dependent and independent variables to help get a sense of how they are distributed, and whether the variables could be considered for transformation.\n\n# Histograms main variables\n\nSRA_Hist &lt;- ggplot(data=CVA_full, aes(x=SRA, fill = Week)) + \n  geom_histogram(aes(y = ..density..), binwidth = 8)+\n  geom_density(alpha=.2, fill=\"white\")+\n  scale_fill_brewer(palette = \"Pastel2\")+\n  facet_wrap(~Week, nrow = 1)+\n  theme_pubr()+\n  ggtitle(\"Self-reported Altruism Scale\")+\n    theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n\nRP_Hist &lt;- ggplot(data=CVA_full, aes(x=RP, fill = Week)) + \n  geom_histogram(aes(y = ..density..), binwidth = 8)+\n  geom_density(alpha=.2, fill=\"white\")+\n  scale_fill_brewer(palette = \"Pastel2\")+\n  facet_wrap(~Week, nrow = 1)+\n  theme_pubr()+\n  ggtitle(\"Covid-19 Risk Perception\")+\n    theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n  \nPSS_Hist &lt;- ggplot(data=CVA_full, aes(x=PSS, fill = Week)) + \n  geom_histogram(aes(y = ..density..), binwidth = 8)+\n  geom_density(alpha=.2, fill=\"white\")+\n  scale_fill_brewer(palette = \"Pastel2\")+\n  facet_wrap(~Week, nrow = 1)+\n  theme_pubr()+\n  ggtitle(\"Perceived Stress Scale\")+\n    theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n \nAnxiety_Hist &lt;- ggplot(data=CVA_full, aes(x=Anxiety, fill = Week)) +\n  geom_histogram(aes(y = ..density..), binwidth = 8)+\n  geom_density(alpha=.2, fill=\"white\")+\n  scale_fill_brewer(palette = \"Pastel2\")+\n  facet_wrap(~Week, nrow = 1)+\n  theme_pubr()+\n  ggtitle(\"DASS-21 Anxiety\")+\n    theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n \nDepression_Hist &lt;- ggplot(data=CVA_full, aes(x=Depression, fill = Week)) + \n  geom_histogram(aes(y = ..density..), binwidth = 1)+\n  geom_density(alpha=.2, fill=\"white\")+\n  scale_fill_brewer(palette = \"Pastel2\")+\n  facet_wrap(~Week, nrow = 1)+\n  theme_pubr()+\n  ggtitle(\"Depression Scale\")+\n    theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n  \nggarrange(SRA_Hist, RP_Hist, PSS_Hist, Anxiety_Hist, Depression_Hist, nrow = 5)\n\n\n\n\n\n\n\n\nAs can be seen, some distributions are relatively normal (e.g. SRA, RP), while others are relatively skewed (Anxiety, Depression). Because the original study transformed all of these variables (justification provided later), that’s the strategy I will use too in order to keep my analysis consistent with the original."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-risk-perception-over-4-weeks",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-risk-perception-over-4-weeks",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Altruism and Risk Perception over 4 weeks",
    "text": "Altruism and Risk Perception over 4 weeks\nI first wanted to see whether Self-Reported Altrusim (SRA) was affected by the week it was collected. We hypothesized that SRA would increase linearly from week 1 to week 4 in accordance to increasing COVID-19 threat. As can be seen below however, this was not the case. While I considered making very simple models to assess how week predicted SRA and Risk Perception (RP), I will forego that here as these measures were clearly unaffected.\n\n#Plot main variables over 4 week period\n\n#create function to have mean estimate and sd in a point plot\ndata_summary &lt;- function(x) {\n   m &lt;- mean(x)\n   ymin &lt;- m-sd(x)/2\n   ymax &lt;- m+sd(x)/2\n   return(c(y=m,ymin=ymin,ymax=ymax))\n} \n\n#SRA&lt;- ggplot(CVA_full, aes(x=Week, y = SRA, color = Week)) +\n # geom_point()+\n #geom_jitter(aes(color = Week), width = 0.18)+\n #stat_summary(fun.data=data_summary, color=\"black\")+\n  #scale_color_brewer(palette = \"Pastel2\")+\n  #  theme_bw()+\n # theme(legend.position = \"none\")+\n  #  theme(text = element_text(size=18))+\n #  labs(y = \"SRA\")\n# SRA\n\n\n#create dataframes per variable to plot line connecting means\nSRA_week &lt;- group_by(CVA_full, Week) %&gt;% summarise(SRA = mean(SRA))\nSRA_week &lt;- data.frame(SRA_week)\n\nRP_week &lt;- group_by(CVA_full, Week) %&gt;% summarise(RP = mean(RP))\nRP_week &lt;- data.frame(RP_week)\n\nPSS_week &lt;- group_by(CVA_full, Week) %&gt;% summarise(PSS = mean(PSS))\nPSS_week &lt;- data.frame(PSS_week)\n\n# na.rm removes Pacific Northwest person. Otherwise, Week 2 estimate is NA in Cases_week dataframe.\nCases_week &lt;- group_by(CVA_full, Week) %&gt;% summarise(Total_State_Cases = mean(Total_State_Cases, na.rm = TRUE))\nCases_week &lt;- data.frame(Cases_week)\n \nCases_state &lt;- ggplot(Cases_week, aes(x=Week, y = Total_State_Cases, group = 1)) +\n  geom_line() +\n  geom_point() +\n  ylab(\"Total Cases\")+\n  theme_pubr()+\ntheme(text = element_text(size=18))\n\n RP_line &lt;- ggplot(CVA_full, aes(x=Week, y = RP, color = SubID)) +\n    geom_jitter(aes(color = Week), width = 0.18)+\n    geom_line(data=RP_week, aes(x=Week, y=RP, group = 1), color='black', size = 1, alpha = 0.5) + \n  geom_point(data=RP_week, aes(x=Week, y=RP), color='black', size = 2, alpha = 0.5) +\n   scale_color_brewer(palette = \"Pastel2\")+\n   theme_pubr()+\n    theme(legend.position = \"none\")+\n    theme(text = element_text(size=18))\n\nSRA_line &lt;- ggplot(CVA_full, aes(x=Week, y = SRA, color = SubID)) +\n    geom_jitter(aes(color = Week), width = 0.18)+\n    geom_line(data=SRA_week, aes(x=Week, y=SRA, group = 1), color='black', size = 1, alpha = 0.5) + \n  geom_point(data=SRA_week, aes(x=Week, y=SRA), color='black', size = 2, alpha = 0.5) +\n   scale_color_brewer(palette = \"Pastel2\")+\n   theme_pubr()+\n    theme(legend.position = \"none\")+\n    theme(text = element_text(size=18))\n \n ggarrange(RP_line, SRA_line, Cases_state)\n\n\n\n\n\n\n\n\nThe bottom graph shows that average COVID-19 cases clearly increased from week to week, exponentially so when we include the final week. Both COVID-19 Risk Perception and Self-Reported Altruism did not change over this time course. It may be the case that people really weren’t concerned enough with the magnitude of the threat of the virus, and this may be why Self-Reported Altruism did not change either. It may also be the case that we did not collect data for a long enough time, or that we picked a time when COVID-19 threat was too recent."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-risk-perception",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-risk-perception",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Altruism and Risk Perception",
    "text": "Altruism and Risk Perception\n\n## Plot the correlation between SRA and RP per week\n\nggplot(CVA_full, aes(x = RP, y = SRA, color = Week))+\n  geom_point()+\n  geom_smooth(method = lm, se=FALSE, color = \"black\", size = 0.5)+\n  facet_wrap(~Week, nrow = 1)+\n  scale_color_brewer(palette = \"Pastel2\")+\n  theme_pubr()+\n  theme(text = element_text(size=18))+\n  theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n\n\n\n\n\n\n\n\nIt looks like risk perception might be a good predictor of SRA as it linearly increases regardless of the week it was collected."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-perceived-stress-score-pss",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-perceived-stress-score-pss",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Altruism and Perceived Stress Score (PSS)",
    "text": "Altruism and Perceived Stress Score (PSS)\n\n## Plot the correlation between SRA and RP per week\n\nggplot(CVA_full, aes(x = PSS, y = SRA, color = Week))+\n  geom_point()+\n  geom_smooth(method = lm, se=FALSE, color = \"black\", size = 0.5)+\n  facet_wrap(~Week, nrow = 1)+\n  scale_color_brewer(palette = \"Pastel2\")+\n  theme_pubr()+\n  theme(text = element_text(size=18))+\n  theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n\n\n\n\n\n\n\n\nNo strong relationship between SRA and PSS is apparent here. PSS indexes more generalised stress, and we predicted that it would not be associated with SRA."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-dass-21-anxiety",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-dass-21-anxiety",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Altruism and DASS-21 Anxiety",
    "text": "Altruism and DASS-21 Anxiety\n\n## Plot the correlation between SRA and RP per week\n\nggplot(CVA_full, aes(x = Anxiety, y = SRA, color = Week))+\n  geom_point()+\n  geom_smooth(method = lm, se=FALSE, color = \"black\", size = 0.5)+\n  facet_wrap(~Week, nrow = 1)+\n  scale_color_brewer(palette = \"Pastel2\")+\n  theme_pubr()+\n  theme(text = element_text(size=18))+\n  theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n\n\n\n\n\n\n\n\nOur hypothesis was that anxiety about unpredictable/proximal threats would predict SRA, and that appears to be the case here."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-dass-21-depression",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#altruism-and-dass-21-depression",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Altruism and DASS-21 Depression",
    "text": "Altruism and DASS-21 Depression\n\n## Plot the correlation between SRA and RP per week\n\nggplot(CVA_full, aes(x = Depression, y = SRA, color = Week))+\n  geom_point()+\n  geom_smooth(method = lm, se=FALSE, color = \"black\", size = 0.5)+\n  facet_wrap(~Week, nrow = 1)+\n  scale_color_brewer(palette = \"Pastel2\")+\n  theme_pubr()+\n  theme(text = element_text(size=18))+\n  theme(legend.position = \"none\") +\n      theme(text = element_text(size=10))\n\n\n\n\n\n\n\n\nDepression does not seem to be associated with SRA."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#spatial-variability-state",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#spatial-variability-state",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Spatial Variability (State)?",
    "text": "Spatial Variability (State)?\nFor interest sake, I am including plots of total COVID cases, and SRA scores per State during the 4 weeks we collected data. This might give a better sense of the trajectory of the virus at this time.\n\nState &lt;- group_by(CVA_full, State, Week) %&gt;% summarise(mean = mean(Total_State_Cases))\nState &lt;- filter(State,  !(State == \"Pacific Northwest\"))\n  \n ggplot(State, aes(x=Week, y = mean, color = State, group)) +\n  #geom_jitter(aes(color = Week))+\n  geom_line(aes(x=Week, y = mean, group = State, color = State), size = 1) +\n  #geom_point(aes(x=Week, y = mean, group = State, color = State)) +\n   facet_wrap(~State)+\n  ylab(\"Total COVID-19 Cases\")+\n  theme_pubr()+\ntheme(text = element_text(size=8))+\n  theme(legend.position = \"none\")+\n  gghighlight()\n\n\n\n\n\n\n\nSRA_state &lt;- group_by(CVA_full, Week, State) %&gt;% summarise(SRA = mean(SRA))\nSRA_state &lt;- filter(SRA_state, !(State == \"Pacific Northwest\"))\n\nggplot(SRA_state, aes(x=Week, y = SRA, group = State, color = State)) +\n  #geom_jitter(aes(color = Week))+\n  geom_line(aes(x=Week, y = SRA, group = State, color = State), size = 1) +\n  #geom_point(aes(x=Week, y = mean, group = State, color = State)) +\n   facet_wrap(~State)+\n  ylab(\"Self-Reported Altruism\")+\n  theme_pubr()+\ntheme(text = element_text(size=8))+\n  theme(legend.position = \"none\")+\n  gghighlight()"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#centering-the-data",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#centering-the-data",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Centering the Data",
    "text": "Centering the Data\nDuring the original frequentist analysis, the models being tested were not converging. Centering all continuous data fixed the issue. In keeping with the spirit of the initial analysis, I have also opted to center all variable. I have also tested this model without centering, and the posterior estimates are very similar (albeit of a different scale). However, uncentered estimates come with a huge swath of divergent transitions anyway, and centering the data reduced this effect substantially. Therefore, it makes sense to center everything before building the model.\n\nCVA_Bayes$SRA_C &lt;- scale(CVA_Bayes$SRA, center = TRUE, scale = TRUE)\nCVA_Bayes$RP_C &lt;- scale(CVA_Bayes$RP, center = TRUE, scale = TRUE)\nCVA_Bayes$PSS_C &lt;- scale(CVA_Bayes$PSS, center = TRUE, scale = TRUE) # Distal\nCVA_Bayes$Anxiety_C &lt;- scale(CVA_Bayes$Anxiety, center = TRUE, scale = TRUE) # Proximal\nCVA_Bayes$Depression_C &lt;- scale(CVA_Bayes$Depression, center = TRUE, scale = TRUE)\nCVA_Bayes$Age_C &lt;- scale(CVA_Bayes$Age, center = TRUE, scale = TRUE)"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#a-note-about-my-priors",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#a-note-about-my-priors",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "A Note About My Priors",
    "text": "A Note About My Priors\nBecause I did not have a lot of prior knowledge about the measures used in the study, I wanted to keep my priors as uncertain as possible. Becuase we collected a lot of data, I figured that the priors would probably be drowned out by the likelihood anyway. Of special note, the prior I chose for correlations between elements in the variance-covarince matrix (lkj_corr_cholesky) is set to 1, meaning extreme correlations are more likely a priori. While it might make sense to set this higher (because maybe extreme correlations are actually less likely a priori), the examples I have seen that use this prior set it to 1, and it does not seem to change anything if I set it higher."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#initial-model-in-mathematical-notation",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#initial-model-in-mathematical-notation",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Initial Model In Mathematical Notation",
    "text": "Initial Model In Mathematical Notation\nHere is the initial model I fit. The model contains parameters for the intercept (SRA), Risk Perception (RP), Perceived Stress (PSS), Anxiety (A), and Depression (D). The model also has a random intercept accounting for SRA scores for each of the four weeks, and a random slope for each of the four predictors accounting for changes per each of the four weeks. State was also initially considered as a random effect, but the model became too complicated and would crash R if it was run. Removing the random effect of State fixed the issue. We would likely require more data from each State to use it in the model.\n\\[SRA_{i} \\sim Normal(\\mu_{i}, \\sigma)\\] \\[\\mu_{i} \\sim \\alpha_{week[i]} + \\beta RP_{week[i]}RP_{i} + \\beta PSS_{week[i]}PSS_{i} + \\beta A_{week[i]}A_{i} + \\beta D_{week[i]}D_{i}\\] \\[\\begin{bmatrix}\\alpha_{week}\\\\ \\beta_{week} \\end{bmatrix} \\sim MVNormal \\begin{pmatrix}\\begin{bmatrix}\\alpha\\\\ \\beta_{i} \\end{bmatrix} , S\n\\end{pmatrix}\\] \\[S = \\begin{pmatrix}\\sigma_{\\alpha} & 0\\\\\n0 & \\sigma_{\\beta}\n\\end{pmatrix}\nR \\begin{pmatrix}\\sigma_{\\alpha} & 0\\\\\n0 & \\sigma_{\\beta}\n\\end{pmatrix} \\] \\[\\alpha \\sim Normal(0, 1)\\] \\[\\beta \\sim Normal(0, 1)\\] \\[\\sigma \\sim Cauchy(0, 1)\\] \\[\\sigma_{\\alpha} \\sim Cauchy(0, 1)\\] \\[\\sigma_{\\beta} \\sim Cauchy(0, 1)\\] \\[R \\sim LKJ_{corr}(1)\\] \\[LKJ_{corr} = \\begin{bmatrix}1 & \\rho_{\\beta_{RP}\\alpha} & \\rho_{\\beta_{PSS}\\alpha} & \\rho_{\\beta_{A}\\alpha} & \\rho_{\\beta_{D}\\alpha}\\\\\n\\rho_{\\alpha\\beta_{RP}} & 1 & \\rho_{\\beta_{PSS}\\beta_{RP}} & \\rho_{\\beta_{A}\\beta_{RP}} & \\rho_{\\beta_{D}\\beta_{RP}}\\\\\n\\rho_{\\alpha\\beta_{PSS}} & \\rho_{\\beta_{RP}\\beta_{PSS}} & 1 & \\rho_{\\beta_{A}\\beta_{PSS}} & \\rho_{\\beta_{D}\\beta_{PSS}}\\\\\n\\rho_{\\alpha\\beta_{A}} & \\rho_{\\beta_{RP}\\beta_{A}} & \\rho_{\\beta_{PSS}\\beta_{A}} & 1 & \\rho_{\\beta_{D}\\beta_{A}}\\\\\n\\rho_{\\alpha\\beta_{D}} & \\rho_{\\beta_{RP}\\beta_{D}} & \\rho_{\\beta_{PSS}\\beta_{D}} & \\rho_{\\beta_{A}\\beta_{D}} & 1\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#initial-statistical-model",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#initial-statistical-model",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Initial Statistical Model",
    "text": "Initial Statistical Model\nBelow I tested the main model used in the original frequestist analysis. Unseen here, my initial model had a very large proportion of samples flagged as divergent. Therefore, I increased the adapt_delta parameter, starting at 0.96. This parameter affects the step-size of the Hamiltonian MCMC algorithm, and increasing it is akin to decreasing the size of a step taken by the algorithm during optimisation. While this action lowers the number of transitions that result in the energy of the Hamiltonian system not being constant (i.e. divergency), it can also make it less likely that the algorithm samples from harder to reach places of the posterior (like the tails for instance). Therefore, I increased the number of samples to 25,000 and the number of chains to 4. Once I identify the best adapt_delta configuration, the samples will be increased for the final model, and only one chain will be used.\n\n# Full COVID model.\nCOVID_Bayes_0.96 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(1), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_0.96.rds\",\niter = 25000, warmup = 2000, cores = 4, chains =4, seed = 123, control = list(adapt_delta = 0.96))\n\nThere are 525 divergent transitions with adapt_delta = 0.96, and I wonder if this can be improved.\n\nCOVID_Bayes_m1_Summary &lt;- tidyMCMC(COVID_Bayes_0.96$fit, conf.int = TRUE, rhat = TRUE, ess = TRUE, conf.level = 0.95,\n              conf.method = \"quantile\", pars = c(\"b_Intercept\", \"b_RP_C\", \"b_PSS_C\",\n                                                 \"b_Anxiety_C\", \"b_Depression_C\", \"sigma\"))\n\nCOVID_Bayes_m1_Summary\n\n# A tibble: 6 × 7\n  term           estimate std.error conf.low conf.high  rhat    ess\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;\n1 b_Intercept    -0.00368    0.156   -0.321      0.323  1.00  23002\n2 b_RP_C          0.169      0.0980  -0.0180     0.365  1.00  22234\n3 b_PSS_C        -0.145      0.140   -0.410      0.131  1.00   6291\n4 b_Anxiety_C     0.306      0.0995   0.108      0.494  1.00  23620\n5 b_Depression_C -0.168      0.131   -0.418      0.105  1.00  19236\n6 sigma           0.949      0.0278   0.897      1.01   1.00 101914\n\n\nI am plotting some of the output, but am not yet interested in the parameter estimates. I only want to look at statistics related to the Hamiltonian MCMC algorithm right now. The rhat values with adapt_delta = 0.96 are all 1, and the ess seems decent. Still, I tried adapt_delta 0.97, 0.98, and 0.99 to see how it affected divergency.\n\n# Full COVID model: adaptive delta = 0.97.\nCOVID_Bayes_0.97 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(1), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_0.97.rds\",\niter = 25000, warmup = 2000, cores = 4, chains =4, seed = 123, control = list(adapt_delta = 0.97))\n\n\n# Full COVID model: adaptive delta = 0.98.\nCOVID_Bayes_0.98 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(1), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_0.98.rds\",\niter = 25000, warmup = 2000, cores = 4, chains =4, seed = 123, control = list(adapt_delta = 0.98))\n\n\n# Full COVID model: adaptive delta = 0.99.\nCOVID_Bayes_0.99 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(1), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_0.99.rds\",\niter = 25000, warmup = 2000, cores = 4, chains =4, seed = 123, control = list(adapt_delta = 0.99))\n\nIn the end, adapt_delta = 0.99 produced the lowest number of divergent transitions. This to me seems adaquate. While I have not shown it here, the estimates and credibility intervals are esentially identical for all adapt_delta values I tried. Before running the model again with a larger sample, I decided to see if there were any patterns to the divergencies that could be addressed when adapt_delta = 0.99 was used."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#divergency",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#divergency",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Divergency",
    "text": "Divergency\nFirst, The model estimates 42 parameters to account for variances and correlations between random effects. However, 1 parameter (__lp) is of a substantially different scale than the others. All other parameters are between -1 and +1, while __lp is near 1000. Therefore, visualising divergency is not possible with this parameter, so I will remove only it and look at the output.\n\nposterior_cp &lt;- as.array(COVID_Bayes_0.99)\nposterior_cp &lt;- posterior_cp[, , 1:41]\nnp_cp &lt;- nuts_params(COVID_Bayes_0.99)\n#head(np_cp)\nlp_cp &lt;- log_posterior(COVID_Bayes_0.99)\n#head(lp_cp)\n\n\ncolor_scheme_set(\"darkgray\")\nmcmc_parcoord(posterior_cp, np = np_cp)\n\n\n\n\n\n\n\n\nEach line shows the joint parameter estimate for a single transition. Red lines show the occurances of divergency somewhere in that transition, meaning on that particular transition there was a divergent sample somewhere. If 1 parameter is causing pathological behaviour, it should be clear such that the red lines concentrate around a specific value for that parameter. Here it is less clear, and may not matter much since the number of divergent transitions is small and they are discarded from the model estimate anyway. Additionally, since most divergent transitions occur closer to the peak of each posterior, they may be caused by the peaks themselves being too steep. For the sake of completeness however, I decided to look only at the fixed parameters entered into the model.\n\nposterior_cp_Fixed &lt;- as.array(COVID_Bayes_0.99)\n\n# Select all chains, but only for the intercept, RP, PSS, Anxiety, and Depression.\nposterior_cp_Fixed &lt;- posterior_cp_Fixed[, , 1:5]\n\n\ncolor_scheme_set(\"darkgray\")\nmcmc_parcoord(posterior_cp_Fixed, np = np_cp)\n\n\n\n\n\n\n\n\nNone of the divergent transitions seem very concentrated to any 1 parameter. Because I have decent control over these parameters via priors and data transformation, I can make some adjustments that could help the model. I cannot do much about the random effects parameters though, even if they are pathological. While the fixed affects do not seem to get stuck at any particularly difficult areas of the posterior, I stil wonder whether any of the specific chains contributed to this effect. Below I plot the marginal densities to assess this this.\n\nmcmc_pairs(posterior_cp_Fixed, np = np_cp)\n\n\n\n\n\n\n\n\nThis plot displays the marginal probability of each coefficient averaged over all other coefficient on the diagonal. The off-diagonal plots represent the joint probability of two parameters. Additionally, each off-diagnal side (left and right) displays 2 of the 4 chains I used. Usually these are mirror images of each other, but here they represent different chain estimates of the joint posterior. Each red point represents a sample taken during a divergent transition. Again, these do not concentrate anywhere on any posterior space, so the divergent transitions do not seem affected by one parameter."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#trace-plot-diagnostics",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#trace-plot-diagnostics",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Trace-plot diagnostics",
    "text": "Trace-plot diagnostics\nThis next diagnostic not only displays the usual MCMC traceplot, it also highlights each instance of a divergent transition. Each red tick on the x-axis below indicates the timepoint a divergent transition took place.\n\ncolor_scheme_set(\"mix-brightblue-gray\")\nmcmc_trace(posterior_cp_Fixed,, np = np_cp) +\n  xlab(\"Post-warmup iteration\")\n\n\n\n\n\n\n\n\nIt does not appear that the chains are divergent because they get hung up in a difficult part of the posterior, at least for the predictors. If they did, we would expect to see divergent transitions clustered at particular timepoints."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#mcmc-nuts-divergence",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#mcmc-nuts-divergence",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "MCMC Nuts Divergence",
    "text": "MCMC Nuts Divergence\nIt is also possible to assess how divergency interacts with the model at a global (full model) scale.\n\ncolor_scheme_set(\"red\")\nmcmc_nuts_divergence(np_cp, lp_cp)\n\n\n\n\n\n\n\n\nThe top panel shows the distribution of the log-posterior when there was no divergence (left) compared to when there was divergence (right). Divergence can mean that some part of the posterior is not being explored, and that does seem to be the case with the right plot. The bottom plot shows the NUTS acceptance statistic, which is essentially the same thing. However, based on examples I have seen, these do not indicate that there is much of an issue with unexplored areas of the posterior, just that divergent transitions occur in more concentrated areas of the posterior."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#mcmc-summary-diagnostics",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#mcmc-summary-diagnostics",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "MCMC Summary Diagnostics",
    "text": "MCMC Summary Diagnostics\nWhile the rhat and ess seem good based on what I learned about them in Statistical Rethinking, I decided to apply some further visualisations to ensure the posteriors are well-estimated."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#rhat-visualisation",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#rhat-visualisation",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Rhat Visualisation",
    "text": "Rhat Visualisation\nI find that visualising rhat values for all parameter estimates gives a much clearer sense of whether the chains converged or not, and can help determine whether the chains reached equalibrium.\n\nrhats_fixed &lt;- rhat(COVID_Bayes_0.99, pars = c(\"b_Intercept\", \"b_RP_C\", \"b_PSS_C\", \"b_Anxiety_C\", \"b_Depression_C\"))\n\nrhats_all &lt;- rhat(COVID_Bayes_0.99)\n\n\ncolor_scheme_set(\"brightblue\") # see help(\"color_scheme_set\")\nmcmc_rhat(rhats_fixed) + yaxis_text(hjust = 1)\n\n\n\n\n\n\n\nmcmc_rhat(rhats_all) + yaxis_text(hjust = 1)\n\n\n\n\n\n\n\n\nI have plotted rhat values for only the fixed effects, as well as for all parameter values. It is obvious that all 4 chains reached equalibrium, and thus, the algorithm converged on the target distribution. If it did not, the chains would contain unequal variances between them, and the rhat value would be different from 1. This makes me confident that the final model that uses only 1 chain will converge."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#effective-sample-size-ratio-test",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#effective-sample-size-ratio-test",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Effective Sample Size Ratio Test",
    "text": "Effective Sample Size Ratio Test\nESS tells us the number of independent samples our samples drawn from a non-random dependent sampler are worth. The central limit theorum bounds uncertainty to the sample size, provided the sampling procedure is random. Hamiltonian MCMC conditions each sample on the previous sample, and so the sampling procedure is not truly random. The less random a sampling procedure is, the less independent samples your depedently drawn samples will be worth. Therefore, looking at the ration between ess and samples drawn is a good diagnistic to assess how bad autocorrelation (non-randomness) is. This is important here, as random effects models can greatly increase autocorrelation.\n\nratios_cp_fixed &lt;- neff_ratio(COVID_Bayes_0.99, pars = c(\"b_Intercept\", \"b_RP_C\", \"b_PSS_C\", \"b_Anxiety_C\", \"b_Depression_C\"))\nratios_cp_all &lt;- neff_ratio(COVID_Bayes_0.99)\nmcmc_neff(ratios_cp_fixed, size = 2) + yaxis_text(hjust = 1)\n\n\n\n\n\n\n\nmcmc_neff(ratios_cp_all, size = 2) + yaxis_text(hjust = 1)\n\n\n\n\n\n\n\n\nOverall, these seem ok. I remember Andrew Gelman saying that anything over .1 is fine, although larger ratios are obviously better as it indicates the process is closer to random. This may contribute to autocorrelation (or may be the result of it), but I will look at that in the final model as autocorrelation can be assessed from a single chain."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#final-model",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#final-model",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Final Model",
    "text": "Final Model\nHere I will follow the advice from Statistical Rethinking and only run a single chain for a larger number of samples. I will also not perform the diagnostics above (unless there are serious issues).\n\n# Full COVID model: adaptive delta = 0.99.\nCOVID_Bayes_Model &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(1), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_Model.rds\",\niter = 40000, warmup = 2000, cores = 4, chains =1, seed = 123, control = list(adapt_delta = 0.99))\n\nThere appears to be only 34 divergent transitions out of 40,000 samples, which seems excellent."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#autocorrelation",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#autocorrelation",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nThe lower rhat ratio displayed earlier might indicate some level of autocorrelation that is unwanted. Because each sample from each chain is dependent upon the previous sample, each second draw from the sampling algorithm will obviously be more correlated with the first draw than the third draw is with the first. This should drop off considerably as the distance increases.\n\nmcmc_acf(posterior_cp, pars = c(\"b_Intercept\", \"b_RP_C\", \"b_PSS_C\", \"b_Anxiety_C\", \"b_Depression_C\"), lags = 10)\n\n\n\n\n\n\n\n\nEach row above is a chain, and each column a parameter. Each sample is perfectly correlated with itself (1), and then about 50% correlated with the second dependent draw. This seems to drop off to nearly zero at the 9th or 10th lagged sample. While this is not perfect, I believe that with a large number of samples this should not effect my estimates too much."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#non-hypothesized-predictors-model",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#non-hypothesized-predictors-model",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Non-hypothesized predictors model",
    "text": "Non-hypothesized predictors model\nNext, I wanted to create a model with the other predictors that were collected but not hypothesized: age, gender, employment, and financial situation. My strategy was to fit them in their own model as predictors of SRA, and pull out the predictors that seemed to predict it well. I kept adapt_delta at 0.99 as this worked well in the initial model.\n\n# Full COVID model: adaptive delta = 0.99.\nCOVID_Bayes_2nd_0.99 &lt;- brm(SRA_C~ 1 + Age_C + Gender2 +  Employment  + financ + (1 + Age_C + Gender2 +  Employment  + financ|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_2nd_0.99.rds\",\niter = 25000, warmup = 2000, cores = 4, chains =4, seed = 123, control = list(adapt_delta = 0.99))\n\nAs this is exploratory, I decided to let brm find most priors for me during optimisation. There were also 320 divergent transitions, but because these are discarded anyway, I decided to just see which parameter estimates might be strong enough to warrent inclusion in the final model.\n\n# Create object-cue Markov array for each chain.\nposterior_COVID &lt;- as.array(COVID_Bayes_2nd_0.99$fit)\n\n\ncolor_scheme_set(\"red\")\nmcmc_intervals(  posterior_COVID,\n  pars = c(\"b_Age_C\", \"b_Gender2Male\", \"b_Gender2Other\", \"b_EmploymentStudent\",\"b_EmploymentUnemployed\",\"b_financJustmeetbasicexpenses\",\"b_financLivecomfortably\",\"b_financMeetneedswithsomeleft\", \"sigma\"),\n  prob = 0.95, # 80% intervals\n  prob_outer = 0.99, # 99%\n  point_est = \"mean\"\n)\n\n\n\n\n\n\n\n\nIt is hard to tell here, but age does not overlap with zero. All other parameter estimates seem far more uncertain. Therefore, I decided to include age only in the final model as well."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#final-model-2",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#final-model-2",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Final Model 2",
    "text": "Final Model 2\nUnfortunately, the model became too complex with age as a fixed and random effect. It kept crashing R, and without a more powerful computer to test the model on, I was forced to remove the random effect portion of the age predictor.\n\n# Full COVID model: adaptive delta = 0.99.\nCOVID_Bayes_Model_2 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(5), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_Model_2.rds\",\niter = 25000, warmup = 2000, cores = 4, chains =4, seed = 123, control = list(adapt_delta = 0.99))\n\nWith 320 divergent transitions, I decided to increase adapt_delta beyond 0.99 to see if that reduces divergencies.\n\n# Full COVID model: adaptive delta = 0.99.\nCOVID_Bayes_Model_2 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(5), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_Model_2.rds\",\niter = 25000, warmup = 2000, cores = 4, chains =4, seed = 123, control = list(adapt_delta = 0.995))\n\nThis seems to have fixed the issue. I’m not going to analyse the source of these divergent transitions as they are low enough to likely be caused by random sharp features of the multivariate parameter space. Below I run the final single-chain model.\n\n# Full COVID model: adaptive delta = 0.99.\nCOVID_Bayes_Model_2_Final &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C + (1+ RP_C + PSS_C + Anxiety_C + Depression_C|Week), data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = Intercept),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"RP_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"PSS_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Anxiety_C\"),\nprior(cauchy(0,2), class = sd, group = Week, coef = \"Depression_C\"),\nprior(cauchy(0,2), class = sd),\nprior(lkj_corr_cholesky(5), class = cor),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/COVID_Bayes_Model_2.rds\",\niter = 40000, warmup = 2000, cores = 4, chains =1, seed = 123, control = list(adapt_delta = 0.995))\n\n\nCOVID_Bayes_Model_Summary &lt;- tidyMCMC(COVID_Bayes_Model_2_Final$fit, conf.int = TRUE, rhat = TRUE, ess = TRUE, conf.level = 0.95,\n              conf.method = \"quantile\", pars = c(\"b_Intercept\", \"b_RP_C\", \"b_PSS_C\",\n                                                 \"b_Anxiety_C\", \"b_Depression_C\", \"b_Age_C\", \"sigma\"))\n\nCOVID_Bayes_Model_Summary\n\n# A tibble: 7 × 7\n  term           estimate std.error conf.low conf.high  rhat   ess\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 b_Intercept    -0.00282    0.158   -0.308     0.313   1.00  8165\n2 b_RP_C          0.150      0.0944  -0.0309    0.330   1.00  8819\n3 b_PSS_C        -0.118      0.106   -0.317     0.0895  1.00 10225\n4 b_Anxiety_C     0.304      0.0932   0.114     0.473   1.00  9984\n5 b_Depression_C -0.136      0.116   -0.353     0.0869  1.00 10093\n6 b_Age_C         0.282      0.0383   0.207     0.357   1.00 32508\n7 sigma           0.908      0.0266   0.858     0.962   1.00 41679\n\n\nWe have very high ess, and rhat values close to 1. Each parameter estimate has changed a bit, but all interpretations remain the same. Age also seems to be an excellent predictor of SRA.\n\n# Create object-cue Markov array for each chain.\nposterior_COVID &lt;- as.array(COVID_Bayes_Model_2_Final$fit)\n\n\ncolor_scheme_set(\"red\")\nmcmc_intervals(  posterior_COVID,\n  pars = c(\"b_RP_C\", \"b_PSS_C\", \"b_Anxiety_C\", \"b_Depression_C\",\"b_Age_C\", \"sigma\"),\n  prob = 0.95, # 80% intervals\n  prob_outer = 0.99, # 99%\n  point_est = \"mean\"\n)"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#mcmc-traceplots",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#mcmc-traceplots",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "MCMC Traceplots",
    "text": "MCMC Traceplots\n\ncolor_scheme_set(\"mix-blue-red\")\nmcmc_trace(COVID_Bayes_Model_2_Final, pars = c(\"b_Intercept\", \"b_RP_C\", \"b_PSS_C\", \"b_Anxiety_C\", \"b_Depression_C\", \"b_Age_C\", \"sigma\"), facet_args = list(ncol = 1, strip.position = \"left\"))\n\n\n\n\n\n\n\n\nI plotted the traceplots to ensure the chains look like they are sampling from the entire posterior parameter space. According to the plots, this seems to be the case."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#posterior-predictive-check-ppc",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#posterior-predictive-check-ppc",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Posterior Predictive Check (PPC)",
    "text": "Posterior Predictive Check (PPC)\nMcElreath introduces PPC’s early in Statistical Rethinking, and I decided to test them out myself on my final model. Briefly, if a model is a good fit for the data, then we should be able to recover the data that generated the model via random sampling of likelihood distributions generated by parameters randomly sampled from the posterior. By drawing random samples from the posterior and then drawing a random data point from a posterior parameter generated likelihood model, a histogram of posterior-generated data should match the actual distribution of the data that generated the posterior model.\n\ny &lt;- as.numeric(CVA_Bayes$SRA_C)\nyrep &lt;- posterior_predict(COVID_Bayes_Model_2_Final, draws = 500)\n\n\ncolor_scheme_set(\"brightblue\")\nppc_stat(y, yrep, stat = \"mean\")\n\n\n\n\n\n\n\n\nFirst, we can see a histogram of all draws from the likelihood distributions generated by each random parameter draw from the posterior model. I have overlayed the mean value from the dependent variable (Self-Reported Altruism score). It’s clear that the mean lines up near perfectly with the peak of the histogram.\n\ncolor_scheme_set(\"brightblue\")\nppc_dens_overlay(y, yrep[1:50, ])\n\n\n\n\n\n\n\n\nHere I have the entire distribution of self-reported altruism scores in dark blue, with randomly created likelihood distributions generated using random parameter draws from the posterior (light blue). Again, the data are recovered quite nicely from the posterior and match the actual distribution of SRA scores."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#model-comparison",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#model-comparison",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Model Comparison",
    "text": "Model Comparison\nTo use a model comparison approach learned in Statistical Rethinking, I decided to see whether the age model was actually better than the model without age. Below I tried to recreated the waic estimates and graphs from Statistical Rethinking as closely as possible.\n\nCOVID_Bayes_Model &lt;- add_criterion(COVID_Bayes_Model, \"waic\")\nCOVID_Bayes_Model_2_Final &lt;- add_criterion(COVID_Bayes_Model_2_Final, \"waic\")\n\n# compare the WAIC estimates\nw &lt;- loo_compare(COVID_Bayes_Model, COVID_Bayes_Model_2_Final,\n                 criterion = \"waic\")\n\ncbind(waic_diff = w[, 1] * -2,\n      se        = w[, 2] * 2)\n\n                          waic_diff       se\nCOVID_Bayes_Model_2_Final   0.00000  0.00000\nCOVID_Bayes_Model          51.82602 15.07536\n\nmodel_weights(COVID_Bayes_Model, COVID_Bayes_Model_2_Final, \n              weights = \"waic\") %&gt;%\n  as_tibble() %&gt;% \n  rename(weight = value) %&gt;% \n  mutate(model  = c(\"COVID_Bayes_Model\", \"COVID_Bayes_Model_2_Final\"),\n         weight = weight %&gt;% round(digits = 2)) %&gt;% \n  select(model, weight) %&gt;% \n  arrange(desc(weight)) %&gt;% \n  knitr::kable()\n\n\n\n\nmodel\nweight\n\n\n\n\nCOVID_Bayes_Model_2_Final\n1\n\n\nCOVID_Bayes_Model\n0\n\n\n\n\n\nAs can be seen, the model that includes age has 100% of the weighting in its favor.\n\nw[, 7:8] %&gt;% \n  data.frame() %&gt;% \n  rownames_to_column(var = \"model_name\") %&gt;% \n  \n  ggplot(aes(x    = model_name, \n             y    = waic, \n             ymin = waic - se_waic, \n             ymax = waic + se_waic)) +\n  geom_pointrange(shape = 21, color = carto_pal(7, \"BurgYl\")[7], fill = carto_pal(7, \"BurgYl\")[5]) +\n  coord_flip() +\n  labs(x = NULL, y = NULL,\n       title = \"My custom WAIC plot\") +\n  theme_classic() +\n  theme(text             = element_text(family = \"Courier\"),\n        axis.ticks.y     = element_blank(),\n        panel.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[3], 1/4)))\n\n\n\n\n\n\n\n\nHere, the waic scores again suggest that the second model is superior. To me this provides some evidence that this model should be used."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#counterfactual-plots",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#counterfactual-plots",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Counterfactual Plots",
    "text": "Counterfactual Plots\nWhile I might not necessarily inlude these in a normal Bayesian analysis, I decided to try and recreate the counterfactual plots Statistical Rethinking liberally uses. Because I used the brm package, I had to use a slightly different approach.\n\nnd &lt;- \n  tibble(RP_C = seq(from = -5, to = 3, length.out = 601),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n         Anxiety_C = mean(CVA_Bayes$Anxiety_C),\n          Depression_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\n\n    # we're finally ready to plot\n  RP_Week1 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 1\", ], aes(x = RP_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1\",\n       y = \"Self-Reported Altruism\",\n       x = \"RP Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n  RP_Week2 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 2\", ], aes(x = RP_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 2\",\n       y = \"Self-Reported Altruism\",\n       x = \"RP Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n  \n    RP_Week3 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 3\", ], aes(x = RP_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 3\",\n       y = \"Self-Reported Altruism\",\n       x = \"RP Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n    \n  RP_Week4 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 4\", ], aes(x = RP_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 4\",\n       y = \"Self-Reported Altruism\",\n       x = \"RP Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\nRisk Perception Per Week\n\nRP_Plot &lt;- ggarrange(RP_Week1, RP_Week2, RP_Week3, RP_Week4, ncol = 2, nrow = 2)\nannotate_figure(RP_Plot,\n                top = text_grob(\"Risk Perception\", color = \"black\", face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\nnd &lt;- \n  tibble(PSS_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         Anxiety_C = mean(CVA_Bayes$Anxiety_C),\n          Depression_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\n\n    # we're finally ready to plot\n  PSS_Week1 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 1\", ], aes(x = PSS_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1\",\n       y = \"Self-Reported Altruism\",\n       x = \"PSS Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n  PSS_Week2 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 2\", ], aes(x = PSS_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 2\",\n       y = \"Self-Reported Altruism\",\n       x = \"PSS Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n  \n  PSS_Week3 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 3\", ], aes(x = PSS_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 3\",\n       y = \"Self-Reported Altruism\",\n       x = \"PSS Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n    \n  PSS_Week4 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 4\", ], aes(x = PSS_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 4\",\n       y = \"Self-Reported Altruism\",\n       x = \"PSS Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\nPerceived Stress Per Week\n\nPSS_Plot &lt;- ggarrange(PSS_Week1, PSS_Week2, PSS_Week3, PSS_Week4, ncol = 2, nrow = 2)\nannotate_figure(PSS_Plot,\n                top = text_grob(\"Perceived Stress\", color = \"black\", face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\nnd &lt;- \n  tibble(Anxiety_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n          Depression_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\n\n    # we're finally ready to plot\n  Anxiety_Week1 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 1\", ], aes(x = Anxiety_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1\",\n       y = \"Self-Reported Altruism\",\n       x = \"Anxiety Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n  Anxiety_Week2 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 2\", ], aes(x = Anxiety_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 2\",\n       y = \"Self-Reported Altruism\",\n       x = \"Anxiety Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n  \n    Anxiety_Week3 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 3\", ], aes(x = Anxiety_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 3\",\n       y = \"Self-Reported Altruism\",\n       x = \"Anxiety Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n    \n  Anxiety_Week4 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 4\", ], aes(x = Anxiety_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 4\",\n       y = \"Self-Reported Altruism\",\n       x = \"Anxiety Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\nAnxiety Per Week\n\nAnxiety_Plot &lt;- ggarrange(Anxiety_Week1, Anxiety_Week2, Anxiety_Week3, Anxiety_Week4, ncol = 2, nrow = 2)\nannotate_figure(Anxiety_Plot,\n                top = text_grob(\"Anxiety\", color = \"black\", face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\nnd &lt;- \n  tibble(Depression_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n          Anxiety_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\n\n    # we're finally ready to plot\n  Depression_Week1 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 1\", ], aes(x = Depression_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1\",\n       y = \"Self-Reported Altruism\",\n       x = \"Depression Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n  Depression_Week2 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 2\", ], aes(x = Depression_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 2\",\n       y = \"Self-Reported Altruism\",\n       x = \"Depression Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n  \n  Depression_Week3 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 3\", ], aes(x = Depression_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 3\",\n       y = \"Self-Reported Altruism\",\n       x = \"Depression Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n    \n  Depression_Week4 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 4\", ], aes(x = Depression_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 4\",\n       y = \"Self-Reported Altruism\",\n       x = \"Depression Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\nDepression Per Week\n\nDepression_Plot &lt;- ggarrange(Depression_Week1, Depression_Week2, Depression_Week3, Depression_Week4, ncol = 2, nrow = 2)\nannotate_figure(Depression_Plot,\n                top = text_grob(\"Depression\", color = \"black\", face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\nnd &lt;- \n  tibble(Age_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n          Anxiety_C = mean(CVA_Bayes$Depression_C),\n         Depression_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week)\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\n\n    # we're finally ready to plot\n  Age_Week1 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 1\", ], aes(x = Age_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1\",\n       y = \"Self-Reported Altruism\",\n       x = \"Age Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n  Age_Week2 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 2\", ], aes(x = Age_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 2\",\n       y = \"Self-Reported Altruism\",\n       x = \"Age Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n  \n  Age_Week3 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 3\", ], aes(x = Age_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 3\",\n       y = \"Self-Reported Altruism\",\n       x = \"Age Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n    \n  Age_Week4 &lt;- ggplot(data = aaaa[aaaa$Week == \"Week 4\", ], aes(x = Age_C, y = Estimate)) +\n  geom_ribbon(aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 4\",\n       y = \"Self-Reported Altruism\",\n       x = \"Age Coefficient\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\nAge Per Week\n\nAge_Plot &lt;- ggarrange(Age_Week1, Age_Week2, Age_Week3, Age_Week4, ncol = 2, nrow = 2)\nannotate_figure(Age_Plot,\n                top = text_grob(\"Age\", color = \"black\", face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\n\nnd &lt;- \n  tibble(RP_C = seq(from = -5, to = 3, length.out = 601),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n         Anxiety_C = mean(CVA_Bayes$Anxiety_C),\n          Depression_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\n    # we're finally ready to plot\n  RP_Graph_AllWeeks &lt;- ggplot(data = aaaa, aes(x = RP_C, y = Estimate)) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 1\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 1\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 2\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 2\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1-4\",\n       y = \"SRA\",\n       x = \"Risk Perception\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\nnd &lt;- \n  tibble(PSS_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         Anxiety_C = mean(CVA_Bayes$Anxiety_C),\n          Depression_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\nPSS_Graph_AllWeeks &lt;- ggplot(data = aaaa, aes(x = PSS_C, y = Estimate)) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 1\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 1\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 2\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 2\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1-4\",\n       y = \"SRA\",\n       x = \"Stress\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\nnd &lt;- \n  tibble(Anxiety_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n          Depression_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\nAnxiety_Graph_AllWeeks &lt;- ggplot(data = aaaa, aes(x = Anxiety_C, y = Estimate)) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 1\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 1\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 2\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 2\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1-4\",\n       y = \"SRA\",\n       x = \"Anxiety\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\nnd &lt;- \n  tibble(Depression_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n          Anxiety_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Age_C = mean(CVA_Bayes$Age_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\nDepression_Graph_AllWeeks &lt;- ggplot(data = aaaa, aes(x = Depression_C, y = Estimate)) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 1\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 1\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 2\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 2\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1-4\",\n       y = \"SRA\",\n       x = \"Depression\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\nnd &lt;- \n  tibble(Age_C = seq(from = -5, to = 5, length.out = 601),\n         RP_C = mean(CVA_Bayes$RP_C),\n         PSS_C = mean(CVA_Bayes$PSS_C),\n          Anxiety_C = mean(CVA_Bayes$Depression_C),\n         Week = CVA_Bayes$Week,\n         Depression_C = mean(CVA_Bayes$Depression_C))\n\naaaa &lt;- fitted(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n  as_tibble() %&gt;% \n  # since `fitted()` and `predict()` name their intervals the same way, \n  # we'll need to `rename()` them to keep them straight\n  rename(f_ll = Q2.5,\n         f_ul = Q97.5) %&gt;% \n  # note how we're just nesting the `predict()` code right inside `bind_cols()`\n  bind_cols(\n    predict(COVID_Bayes_Model_2_Final, newdata = nd) %&gt;% \n      as_tibble() %&gt;% \n      # since we only need the intervals, we'll use `transmute()` rather than `mutate()`\n      transmute(p_ll = Q2.5,\n                p_ul = Q97.5),\n    # now tack on the `nd` data\n    nd)\n\nAge_Graph_AllWeeks &lt;- ggplot(data = aaaa, aes(x = Age_C, y = Estimate)) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 1\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 1\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 2\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 2\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  geom_ribbon(data = aaaa[aaaa$Week == \"Week 3\", ],aes(ymin = p_ll, ymax = p_ul),\n              fill = \"firebrick\", alpha = 1/5) +\n  geom_smooth(data = aaaa[aaaa$Week == \"Week 3\", ], aes(ymin = f_ll, ymax = f_ul),\n              stat = \"identity\",\n              fill = \"firebrick\", color = \"firebrick4\", alpha = 1/5, size = 1/4) +\n  coord_cartesian(xlim = range(CVA_Bayes$RP_C),\n                  ylim = c(-3, 4)) +\n  labs(subtitle = \"Counterfactual plot: Week 1-4\",\n       y = \"SRA\",\n       x = \"Age\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\nCounterfactual Plots: All Weeks Overlayed\n\nAllWeeksPlot &lt;- ggarrange(RP_Graph_AllWeeks, PSS_Graph_AllWeeks, Anxiety_Graph_AllWeeks, Depression_Graph_AllWeeks, Age_Graph_AllWeeks, ncol = 2, nrow = 3)\nannotate_figure(AllWeeksPlot,\n                top = text_grob(\"All Weeks\", color = \"black\", face = \"bold\", size = 14))\n\n\n\n\n\n\n\n\nHere I have overlayed each of the 4 weeks predictions for each parameter to show that they do not change very much from week to week."
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#no-pooling-model",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#no-pooling-model",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "No pooling model",
    "text": "No pooling model\nFirst, I fit a model that does not account for week. This is a no-pooing model because we are averaging over all clusters (i.e. Week).\n\nm_no_pooled_Week1 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C, data=CVA_Bayes[CVA_Bayes$Week == \"Week 1\", ],\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/m_no_pooled_Week1.rds\",\niter = 40000, warmup = 2000, cores = 4, chains =1, seed = 123, control = list(adapt_delta = 0.995))\n\nm_no_pooled_Week2 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C, data=CVA_Bayes[CVA_Bayes$Week == \"Week 2\", ],\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/m_no_pooled_Week2.rds\",\niter = 40000, warmup = 2000, cores = 4, chains =1, seed = 123, control = list(adapt_delta = 0.995))\n\nm_no_pooled_Week3 &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C, data=CVA_Bayes[CVA_Bayes$Week == \"Week 3\", ],\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/m_no_pooled_Week3.rds\",\niter = 40000, warmup = 2000, cores = 4, chains =1, seed = 123, control = list(adapt_delta = 0.995))\n\nm_no_pooled_Week4&lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C, data=CVA_Bayes[CVA_Bayes$Week == \"Week 4\", ],\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/m_no_pooled_Week4.rds\",\niter = 40000, warmup = 2000, cores = 4, chains =1, seed = 123, control = list(adapt_delta = 0.995))\n\n\ndf_no_pooled_Week1 &lt;- data_frame(\n  Model = \"no pooling\",\n  Week = \"Week 1\",\n  Intercept = fixef(m_no_pooled_Week1)[1], \n  Slope_RP_C = fixef(m_no_pooled_Week1)[2],\n  Slope_PSS_C = fixef(m_no_pooled_Week1)[3],\n  Slope_Anxiety_C = fixef(m_no_pooled_Week1)[4],\n  Slope_Depression_C = fixef(m_no_pooled_Week1)[5],\n  Slope_Age_C = fixef(m_no_pooled_Week1)[6])\n\ndf_no_pooled_Week2 &lt;- data_frame(\n  Model = \"no pooling\",\n  Week = \"Week 2\",\n  Intercept = fixef(m_no_pooled_Week2)[1], \n  Slope_RP_C = fixef(m_no_pooled_Week2)[2],\n  Slope_PSS_C = fixef(m_no_pooled_Week2)[3],\n  Slope_Anxiety_C = fixef(m_no_pooled_Week2)[4],\n  Slope_Depression_C = fixef(m_no_pooled_Week2)[5],\n  Slope_Age_C = fixef(m_no_pooled_Week2)[6])\n\ndf_no_pooled_Week3 &lt;- data_frame(\n  Model = \"no pooling\",\n  Week = \"Week 3\",\n  Intercept = fixef(m_no_pooled_Week3)[1], \n  Slope_RP_C = fixef(m_no_pooled_Week3)[2],\n  Slope_PSS_C = fixef(m_no_pooled_Week3)[3],\n  Slope_Anxiety_C = fixef(m_no_pooled_Week3)[4],\n  Slope_Depression_C = fixef(m_no_pooled_Week3)[5],\n  Slope_Age_C = fixef(m_no_pooled_Week3)[6])\n\ndf_no_pooled_Week4 &lt;- data_frame(\n  Model = \"no pooling\",\n  Week = \"Week 4\",\n  Intercept = fixef(m_no_pooled_Week4)[1], \n  Slope_RP_C = fixef(m_no_pooled_Week4)[2],\n  Slope_PSS_C = fixef(m_no_pooled_Week4)[3],\n  Slope_Anxiety_C = fixef(m_no_pooled_Week4)[4],\n  Slope_Depression_C = fixef(m_no_pooled_Week4)[5],\n  Slope_Age_C = fixef(m_no_pooled_Week4)[6])\n\ndf_no_pooling &lt;- rbind(df_no_pooled_Week1, df_no_pooled_Week2, df_no_pooled_Week3, df_no_pooled_Week4)\n\n\n# Full COVID model: adaptive delta = 0.99.\nm_pooled &lt;- brm(SRA_C~ 1 + RP_C + PSS_C + Anxiety_C + Depression_C  + Age_C, data=CVA_Bayes,\n           family = gaussian(),\n           prior = c(\nprior(normal(0, 1), class = Intercept),\nprior(normal(0, 1), class = b, coef = \"RP_C\"),\nprior(normal(0, 1), class = b, coef = \"PSS_C\"),\nprior(normal(0, 1), class = b, coef = \"Anxiety_C\"),\nprior(normal(0, 1), class = b, coef = \"Depression_C\"),\n#prior(normal(0, 1), class = b, coef = \"Age_C\"),\nprior(cauchy(0, 1), class = sigma)),\nfile = \"C:/Users/STPI0560/Desktop/Website/content/projects/2-covid19-altruism/models/m_pooled.rds\",\niter = 40000, warmup = 2000, cores = 4, chains =1, seed = 123, control = list(adapt_delta = 0.995))\n\n# Repeat the intercept and slope terms for each participant\ndf_pooled &lt;- data_frame(\n  Model = \"complete pooling\",\n  Week = unique(CVA_Bayes$Week),\n  Intercept = fixef(m_pooled)[1], \n  Slope_RP_C = fixef(m_pooled)[2],\n  Slope_PSS_C = fixef(m_pooled)[3],\n  Slope_Anxiety_C = fixef(m_pooled)[4],\n  Slope_Depression_C = fixef(m_pooled)[5],\n  Slope_Age_C = fixef(m_pooled)[6])\n\nhead(df_pooled)\n\nlibrary(dplyr)\n\n# create a vector with letters in the desired order\nx &lt;- c(\"Week 1\", \"Week 2\", \"Week 3\", \"Week 4\")\n\ndf_pooled %&gt;%\n  slice(match(x, Week))\n\n\n# Join the raw data so we can use plot the points and the lines.\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling) %&gt;% \n  left_join(CVA_Bayes, by = \"Week\")\n\np_model_comparison_RP &lt;- ggplot(df_models) + \n  aes(x = RP_C, y = SRA_C) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_abline(aes(intercept = Intercept, slope = Slope_RP_C, color = Model),\n              size = .75) + \n  geom_point() +\n  facet_wrap(~ Week, ncol = 4) +\n  xlab(\"RP_C\") + \n  ylab(\"SRA_C\") +\n  ggtitle(\"RP\")\n\np_model_comparison_RP\n\n\n\n\n\n\n\n# Join the raw data so we can use plot the points and the lines.\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling) %&gt;% \n  left_join(CVA_Bayes, by = \"Week\")\n\np_model_comparison_PSS &lt;- ggplot(df_models) + \n  aes(x = PSS_C, y = SRA_C) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_abline(aes(intercept = Intercept, slope = Slope_PSS_C, color = Model),\n              size = .75) + \n  geom_point() +\n  facet_wrap(~ Week, ncol = 4) +\n  xlab(\"PSS_C\") + \n  ylab(\"SRA_C\") +\n  ggtitle(\"PSS\")\n\np_model_comparison_PSS\n\n\n\n\n\n\n\n# Join the raw data so we can use plot the points and the lines.\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling) %&gt;% \n  left_join(CVA_Bayes, by = \"Week\")\n\np_model_comparison_Anxiety &lt;- ggplot(df_models) + \n  aes(x = Anxiety_C, y = SRA_C) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_abline(aes(intercept = Intercept, slope = Slope_RP_C, color = Model),\n              size = .75) + \n  geom_point() +\n  facet_wrap(~ Week, ncol = 4) +\n  xlab(\"Anxiety_C\") + \n  ylab(\"SRA_C\") +\n  ggtitle(\"Anxiety\")\n\np_model_comparison_Anxiety\n\n\n\n\n\n\n\n# Join the raw data so we can use plot the points and the lines.\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling) %&gt;% \n  left_join(CVA_Bayes, by = \"Week\")\n\np_model_comparison_Depression &lt;- ggplot(df_models) + \n  aes(x = Depression_C, y = SRA_C) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_abline(aes(intercept = Intercept, slope = Slope_RP_C, color = Model),\n              size = .75) + \n  geom_point() +\n  facet_wrap(~ Week, ncol = 4) +\n  xlab(\"Depression_C\") + \n  ylab(\"SRA_C\") +\n  ggtitle(\"Depression\")\n\np_model_comparison_Depression\n\n\n\n\n\n\n\n\nAbove are the parameters estimated from the complete pooling and no pooing models. The complete pooling estimate will be the same for each Week, while the no pooling will differ from week to week. That appears to be the case here.\nNext, I will plot a partially pooled regression line for each week.\n\n# Make a dataframe with the fitted effects\n#df_partial_pooling &lt;- fixef(COVID_Bayes_Model_2_Final)\n\n\nIntercept1 &lt;- coef(COVID_Bayes_Model_2_Final)$Week[, , 1]\nRP_C1 &lt;- coef(COVID_Bayes_Model_2_Final)$Week[, , 2]\nPSS_C1 &lt;- coef(COVID_Bayes_Model_2_Final)$Week[, , 3]\nAnxiety_C1 &lt;- coef(COVID_Bayes_Model_2_Final)$Week[, , 4]\nDepression_C1 &lt;- coef(COVID_Bayes_Model_2_Final)$Week[, , 5]\nAge_C1 &lt;- coef(COVID_Bayes_Model_2_Final)$Week[, , 6]\n\n\n# Repeat the intercept and slope terms for each participant\ndf_partial_pooling  &lt;- data_frame(\n  Model = \"Partial pooling\",\n  Week = unique(CVA_Bayes$Week),\n  Intercept = Intercept1[,1], \n  Slope_RP_C = RP_C1[,1],\n  Slope_PSS_C = PSS_C1[,1],\n  Slope_Anxiety_C = Anxiety_C1[,1],\n  Slope_Depression_C = Depression_C1[,1],\n  Slope_Age_C = Age_C1[,1])\n\n\ndf_models &lt;- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %&gt;% \n  left_join(CVA_Bayes, by = \"Week\")\n\n# Replace the data-set of the last plot\n\np_model_comparison_RP %+% df_models\n\n\n\n\n\n\n\np_model_comparison_PSS %+% df_models\n\n\n\n\n\n\n\np_model_comparison_Anxiety %+% df_models\n\n\n\n\n\n\n\np_model_comparison_Depression %+% df_models\n\n\n\n\n\n\n\n\nThe partial pooling model is supposed to be a compromise between the two models, and importantly, should shrink the estimates within each Week closer to the overall average parameter value. To see this, we can do a shrinkage plot similar to what McElreath does.\n\n# Also visualize the point for the fixed effects\ndf_fixef &lt;- data_frame(\n  Model = \"Partial pooling (average)\",\n  Intercept = fixef(m_pooled)[1], \n  Slope_RP_C = fixef(m_pooled)[2],\n  Slope_PSS_C = fixef(m_pooled)[3],\n  Slope_Anxiety_C = fixef(m_pooled)[4],\n  Slope_Depression_C = fixef(m_pooled)[5])\n\n# Complete pooling / fixed effects are center of gravity in the plot\ndf_gravity &lt;- df_pooled %&gt;% \n  distinct(Model, Intercept, Slope_RP_C, Slope_PSS_C, Slope_Anxiety_C, Slope_Depression_C) %&gt;% \n  bind_rows(df_fixef)\n#df_gravity"
  },
  {
    "objectID": "content/projects/2-covid19-altruism/2-covid19-altruism.html#shrinkage-plots",
    "href": "content/projects/2-covid19-altruism/2-covid19-altruism.html#shrinkage-plots",
    "title": "Threat Imminence And Everyday Altruism During The COVID-19 Pandemic",
    "section": "Shrinkage Plots",
    "text": "Shrinkage Plots\nBelow are a number of graphs showing how the partial pooling model estimates for each cluster (week) move closer to the average. It should appear as though gravity is pulling the estimates towards a center mass of the graph. For the most part, the graphs reflect this, although there are some instances where they either don’t shrink closer, or even get further away. I’m not sure why that is the case. Perhaps the models are not optomised properly. Still, there generally does seem to be an advantage for using the partial pooled models. Additionally, I am most interested in seeing the slope parameters shrinking towards the global estimate, which is what I see in almost every arrow (meaning, if we ignore the direction on the x-axis, the arrows point closer to the true estimate on the y-axis most of the time). Or, 13/16 arrows point closer to the pooled parameter estimate than further from it.\n\ndf_pulled &lt;- bind_rows(df_no_pooling, df_partial_pooling)\n\nggplot(df_pulled) + \n  aes(x = Intercept, y = Slope_RP_C, color = Model) + \n  geom_point(size = 2) + \n  geom_point(data = df_gravity, size = 5) + \n  # Draw an arrow connecting the observations between models\n  geom_path(aes(group = Week, color = NULL), \n            arrow = arrow(length = unit(.02, \"npc\"))) + \n  # Use ggrepel to jitter the labels away from the points\n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL), \n    data = df_no_pooling) + \n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL)) + \n  theme(legend.position = \"bottom\") + \n  ggtitle(\"Pooling of regression parameters\") + \n  xlab(\"Intercept estimate\") + \n  ylab(\"RP estimate\") + \n  scale_color_brewer(palette = \"Dark2\") \n\n\n\n\n\n\n\nggplot(df_pulled) + \n  aes(x = Intercept, y = Slope_PSS_C, color = Model) + \n  geom_point(size = 2) + \n  geom_point(data = df_gravity, size = 5) + \n  # Draw an arrow connecting the observations between models\n  geom_path(aes(group = Week, color = NULL), \n            arrow = arrow(length = unit(.02, \"npc\"))) + \n  # Use ggrepel to jitter the labels away from the points\n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL), \n    data = df_no_pooling) + \n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL)) + \n  theme(legend.position = \"bottom\") + \n  ggtitle(\"Pooling of regression parameters\") + \n  xlab(\"Intercept estimate\") + \n  ylab(\"PSS estimate\") + \n  scale_color_brewer(palette = \"Dark2\") \n\n\n\n\n\n\n\nggplot(df_pulled) + \n  aes(x = Intercept, y = Slope_Anxiety_C, color = Model) + \n  geom_point(size = 2) + \n  geom_point(data = df_gravity, size = 5) + \n  # Draw an arrow connecting the observations between models\n  geom_path(aes(group = Week, color = NULL), \n            arrow = arrow(length = unit(.02, \"npc\"))) + \n  # Use ggrepel to jitter the labels away from the points\n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL), \n    data = df_no_pooling) + \n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL)) + \n  theme(legend.position = \"bottom\") + \n  ggtitle(\"Pooling of regression parameters\") + \n  xlab(\"Intercept estimate\") + \n  ylab(\"Anxiety estimate\") + \n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\nggplot(df_pulled) + \n  aes(x = Intercept, y = Slope_Depression_C, color = Model) + \n  geom_point(size = 2) + \n  geom_point(data = df_gravity, size = 5) + \n  # Draw an arrow connecting the observations between models\n  geom_path(aes(group = Week, color = NULL), \n            arrow = arrow(length = unit(.02, \"npc\"))) + \n  # Use ggrepel to jitter the labels away from the points\n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL), \n    data = df_no_pooling) + \n  ggrepel::geom_text_repel(\n    aes(label = Week, color = NULL)) + \n  theme(legend.position = \"bottom\") + \n  ggtitle(\"Pooling of regression parameters\") + \n  xlab(\"Intercept estimate\") + \n  ylab(\"Depression estimate\") + \n  scale_color_brewer(palette = \"Dark2\")"
  },
  {
    "objectID": "content/posts/1-eyeball-test/1-eyeball-test.html",
    "href": "content/posts/1-eyeball-test/1-eyeball-test.html",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "",
    "text": "# load libraries\nlibrary(tidyverse)\nlibrary(tidyr)\nlibrary(lsr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "content/posts/1-eyeball-test/1-eyeball-test.html#how-does-a-simulation-work",
    "href": "content/posts/1-eyeball-test/1-eyeball-test.html#how-does-a-simulation-work",
    "title": "Why You Should Pre-specify Exploratory Analyses",
    "section": "How Does a Simulation Work?",
    "text": "How Does a Simulation Work?\nThis simulation will assume the null hypthesis is true. Therefore, there won’t be any difference between the groups at the population level. There may, however, be difference at the sample level. In fact, there almost always will be. Because I prefer linear regression, I will simulate data from a linear model:\n\\(Y_{i} \\mid \\beta_{0}, \\beta_{1}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0} + \\beta_{1}X_{i}\\)\nHowever, because the slope will always be cancelled out by zero, this model can be simplified to this:\n\\(Y_{i} \\mid \\beta_{0}, \\sigma \\sim N (\\mu_{i}, \\sigma^{2})\\) with \\(\\mu_{i} = \\beta_{0}\\)\nFirst, I make an indicator for the independent variable. One group will have a zero, and the other a one. This also means the sample size is going to be n = 100.\n\nx &lt;- rep(c(0, 1), each = 50)\nx\n\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nTo make the dependent variable, one would need to add and multiply the model coefficients with the independent variable(s). That is what I will do here. No measurement is perfect though, so every sample will be measured with some error. I am simulating this error with the rnorm() function. Because there is no difference at the population level between the two groups, both groups are sampled from a normal distribution with a mean of 350 and a standard deviation of 30. To calculate the difference between groups, one would add a slope to the intercept. Here, the slope is zero and cancels out the influence of the independent variable.\n\nset.seed(1234)\ny &lt;- rnorm(100, mean = 350, sd = 30) + 0*x\ny\n\n  [1] 313.7880 358.3229 382.5332 279.6291 362.8737 365.1817 332.7578 333.6010\n  [9] 333.0664 323.2989 335.6842 320.0484 326.7124 351.9338 378.7848 346.6914\n [17] 334.6697 322.6641 324.8848 422.4751 354.0226 335.2794 336.7836 363.7877\n [25] 329.1884 306.5539 367.2427 319.2903 349.5459 321.9215 383.0689 335.7322\n [33] 328.7168 334.9623 301.1272 314.9714 284.5988 309.7702 341.1712 336.0231\n [41] 393.4849 317.9407 324.3391 341.5813 320.1698 320.9446 316.7805 312.4404\n [49] 334.2852 335.0945 295.8191 332.5377 316.7333 319.5511 345.1307 366.8917\n [57] 399.4345 326.7994 398.1773 315.2657 369.6977 426.4697 348.9572 329.9110\n [65] 349.7719 403.3125 315.8418 391.0348 389.8869 360.0942 350.2068 336.3359\n [73] 339.0043 369.4486 412.1081 345.3980 308.2790 328.2925 357.7479 340.4882\n [81] 344.6663 344.9002 308.8309 344.7864 375.5070 370.9283 366.4999 337.9180\n [89] 344.2522 314.1642 348.4052 357.6559 401.1789 380.0454 335.1325 360.6665\n [97] 315.9618 376.3461 379.1875 413.6335\n\n\nNow the data can be put in a dataframe\n\nsim_df &lt;- data.frame(x, y)\nhead(sim_df)\n\n  x        y\n1 0 313.7880\n2 0 358.3229\n3 0 382.5332\n4 0 279.6291\n5 0 362.8737\n6 0 365.1817\n\n\nFinally, a linear regression can be performed on the sample. Remember, their is no difference between the two groups at the pupulation level, but there almost certainlt will be differences at the sample level (due to sampling error).\n\nmodel &lt;-  lm(y ~ x, data = sim_df)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = sim_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.367 -17.301  -3.815  15.830  86.067 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  336.408      4.090  82.242  &lt; 2e-16 ***\nx             17.777      5.785   3.073  0.00274 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.92 on 98 degrees of freedom\nMultiple R-squared:  0.0879,    Adjusted R-squared:  0.07859 \nF-statistic: 9.444 on 1 and 98 DF,  p-value: 0.002743\n\n\nAbove I can see the slope coefficient (x) is actually statistically significant. This is likely due to the dact that the Intercept estimate is so low. Both groups were sampled from the same distribution centered at 350. THe Intercept is 336, and the slope is 18. This means the mean of the second group is 336 + 18 = 354. In frequentist statistics, probabilities are conceptualized in reference to long-term frequencies. For instance, we know a fair coin has a 50% probability of coming up heads because if we flipped that coin a theoretically infinite number of times, approximately 50% of those flips would land heads. P-values can be thought of the same way: If you were to run the same experiment an infinite number of times, fixing the sample size and collecting a new sample with each experiment, how many p-values would be below your threshold (in most cases in science, the threshold is 0.05)? When the null hypothesis is true, only 5% of experiments will yield a p-value below 0.05."
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "The data I am analysing comes from a paper I recently published in the Journal Of Neuroscience (https://www.jneurosci.org/content/44/22/e1232232024). The paper involves 3 experiments, of which I am only analysing data from the first. I have chosen this study because I actually wanted to do a Bayesian analysis originally, and wanted to do a random effects model as well. We opted for a Frequentist analysis and a 3-way ANOVA. Here I will perform the same analyses using a Bayesian random-effects model.\n\n\nPeople are notoriously bad at identifying odors. Herrick (1993) noted that the olfactory cortex of most species is large, relative to other areas, and it is only in humans that the olfactory epithelium becomes dwarfed by the neocortex. This is likely do to evolution favoring cognitive capabilities, along with senses more important to our lives, like vision. Herrick hypothesized that, because the human olfactory cortex still contains dense interconnectivity with other sensory modalities, that maybe olfaction relies on other senses to help with olfactory identification and localization. Additionally, a recent study found that when people describe different types of sensory stimuli, all but olfaction are described abstractly (i.e. bright). Olfactory stimuli alone Are described as object-based (i.e. lemon).\n\n\n\nAfter removing some participants for poor performance, this dataset includes 63 participants (40 female; age range: 18-65; mean age: 32 years) total.\n\n\n\nWe developed a behavioural task where people listened to a predictive cue, were presented with a target, and then had to determine whether the cue and target matched or did not match. The cue was delivered by a voice, and the target was either visual or olfactory. The cue could also be object-based (i.e. lemon) or category-based (i.e. fruit). The targets consisted of 4 stimuli: lavender, lilac, lemon, pear. These could be presented both visually and in an olfactory manner. We hypothesized that, if olfaction requires information from another sensory modality, that their would be a large disparity in reaction time between matching (congruent) and non-matching (incongruent) cues and targets. We further hypothesized that this disparity would not be seen when the targets were visual. We also hypothesized that, in olfaction, this disparity would be larger when the cues were object-based, rather than category based.\n\n\n\nTheir are several predictions that are important for this assignment. First, we predicted much slower responses to olfactory targets. We also predicted participants would be slower at responding when cues and targets did not match. We further predicted that this slowness in responding to cues and targets that were incongruent would be larger when the targets were olfactory. Finally, we predicted that, in olfaction, people would respond more slowly to incongruent cues/targets whe nthe cues were object-based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#short-introduction",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#short-introduction",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "People are notoriously bad at identifying odors. Herrick (1993) noted that the olfactory cortex of most species is large, relative to other areas, and it is only in humans that the olfactory epithelium becomes dwarfed by the neocortex. This is likely do to evolution favoring cognitive capabilities, along with senses more important to our lives, like vision. Herrick hypothesized that, because the human olfactory cortex still contains dense interconnectivity with other sensory modalities, that maybe olfaction relies on other senses to help with olfactory identification and localization. Additionally, a recent study found that when people describe different types of sensory stimuli, all but olfaction are described abstractly (i.e. bright). Olfactory stimuli alone Are described as object-based (i.e. lemon)."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#participant-information",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#participant-information",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "After removing some participants for poor performance, this dataset includes 63 participants (40 female; age range: 18-65; mean age: 32 years) total."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#hypotheses",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#hypotheses",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "We developed a behavioural task where people listened to a predictive cue, were presented with a target, and then had to determine whether the cue and target matched or did not match. The cue was delivered by a voice, and the target was either visual or olfactory. The cue could also be object-based (i.e. lemon) or category-based (i.e. fruit). The targets consisted of 4 stimuli: lavender, lilac, lemon, pear. These could be presented both visually and in an olfactory manner. We hypothesized that, if olfaction requires information from another sensory modality, that their would be a large disparity in reaction time between matching (congruent) and non-matching (incongruent) cues and targets. We further hypothesized that this disparity would not be seen when the targets were visual. We also hypothesized that, in olfaction, this disparity would be larger when the cues were object-based, rather than category based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#predictions",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#predictions",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "",
    "text": "Their are several predictions that are important for this assignment. First, we predicted much slower responses to olfactory targets. We also predicted participants would be slower at responding when cues and targets did not match. We further predicted that this slowness in responding to cues and targets that were incongruent would be larger when the targets were olfactory. Finally, we predicted that, in olfaction, people would respond more slowly to incongruent cues/targets whe nthe cues were object-based."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-global-environment",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-global-environment",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Global Environment",
    "text": "Load Global Environment\n\n# load variables that are saved outside R\nload(\"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/workspace/assignmentworkspace.RData\")"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-libraries",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-libraries",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n# load libraries\nlibrary(tidyverse)\nlibrary(kableExtra)\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidybayes) \nlibrary(ggpubr)\nlibrary(gridExtra)"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#set-ggplot-theme-and-color-scheme",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#set-ggplot-theme-and-color-scheme",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Set ggplot Theme And Color Scheme",
    "text": "Set ggplot Theme And Color Scheme\n\n# set ggplot theme\ntheme_set(theme_minimal())\n# set ggplot colors\ncolors &lt;- c(\"#d1e1ec\", \"#b3cde0\", \"#6497b1\", \"#005b96\", \"#03396c\", \"#011f4b\")"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#example-dataframe",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#example-dataframe",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Example Dataframe",
    "text": "Example Dataframe\nBelow is an example for how one participants’ data might look. These data are fake, but provide an overview for how the data are organized.\n\n# Fill dataframe with fake values for one participant\nfaketable &lt;- data.frame(\n  row = c(1, 2, 3, 4, \"...\", 44, 45, 46, 47, \"...\", 82, 83, 84, 85, \"...\", 127, 128, 129, 130),\n  subject = c(rep(1, times = 4), \"...\", rep(1, times = 4), \"...\", rep(1, times = 4), \"...\", rep(1, times = 4)),\n  modality = c(rep(\"visual\", times = 4), \"...\", rep(\"visual\", times = 4), \"...\", rep(\"olfactory\", times = 4), \"...\", rep(\"olfactory\", times = 4)),\n  congruency = c(rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\",rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\", rep(c(\"congruent\", \"incongruent\"), times = 2), \"...\",rep(c(\"congruent\", \"incongruent\"), times = 2)),\n  cue_type = c(rep(\"object\", time = 4), \"...\", rep(\"category\", times = 4), \"...\", rep(\"object\", time = 4), \"...\", rep(\"category\", times = 4)),\n  auditory_cue = c(\"lavender\", \"lilac\", \"lemon\", \"pear\", \"...\", \"flower\", \"flower\", \"fruit\", \"fruit\", \"...\", \"lavender\", \"lilac\", \"lemon\", \"pear\", \"...\", \"flower\", \"flower\", \"fruit\", \"fruit\"),\n  target = c(\"lavender\", \"lavender\", \"lemon\", \"lemon\", \"...\", \"lavender\", \"pear\", \"lemon\", \"lilac\", \"...\", \"lavender\", \"lavender\", \"lemon\", \"lemon\", \"...\", \"lavender\", \"pear\", \"lemon\", \"lilac\"),\n  reaction_time = c(564, 740, 602, 557, \"...\", 471, 649, 668, 519, \"...\", 1121, 1576, 1844, 1343, \"...\", 1876, 1265, 1721, 1846)\n  )\n# display fake dataframe with nice kable styling\nfaketable %&gt;%\n  kbl(caption = \"Example dataframe\") %&gt;%\n  kable_classic(full_width = F, html_font = \"Cambria\")\n\n\nExample dataframe\n\n\nrow\nsubject\nmodality\ncongruency\ncue_type\nauditory_cue\ntarget\nreaction_time\n\n\n\n\n1\n1\nvisual\ncongruent\nobject\nlavender\nlavender\n564\n\n\n2\n1\nvisual\nincongruent\nobject\nlilac\nlavender\n740\n\n\n3\n1\nvisual\ncongruent\nobject\nlemon\nlemon\n602\n\n\n4\n1\nvisual\nincongruent\nobject\npear\nlemon\n557\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44\n1\nvisual\ncongruent\ncategory\nflower\nlavender\n471\n\n\n45\n1\nvisual\nincongruent\ncategory\nflower\npear\n649\n\n\n46\n1\nvisual\ncongruent\ncategory\nfruit\nlemon\n668\n\n\n47\n1\nvisual\nincongruent\ncategory\nfruit\nlilac\n519\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n82\n1\nolfactory\ncongruent\nobject\nlavender\nlavender\n1121\n\n\n83\n1\nolfactory\nincongruent\nobject\nlilac\nlavender\n1576\n\n\n84\n1\nolfactory\ncongruent\nobject\nlemon\nlemon\n1844\n\n\n85\n1\nolfactory\nincongruent\nobject\npear\nlemon\n1343\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127\n1\nolfactory\ncongruent\ncategory\nflower\nlavender\n1876\n\n\n128\n1\nolfactory\nincongruent\ncategory\nflower\npear\n1265\n\n\n129\n1\nolfactory\ncongruent\ncategory\nfruit\nlemon\n1721\n\n\n130\n1\nolfactory\nincongruent\ncategory\nfruit\nlilac\n1846"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-data",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#load-data",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Load Data",
    "text": "Load Data\nThe data I am using is a single .csv file from our previous experiment. These data were pre-processed in a separate R script that took each individual participants’ data and combined them into a single dataframe. The data are not aggregated, so they still represent the participants’ raw data. However, 5 participants were removed for having very low accuracy in the task. Additionally, we were only interested in trials where participants’ correctly determined whether the cue and target matched/did not match. Thus, these data only represent “correct” trials. Additionally, all participants’ in this dataset had at least 80% accuracy across the study.\n\n# load data\ndf &lt;- read.csv(file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/data/object_category.csv\", header = TRUE)\n# glimpse first few rows\nhead(df)\n\n  participant modality  congruency cue_type      cue   target reaction_time\n1         101   visual incongruent   object     Pear Lavender     1933.3277\n2         101   visual   congruent   object     Pear     Pear      400.3761\n3         101   visual   congruent   object    Lemon    Lemon      500.3686\n4         101   visual incongruent   object    Lilac     Pear      833.7295\n5         101   visual   congruent   object    Lilac    Lilac      533.7733\n6         101   visual   congruent   object Lavender Lavender      883.7940\n\n\nSome columns need to be changed to factors for the analysis.\n\n# Columns to convert to factors\ncols_to_factor &lt;- c(\"participant\", \"modality\", \"congruency\", \"cue_type\", \"cue\", \"target\")\n# Convert specified columns to factors\ndf[cols_to_factor] &lt;- lapply(df[cols_to_factor], as.factor)\n\nFirst, I wanted to plot the distribution of the dependent variable (reaction time) for both visual and olfactory target trials. This is because reaction time is often skewed. If the distributions are indeed skewed, I may want to model them using an Exgaussian distribution.\n\n# Add mean lines\nggplot(df, aes(x=reaction_time, fill = modality)) +\n  geom_histogram(bins = 100)+\n  theme(legend.position=\"top\") +\n  scale_fill_manual(values = c(colors[2], colors[5]))\n\n\n\n\n\n\n\n\nThey definitely appear skewed. Thus, I will take that into account with my model. However, I want to try both an Exgaussian and Gaussian model. I will then see how well each model recovers the distribution of the dependent variable using a Posterior Predictive Check (PPC)."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#gaussian-model",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#gaussian-model",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Gaussian model",
    "text": "Gaussian model\n\nPrior Predictive Check.\nI want to use informative priors for my model as I have already conducted several other similar studies and have several ideas about how reaction times will differ between different conditions. My original plan was to conduct a Prior Predictive Check to see how my informative priors would generate reaction time data. However, this proved too difficult and time consuming. The Prior Predictive Check gave estimates of some pararmeters in the millions, while also having very low effective sample sizes (ess). Some of the ess values were as low as 20, even with 4 chains running for 10,000 iterations. I think the model is simply to complex for the number of iterations I would need to run for stable estimates. Therefore, I will be going ahead without a Prior Predictive Check.\n\n\nInformative Priors\nI decided to use informative priors as I have run several studies looking at modality and congruency regarding vision and olfaction. Those have been outlined in the Notation section above. I did not put informative priors on the interaction terms however, because it is very hard to predict exactly how they will turn out.\n\n\nFitting Gaussian Model in brm\nFirst, I estimated the 3-way factorial model using a Gaussian distribution.\n\n# run Gaussian model in brm\ngaussian_model &lt;- brm(\n  reaction_time ~ modality*congruency*cue_type + (target|participant),\n  data = df,\n  family = gaussian(),\n  prior = c(prior(normal(1200, 200), class = Intercept),\n            prior(normal(500, 100), class = b, coef = \"modalityvisual\"),\n            prior(normal(200, 50), class = b, coef = \"congruencyincongruent\"),\n            prior(normal(100, 50), class = b, coef = \"cue_typeobject\"),\n            prior(cauchy(0, 2), class = sd),\n            prior(cauchy(0, 2), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter  = 6000,\n  warmup = 2000,\n  file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/models/gaussian_model\"\n  )"
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#exgaussian-model",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#exgaussian-model",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Exgaussian Model",
    "text": "Exgaussian Model\nNext, I run an Exgaussian model. Everything is identical to the Gaussian model otherwise.\n\n# run Exgaussian model in brm\nexgaussian_model &lt;- brm(\n  reaction_time ~ modality*congruency*cue_type + (target|participant),\n  data = df,\n  family = exgaussian(),\n  prior = c(prior(normal(1200, 200), class = Intercept),\n            prior(normal(500, 100), class = b, coef = \"modalityvisual\"),\n            prior(normal(200, 50), class = b, coef = \"congruencyincongruent\"),\n            prior(normal(100, 50), class = b, coef = \"cue_typeobject\"),\n            prior(cauchy(0, 2), class = sd),\n            prior(cauchy(0, 2), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter  = 6000,\n  warmup = 2000, \n  file = \"C:/Users/STPI0560/Desktop/Archieve/Stat 2.5/assignment/models/exgaussian_model\",\n  )\n\nUsing informative priors with the Exgaussian model led to sever errors, with R-hat values over 3 and ess values as low as 5. Not shown here, I ran another Exgaussian model where I used the default priors brm gives you. This model did well, so I will use it from here."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#model-comparison",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#model-comparison",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Model Comparison",
    "text": "Model Comparison\nMy plan was to first compare these models using the loo function. However, due to their complexity, I would need to redo both models and save the parameters as the models go. I have done this before and know it would take at least twice as long to run (sometimes longer). Given that the Exgaussian model took more than a day to complete, I just don’t have time. Instead I will let the PPC below guide my model selection."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#mcmc-traceplots",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#mcmc-traceplots",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "MCMC Traceplots",
    "text": "MCMC Traceplots\n\nmcmc_trace(gaussian_model, pars = c(\"b_Intercept\", \"b_modalityvisual\", \"b_congruencyincongruent\", \"b_cue_typeobject\", \"b_modalityvisual:congruencyincongruent\", \"b_modalityvisual:cue_typeobject\", \"b_congruencyincongruent:cue_typeobject\", \"b_modalityvisual:congruencyincongruent:cue_typeobject\", \"sigma\"), \n           facet_args = list(ncol = 2, strip.position = \"left\"))\n\n\n\n\n\n\n\n\nAll chains had Rhat values of 1, so I know they converged. This just gives a bit more visual evidence of that."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#fixed-effects",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#fixed-effects",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nHere I am taking all draws from all markov chains for all parameters and plotting a histogram of them. I will also take the model estimates and credible intervals and plot those under the histogram.\n\n# make posterior draws dataframe\ndraws_posterior &lt;- gaussian_model |&gt;\n  # select coefficients\n  spread_draws(b_Intercept, b_modalityvisual, b_congruencyincongruent, b_cue_typeobject, `b_modalityvisual:congruencyincongruent`, `b_modalityvisual:congruencyincongruent:cue_typeobject`) |&gt;\n  # new column called \"posterior\"\n  mutate(distribution = \"posterior\")\n\n\nMain Effects\nBelow I plot the posterior distributions for the main fixed effects.\n\n# graph of modality\nmodality_main_graph &lt;- ggplot(draws_posterior, aes(x = b_modalityvisual)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[2, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[2, 3], y = 0, xend = fixef(gaussian_model)[2, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Modality\") +\n  labs(x = \"Vision - Olfaction\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# graph of congruency\ncongruency_main_graph &lt;- ggplot(draws_posterior, aes(x = b_congruencyincongruent)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[3, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[3, 3], y = 0, xend = fixef(gaussian_model)[3, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Congruency\") +\n  labs(x = \"Incongruent - Congruent\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# graph of cue-type\ncuetype_main_graph &lt;- ggplot(draws_posterior, aes(x = b_cue_typeobject)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[4, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[4, 3], y = 0, xend = fixef(gaussian_model)[4, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Main Effect: Cue Type\") +\n  labs(x = \"Object-Cue - Category-Cue\",y = \"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# arrange graphs beside one another\nggarrange(modality_main_graph, congruency_main_graph, cuetype_main_graph, ncol = 3)\n\n\n\n\n\n\n\n\nFirst, visual trials are 1176ms faster than olfactory trials. Because olfactory trials are the intercept (1764ms), visual trials are estimated to be approximately 588ms. This is a very big difference, but not surprising as it takes longer to process an odor compared to a visual stimulus. Next, incongruent trials are 109ms slower than congruent trials. So people tend to respond more slowly when the auditory cue and target do not match. Finally, people are 118ms faster at responding to targets when the cue is object-based rather than category-based.\nCentral to our main hypothesis, we were interested in whether the congruency effect (people responding slower when the cue and target did not match) was larger for olfactory target trials compared to visual target trials.\n\n# graph of modality x congruency posterior\ncongruency_modality_graph &lt;- ggplot(draws_posterior, aes(x = `b_modalityvisual:congruencyincongruent`)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[5, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[5, 3], y = 0, xend = fixef(gaussian_model)[5, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Interaction: Modality x Congruency\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\ncongruency_modality_cuetype_graph &lt;- ggplot(draws_posterior, aes(x = `b_modalityvisual:congruencyincongruent:cue_typeobject`)) +\n  geom_histogram(binwidth = 0.75, position = \"identity\", color = colors[3], fill = colors[3]) +\n  geom_point(aes(x = fixef(gaussian_model)[8, 1], y = 0), colour = \"black\", size = 3) +\n  geom_segment(aes(x = fixef(gaussian_model)[8, 3], y = 0, xend = fixef(gaussian_model)[8, 4], yend = 0), size = 1, color = \"black\") +\n  ggtitle(\"Interaction: Modality x Congruency x Cue-Type\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(axis.title = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 11))\n\n# arrange graphs beside one another\nggarrange(congruency_modality_graph, congruency_modality_cuetype_graph, ncol = 2)\n\n\n\n\n\n\n\n\nThis can be interpreted like this: people respond more slowly to incongruent cue target presentations, but this difference is greater when the target is olfactory compared to when it is visual. We could also say they are 53ms slower at responding to incongruent olfactory targets compared to incongruent visual targets. In addition, the 95% credible intervals do not overlap with zero (-92.02, -15.15). All of the 95% most probable reaction times for this interaction are negative, so I think we can say this difference is likely negative too. For comparison, this interaction was statistically significant in the original paper (p = 0.02). I actually think a p-value that close to the threshold is not amazingly convincing, so it kind of matches the interpretation of the credible interval here. The plot on the right represents the 3-way interaction. This lets us see whether the disparity in congruent and incongruent reaction times, which is larger in olfaction, occurs when object-cues preceed the olfactory target. The model suggests this is not the case.\nThere are other interactions as well, but they were not central to our main hypotheses so I won’t go into them here. In addition, I’m not a fan of 3-way interactions as they are hard to interpret and usually very underpowered to detect, so I don’t see much value in looking at the 3-way interaction here."
  },
  {
    "objectID": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#modality-random-effects",
    "href": "content/projects/1-jneuro-behavioural/1-jneuro-behavioural.html#modality-random-effects",
    "title": "Journal Of Neuroscience: Bayesian Reanalysis",
    "section": "Modality random effects",
    "text": "Modality random effects\nBelow is some pre-processing for displaying main random-effects estimates.\n\n# temporary dataframe to be filled in loop\ntemp_df &lt;- data.frame(\"participant\" = factor(),\n                      \"empirical_olfactory_rt\" = numeric(),\n                      \"estimated_olfactory_rt\" = numeric(),\n                      \"empicial_visual_rt\" = numeric(),\n                      \"estimated_visual_rt\" = numeric())\n# for each unique participant:\nfor (i in unique(df$participant)) { \n  # ith participants' reaction time random effect for olfaction\n  olfaction_rt_i &lt;- fixef(gaussian_model)[1]  + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4]\n  # ith participants' reaction time random effect for vision\n  visual_rt_i &lt;- fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]) + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4]\n  # dataframe of fixed and random modality effect estimates for ith person\n  participant_df &lt;- data.frame(\"participant\" = i,\n                      \"empirical_olfactory_rt\" = fixef(gaussian_model)[1],\n                      \"estimated_olfactory_rt\" = olfaction_rt_i,\n                      \"empicial_visual_rt\" = fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]),\n                      \"estimated_visual_rt\" = fixef(gaussian_model)[1] + (fixef(gaussian_model)[2]) + ranef(gaussian_model)$participant[i, 1, 1] + ranef(gaussian_model)$participant[i, 1, 2] + ranef(gaussian_model)$participant[i, 1, 3] + ranef(gaussian_model)$participant[i, 1, 4])\n  # here all participant fixed and random modality effect estimates are combined in a single dataframe\n  temp_df &lt;- rbind(temp_df, participant_df)\n}\n\n\n# reshape from wide to long\nmodality_df &lt;- temp_df %&gt;% pivot_longer(cols = c('empirical_olfactory_rt', 'estimated_olfactory_rt', 'empicial_visual_rt', 'estimated_visual_rt'),\n                    names_to = 'coefficient',\n                    values_to = 'rt')\n# create new column\nmodality_df$group &lt;- factor(ifelse(modality_df$coefficient == \"empirical_olfactory_rt\", \"empirical\",\n                            ifelse(modality_df$coefficient == \"empicial_visual_rt\", \"empirical\", \"estimated\")))\n# create new column\nmodality_df$group2 &lt;- factor(ifelse(modality_df$coefficient == \"empirical_olfactory_rt\", \"olfactory\",\n                            ifelse(modality_df$coefficient == \"estimated_olfactory_rt\", \"olfactory\", \"visual\")))\n# rename columns\ncolnames(modality_df) &lt;- c(\"participant\", \"coefficient\", \"rt\", \"estimate\", \"modality\")\n\nmodality_df_estimates &lt;- modality_df[modality_df$estimate == \"estimated\", ]\nmodality_df_estimates &lt;- aggregate(modality_df$rt, list(modality_df$participant, modality_df$modality), mean)\ncolnames(modality_df_estimates) &lt;- c(\"participant\", \"modality\", \"rt\")\n\n# Calculate mean rt for each modality\nmean_values &lt;- modality_df_estimates %&gt;%\n  group_by(modality) %&gt;%\n  summarize(mean_rt = mean(rt))\n\nmean_values &lt;- data.frame(\"participant\" = c(101, 101),\n                          \"modality\" = c(\"olfactory\", \"visual\"),\n                          \"rt\" = c(1837, 612))\nmean_values$participant &lt;- as.factor(mean_values$participant)\nmean_values$modality &lt;- as.factor(mean_values$modality)\n\n\n# graph random effects for modality\nggplot(modality_df_estimates, aes(x = modality, y = rt, group = participant)) +\n  geom_point(color = \"grey\") +\n  geom_line(color = \"grey\")  +\n  geom_point(data = mean_values, aes(x = modality, y = rt), color = \"black\", size = 3) +\n  geom_line(data = mean_values, aes(x = modality, y = rt, group = 1), color = \"black\", size = 1.5) +\n  ggtitle(\"Modality Random Effects\")\n\n\n\n\n\n\n\n\nAn alternate way to view this is to put each participants’ random effect estimate on a plot with the fixed effect estimate, and plot each of these on individual panels. Maybe not as intuitive as the previous plot, but it still makes sense to look at it like this.\n\n# empty list where each runner's graph will be saved.\ntemp_list &lt;- list()  \n# loop through runners, grab their specific intercepts/slope from previous dataframe, and save graph to list\nfor (i in unique(modality_df$participant)) {\n  # runner-specific graph\n  temp_graph &lt;- ggplot(modality_df[modality_df$participant == i, ], aes(x = modality, y = rt, color = estimate, group = estimate)) +\n  geom_point() +\n  geom_line() +\n    ylim(0, 2800) +\n    ggtitle(paste0(\"Subject \", i)) +\n    theme(legend.position = \"none\")\n  # save ith runner graph to temp_list\n  temp_list[[i]] &lt;- temp_graph\n}\n\narg_list &lt;- c(temp_list, list(nrow = 13, ncol = 5))\n# Use do.call to pass all elements of graph_list to ggarrange\narranged_plots &lt;- do.call(\"ggarrange\", arg_list)\n# Print or display the arranged plots\nprint(arranged_plots)"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html",
    "href": "content/projects/3-adult-income/AdultIncome.html",
    "title": "\nTable of Contents\n",
    "section": "",
    "text": "Table of Contents"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#introduction",
    "href": "content/projects/3-adult-income/AdultIncome.html#introduction",
    "title": "\nTable of Contents\n",
    "section": "Introduction",
    "text": "Introduction\nA census is the procedure of systematically acquiring and recording information about the members of a given population. The census is a special, wide-range activity, which takes place once a decade in the entire country. The purpose is to gather information about the general population, in order to present a full and reliable picture of the population in the country - its housing conditions and demographic, social and economic characteristics. The information collected includes data on age, gender, country of origin, marital status, housing conditions, marriage, education, employment, etc."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#data-description",
    "href": "content/projects/3-adult-income/AdultIncome.html#data-description",
    "title": "\nTable of Contents\n",
    "section": "Data Description",
    "text": "Data Description\nThis data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). The prediction task is to determine whether a person makes less than $50K a year."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#categorical-attributes",
    "href": "content/projects/3-adult-income/AdultIncome.html#categorical-attributes",
    "title": "\nTable of Contents\n",
    "section": "Categorical Attributes",
    "text": "Categorical Attributes\nBelow is a description of all categorical predictors in the dataset.\n\nworkclass: Individual work category \n\nlevels: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.  \n\neducation: Individual’s highest education degree \n\nlevels: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.  \n\nmarital-status: Individual marital status \n\nlevels: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.  \n\noccupation: Individual’s occupation \n\nlevels: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.  \n\nrelationship: Individual’s relation in a family \n\nlevels: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.  \n\nrace: Race of Individual \n\nlevels: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.  \n\nsex: Individual’s sex \n\nlevels: Female, Male.  \n\nnative-country: Individual’s native country \n\nlevels: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#continuous-attributes",
    "href": "content/projects/3-adult-income/AdultIncome.html#continuous-attributes",
    "title": "\nTable of Contents\n",
    "section": "Continuous Attributes",
    "text": "Continuous Attributes\nBelow is a description of all the continuous predictors in the dataset.\n\nage: Age of an individual \nfnlwgt: final weight \ncapital-gain \ncapital-loss \nhours-per-week: Individual’s working hour per week"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#load-libraries",
    "href": "content/projects/3-adult-income/AdultIncome.html#load-libraries",
    "title": "\nTable of Contents\n",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n# Import packages.\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport scikitplot\nimport missingno as msno\nfrom pathlib import Path\nimport warnings\nimport random\nfrom lime import lime_tabular\nimport shap\nfrom sklearn import ensemble, preprocessing, tree, model_selection, metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost\nfrom mlxtend.classifier import StackingClassifier\nfrom yellowbrick.classifier import ConfusionMatrix, ROCAUC, ClassificationReport, PrecisionRecallCurve, DiscriminationThreshold, ClassPredictionError, ClassBalance\nfrom yellowbrick.model_selection import LearningCurve, ValidationCurve"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#suppress-warnings",
    "href": "content/projects/3-adult-income/AdultIncome.html#suppress-warnings",
    "title": "\nTable of Contents\n",
    "section": "Suppress Warnings",
    "text": "Suppress Warnings\nWarnings usually don’t relate to anything that will affect the actual analysis, so I turn them off.\n\n# Turn warnings off globally.\ndef warn(*args, **kwargs):\n    pass\n\n#import warnings\nwarnings.warn = warn"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#set-seed",
    "href": "content/projects/3-adult-income/AdultIncome.html#set-seed",
    "title": "\nTable of Contents\n",
    "section": "Set Seed",
    "text": "Set Seed\n\n# Seed for reproducibility.\nrandom.seed(10)"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#display-options",
    "href": "content/projects/3-adult-income/AdultIncome.html#display-options",
    "title": "\nTable of Contents\n",
    "section": "Display Options",
    "text": "Display Options\n\n# Display all dataframe columns.\npd.set_option('display.max_columns', None)\n\n# Display all dataframe rows.\npd.set_option('display.max_rows', None)"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#path-manager",
    "href": "content/projects/3-adult-income/AdultIncome.html#path-manager",
    "title": "\nTable of Contents\n",
    "section": "Path Manager",
    "text": "Path Manager\n\n# Make project folder working directory.\n%cd \"C:\\Users\\STPI0560\\Desktop\\Python Projects\\Adult Income\"\n\nC:\\Users\\STPI0560\\Desktop\\Python Projects\\Adult Income"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#define-custom-functions",
    "href": "content/projects/3-adult-income/AdultIncome.html#define-custom-functions",
    "title": "\nTable of Contents\n",
    "section": "Define Custom Functions",
    "text": "Define Custom Functions\nIf I do something similar more than once, I will write a function so as not to clutter up the workspace. Each of these functions will be run multiple times throughout this analysis.\n\n# %load \"bin\\getDuplicateColumns.py\"\ndef getDuplicateColumns(df):\n    '''\n    Get a list of duplicate columns.\n    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n    :param df: Dataframe object\n    :return: List of columns whose contents are duplicates.\n    '''\n    # Define empty set.\n    duplicateColumnNames = set()\n\n    # Iterate over all the columns in dataframe\n    for x in range(df.shape[1]):\n\n        # Select column at xth index.\n        col = df.iloc[:, x]\n\n        # Iterate over all the columns in DataFrame from (x+1)th index till end\n        for y in range(x + 1, df.shape[1]):\n\n            # Select column at yth index.\n            otherCol = df.iloc[:, y]\n\n            # Check if two columns at x 7 y index are equal\n            if col.equals(otherCol):\n\n                duplicateColumnNames.add(df.columns.values[y])\n\n    return list(duplicateColumnNames)\n\n\n# %load \"bin\\plotPredictors.py\"\ndef plotPredictors(data, predictor, width, height):\n    '''\n    Return a plot with frequency of categorical variables for an inputed predictor.\n    data: Input dataframe in pandas format.\n    predictor: Name of predictor column, in quotes (\"\").\n    width: Width of plot.\n    height: Height of plot.\n    '''\n    # Set plot size.\n    plt.figure(figsize = (width, height))\n    \n    # Set title.\n    plt.title(predictor)\n    \n    # Define graph.\n    ax = sns.countplot(x = predictor, data = data, hue = \"income\")\n    \n    # If predictor is occupation, tilt x-axis labels (so they fit)...\n    if predictor == \"occupation\" or \"native_country\":\n        plt.xticks(rotation=30)\n        for p in ax.patches:\n            height = p.get_height()\n            return plt.show()\n    # ... otherwise, don't tilt x-axis labels.\n    else:\n        for p in ax.patches:\n            height = p.get_height()\n            return plt.show()\n\n\n# %load \"bin\\testClassifiers.py\"\ndef testClassifiers(classifierList, X_train, y_train, X_vl, y_val):\n    '''\n    Return a dataframe with 1 row for each classifier inputed in the function's\n    arguements. Each row contains: Classifier name, accuracy, recall score, and\n    precision score.\n    data: Input: classifierList, X_train, y_train, X_test, y_test. \n    classifierList: List of classifiers you want to test. \n    Example: [DecisionTreeClassifier, KNeighborsClassifier, GaussianNB].\n    X_train: Matrix of training predictors (numeric).\n    y_train: Vector of training response variable identity (numeric).\n    X_test: Matrix of testing predictors (numeric).\n    y_test: Vector of testing response variable identity (numeric).\n    '''\n    # Make empty dataframe.\n    model_df = pd.DataFrame(columns=['Classifier', 'Accuracy', 'Recall', 'Precision', 'f1'])\n\n    # Repeat for each classifier in the list defined in the functions arguement.\n    for i in classifierList:\n        \n        # For each model:\n        model = i()\n        \n        # Get model name from current classifier.\n        model_name = type(model).__name__\n        \n        # Fit the data with the training model.\n        model.fit(X_train, y_train)\n        \n        # make predictions on test data using training model.\n        yhat = model.predict(X_val)\n        \n        # test accuracy for current model.\n        acc = accuracy_score(y_val, yhat)\n        \n        # test recall for current model\n        recall = recall_score(y_val, yhat)\n        \n        # test precision for current model.\n        precision = precision_score(y_val, yhat)\n        \n        f1 = f1_score(y_val, yhat)\n    \n        # Create list for model i with caculated information.\n        row = [model_name, acc, recall, precision, f1]\n        \n        # Add list as new row in model_df.\n        model_df.loc[len(model_df)] = row\n    \n    # Return dataframe with performance of each model.\n    return model_df\n\n\ndef testxgboost(classifierList, X_train, y_train, X_vl, y_val):\n    '''\n    Return a dataframe with 1 row for each classifier inputed in the function's\n    arguements. Each row contains: Classifier name, accuracy, recall score, and\n    precision score.\n    data: Input: classifierList, X_train, y_train, X_test, y_test. \n    classifierList: List of classifiers you want to test. \n    Example: [DecisionTreeClassifier, KNeighborsClassifier, GaussianNB].\n    X_train: Matrix of training predictors (numeric).\n    y_train: Vector of training response variable identity (numeric).\n    X_test: Matrix of testing predictors (numeric).\n    y_test: Vector of testing response variable identity (numeric).\n    '''\n    # Make empty dataframe.\n    model_df = pd.DataFrame(columns=['Classifier', 'Accuracy', 'Recall', 'Precision', 'f1'])\n\n    # Repeat for each classifier in the list defined in the functions arguement.\n    for i in classifierList:\n        \n        # For each model:\n        model = i(random_state = 42,\n                              eta = 0.1,\n                              ubsample = 0.8,\n                              colsample_bytree = 0.2,\n                              max_depth = 4,\n                              min_child_weight = 1,\n                             use_label_encoder = False)\n        \n        # Get model name from current classifier.\n        model_name = type(model).__name__\n        \n        # Fit the data with the training model.\n        model.fit(X_train, y_train)\n        \n        # make predictions on test data using training model.\n        yhat = model.predict(X_val)\n        \n        # test accuracy for current model.\n        acc = accuracy_score(y_val, yhat)\n        \n        # test recall for current model\n        recall = recall_score(y_val, yhat)\n        \n        # test precision for current model.\n        precision = precision_score(y_val, yhat)\n        \n        f1 = f1_score(y_val, yhat)\n    \n        # Create list for model i with caculated information.\n        row = [model_name, acc, recall, precision, f1]\n        \n        # Add list as new row in model_df.\n        model_df.loc[len(model_df)] = row\n    \n    # Return dataframe with performance of each model.\n    return model_df"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#load-data",
    "href": "content/projects/3-adult-income/AdultIncome.html#load-data",
    "title": "\nTable of Contents\n",
    "section": "Load Data",
    "text": "Load Data\n\n# Read data.\ndf = pd.read_csv('data\\Adult.csv')"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#check-data-structure",
    "href": "content/projects/3-adult-income/AdultIncome.html#check-data-structure",
    "title": "\nTable of Contents\n",
    "section": "Check Data Structure",
    "text": "Check Data Structure\n\n# Look at first few rows of dataframe.\ndf.head()\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducational-num\nmarital-status\noccupation\nrelationship\nrace\ngender\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n25\nPrivate\n226802\n11th\n7\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n38\nPrivate\n89814\nHS-grad\n9\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0\n0\n50\nUnited-States\n&lt;=50K\n\n\n2\n28\nLocal-gov\n336951\nAssoc-acdm\n12\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n3\n44\nPrivate\n160323\nSome-college\n10\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n7688\n0\n40\nUnited-States\n&gt;50K\n\n\n4\n18\n?\n103497\nSome-college\n10\nNever-married\n?\nOwn-child\nWhite\nFemale\n0\n0\n30\nUnited-States\n&lt;=50K\n\n\n\n\n\n\n\n\n# Check shape of data.\ndf.shape\n\n(48842, 15)\n\n\n\n# Check data type of each column.\nprint('Data type of each column of Dataframe :')\ndf.dtypes\n\nData type of each column of Dataframe :\n\n\nage                 int64\nworkclass          object\nfnlwgt              int64\neducation          object\neducational-num     int64\nmarital-status     object\noccupation         object\nrelationship       object\nrace               object\ngender             object\ncapital-gain        int64\ncapital-loss        int64\nhours-per-week      int64\nnative-country     object\nincome             object\ndtype: object\n\n\n\n# Summary statistics.\ndf.describe(include = 'all')\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducational-num\nmarital-status\noccupation\nrelationship\nrace\ngender\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\ncount\n48842.000000\n48842\n4.884200e+04\n48842\n48842.000000\n48842\n48842\n48842\n48842\n48842\n48842.000000\n48842.000000\n48842.000000\n48842\n48842\n\n\nunique\nNaN\n9\nNaN\n16\nNaN\n7\n15\n6\n5\n2\nNaN\nNaN\nNaN\n42\n2\n\n\ntop\nNaN\nPrivate\nNaN\nHS-grad\nNaN\nMarried-civ-spouse\nProf-specialty\nHusband\nWhite\nMale\nNaN\nNaN\nNaN\nUnited-States\n&lt;=50K\n\n\nfreq\nNaN\n33906\nNaN\n15784\nNaN\n22379\n6172\n19716\n41762\n32650\nNaN\nNaN\nNaN\n43832\n37155\n\n\nmean\n38.643585\nNaN\n1.896641e+05\nNaN\n10.078089\nNaN\nNaN\nNaN\nNaN\nNaN\n1079.067626\n87.502314\n40.422382\nNaN\nNaN\n\n\nstd\n13.710510\nNaN\n1.056040e+05\nNaN\n2.570973\nNaN\nNaN\nNaN\nNaN\nNaN\n7452.019058\n403.004552\n12.391444\nNaN\nNaN\n\n\nmin\n17.000000\nNaN\n1.228500e+04\nNaN\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n1.000000\nNaN\nNaN\n\n\n25%\n28.000000\nNaN\n1.175505e+05\nNaN\n9.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\nNaN\n\n\n50%\n37.000000\nNaN\n1.781445e+05\nNaN\n10.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n40.000000\nNaN\nNaN\n\n\n75%\n48.000000\nNaN\n2.376420e+05\nNaN\n12.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n45.000000\nNaN\nNaN\n\n\nmax\n90.000000\nNaN\n1.490400e+06\nNaN\n16.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n99999.000000\n4356.000000\n99.000000\nNaN\nNaN\n\n\n\n\n\n\n\nDashes (-) can sometimes cause problems in predictor names. Below I change all dashes to underscores.\n\n# Remove '-' from column names.\ndf = df.rename(columns = {'educational-num': 'educational_num', 'marital-status': 'marital_status', 'capital-gain': 'capital_gain', 'capital-loss': 'capital_loss', 'hours-per-week': 'hours_per_week', 'native-country': 'native_country'})"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#missing-data-list",
    "href": "content/projects/3-adult-income/AdultIncome.html#missing-data-list",
    "title": "\nTable of Contents\n",
    "section": "Missing Data List",
    "text": "Missing Data List\n\n# Print a list of each column that has at least 1 missing value.\nprint('List of columns with missing values:', [col for col in df.columns if df[col].isnull().any()], '\\n')\nprint('Number of missing values per column:')\n\n# Number of missing variables for each predictor, as a percentage.\ndf.isnull().mean() * 100\n\nList of columns with missing values: [] \n\nNumber of missing values per column:\n\n\nage                0.0\nworkclass          0.0\nfnlwgt             0.0\neducation          0.0\neducational_num    0.0\nmarital_status     0.0\noccupation         0.0\nrelationship       0.0\nrace               0.0\ngender             0.0\ncapital_gain       0.0\ncapital_loss       0.0\nhours_per_week     0.0\nnative_country     0.0\nincome             0.0\ndtype: float64\n\n\nConclusion: Luck for me, there is no missing data at all (the data were likely cleaned prior to being put online)."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#workclass",
    "href": "content/projects/3-adult-income/AdultIncome.html#workclass",
    "title": "\nTable of Contents\n",
    "section": "Workclass",
    "text": "Workclass\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"workclass\" labels.\nplotPredictors(df, 'workclass', 12, 7)\n\n\n\n\n\n\n\n\nSome data are labelled “?”. I could deal with this in a miriad of ways. I could remove all observations, or code a new column that is 1 when that observation was missing data, and 0 otherwise. Additionally, I could replace all “?” labels with the most frequent class label. To make this more concrete, I’ll look at the percentage of the dataset containing people whose workclass is labelled “Private”, and what precentage are labelled “?”.\n\n# Display percentage of each instance in \"workclass\" column,\ndf['workclass'].value_counts(normalize = True) * 100\n\nPrivate             69.419762\nSelf-emp-not-inc     7.907129\nLocal-gov            6.420703\n?                    5.730724\nState-gov            4.055935\nSelf-emp-inc         3.470374\nFederal-gov          2.931903\nWithout-pay          0.042996\nNever-worked         0.020474\nName: workclass, dtype: float64\n\n\n69% of workclass are labelled “Private”, and only 5% are labelled “?”. Therefore, I’m going to change all instances of “?” to “Private”.\n\n# Replace all instances of \"?\" in the \"worclass\" column with \"Private\",\ndf['workclass'] = df['workclass'].str.replace('?', 'Private')\n\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Replot \"workclass\" labels.\nplotPredictors(df, 'workclass', 12, 8)\n\n\n\n\n\n\n\n\nConclusion: The majority of people in all working class categories make less than &lt;=50K, with the exception of those who are self-employed."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#education",
    "href": "content/projects/3-adult-income/AdultIncome.html#education",
    "title": "\nTable of Contents\n",
    "section": "Education",
    "text": "Education\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"education\" column labels.\nplotPredictors(df, 'education', 20, 8)\n\n\n\n\n\n\n\n\nConclusion: There is some data from individuals who do not have more than a preschool education, which is interesting given the minimum age of the dataset is 17. These won’t be removed, but I may look at this later on.\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"educational_num\" column labels.\nplotPredictors(df, 'educational_num', 12, 8)\n\n\n\n\n\n\n\n\nConclusion: This variable looks like a categorical version of “education”. Thus, it is redundent and can probably be removed.\n\n# Drop \"educational_num\" column.\ndf = df.drop(columns = ['educational_num'])"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#marital-status",
    "href": "content/projects/3-adult-income/AdultIncome.html#marital-status",
    "title": "\nTable of Contents\n",
    "section": "Marital-status",
    "text": "Marital-status\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"marital_status\" column labels.\nplotPredictors(df, 'marital_status', 14, 8)\n\n\n\n\n\n\n\n\nConclusion: Unsurprisingly, married people seem to be the group who make a majority of people making &gt;50K."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#occupation",
    "href": "content/projects/3-adult-income/AdultIncome.html#occupation",
    "title": "\nTable of Contents\n",
    "section": "Occupation",
    "text": "Occupation\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"occupation\" column labels.\nplotPredictors(df, 'occupation', 18, 12)\n\n\n\n\n\n\n\n\nConclusion: Like before, some data are labelled “?”. I’ll quickly look at which labels occur in the greatest number.\n\n# Display percentage of each instance in \"workclass\" column,\ndf['occupation'].value_counts(normalize = True) * 100\n\nProf-specialty       12.636665\nCraft-repair         12.513820\nExec-managerial      12.460587\nAdm-clerical         11.488064\nSales                11.268990\nOther-service        10.079440\nMachine-op-inspct     6.187298\n?                     5.751198\nTransport-moving      4.821670\nHandlers-cleaners     4.242251\nFarming-fishing       3.050653\nTech-support          2.960567\nProtective-serv       2.012612\nPriv-house-serv       0.495475\nArmed-Forces          0.030711\nName: occupation, dtype: float64\n\n\nI could randomly assign the “?” labels to the top few most represented labels, but for consistency, I will assign them all to the top label “Prof-specialty”.\n\n# Replace all instances of \"?\" with \"Prof-specialty\".\ndf['occupation'] = df['occupation'].str.replace('?', 'Prof-speciality')\n\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Replot \"occupation\" column labels.\nplotPredictors(df, 'occupation', 20, 8)\n\n\n\n\n\n\n\n\nConclusion: A few groups (e.g. “Exec-managerial”, “Prof-specialty”) seem to make the majority of people making &gt;50K."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#relationship",
    "href": "content/projects/3-adult-income/AdultIncome.html#relationship",
    "title": "\nTable of Contents\n",
    "section": "Relationship",
    "text": "Relationship\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"relationship\" column labels.\nplotPredictors(df, 'relationship', 10, 8)\n\n\n\n\n\n\n\n\nConclusion: While wives are much less represented (probably because the dataset has more males), they seem to be just as likely to be in either category of the response variable. However, married people in general earn more."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#race",
    "href": "content/projects/3-adult-income/AdultIncome.html#race",
    "title": "\nTable of Contents\n",
    "section": "Race",
    "text": "Race\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"race\" column labels.\nplotPredictors(df, 'race', 10, 8)\n\n\n\n\n\n\n\n\nConclusion: The race “White” seems to be more likely to make &gt;50K, but they are also magnitudes more represented than any other group. For instance, “Asian-Pac-Islander” seems to make up proportionally the same amount of each response variable, but they are very underrepresented."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#gender",
    "href": "content/projects/3-adult-income/AdultIncome.html#gender",
    "title": "\nTable of Contents\n",
    "section": "Gender",
    "text": "Gender\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"gender\" column labels.\nplotPredictors(df, 'gender', 7, 8)\n\n\n\n\n\n\n\n\nConclusion: Males are more likely to make &gt;50K than females, even accounting for their greater representation in the dataset."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#native-country",
    "href": "content/projects/3-adult-income/AdultIncome.html#native-country",
    "title": "\nTable of Contents\n",
    "section": "Native-Country",
    "text": "Native-Country\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"gender\" column labels.\nplotPredictors(df, 'native_country', 15, 8)\n\n\n\n\n\n\n\n\nUnited states is so dominant, it is hard to see the other groups. I should remove the United States to see the others better.\n\n# Remove United-States from graph.\ndf2 = df[df['native_country'] != 'United-States']\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"gender\" column labels.\nplotPredictors(df2, 'native_country', 15, 8)\n\n\n\n\n\n\n\n\nNote that the order of colors for the response variable has been flipped now (I won’t correct this because it will only affect this graph). It appears that no “native_country” group dominates the &gt;50K label. However, some data are labelled “?”. I should see how they are represented compared to the other groups.\n\n# Display percentage of each instance in \"workclass\" column,\ndf['native_country'].value_counts(normalize = True) * 100\n\nUnited-States                 89.742435\nMexico                         1.947095\n?                              1.754637\nPhilippines                    0.603988\nGermany                        0.421768\nPuerto-Rico                    0.376725\nCanada                         0.372630\nEl-Salvador                    0.317350\nIndia                          0.309160\nCuba                           0.282544\nEngland                        0.260022\nChina                          0.249785\nSouth                          0.235453\nJamaica                        0.217026\nItaly                          0.214979\nDominican-Republic             0.210884\nJapan                          0.188362\nGuatemala                      0.180173\nPoland                         0.178125\nVietnam                        0.176078\nColumbia                       0.174031\nHaiti                          0.153556\nPortugal                       0.137177\nTaiwan                         0.133082\nIran                           0.120798\nNicaragua                      0.100323\nGreece                         0.100323\nPeru                           0.094181\nEcuador                        0.092134\nFrance                         0.077802\nIreland                        0.075754\nHong                           0.061423\nThailand                       0.061423\nCambodia                       0.057328\nTrinadad&Tobago                0.055280\nLaos                           0.047091\nYugoslavia                     0.047091\nOutlying-US(Guam-USVI-etc)     0.047091\nScotland                       0.042996\nHonduras                       0.040948\nHungary                        0.038901\nHoland-Netherlands             0.002047\nName: native_country, dtype: float64\n\n\n1.7% of respondents have “?” as a country. Since the United States covers 89% of respondents native countries, I will replace all “?” with “United-States”. Additionally, 1.75% of respondents are listed as being from “Inited-States”. They will all be changed to “United-States” as well.\n\n# Replace all instances of \"?\" in the \"worclass\" column with \"Private\".\ndf['native_country'] = df['native_country'].str.replace('?','United-States')\n\n# Replace all instances of \"Inited-States\" in the \"worclass\" column with \"United-States\".\ndf['native_country'] = df['native_country'].str.replace('Inited-States','United-States')\n\n\n# Remove United-States from graph.\ndf2 = df[df['native_country'] != 'United-States']\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Plot \"gender\" column labels.\nplotPredictors(df2, 'native_country', 15, 8)\n\n\n\n\n\n\n\n\nConclusion: No group is more likely to make &gt;50K, but groups like Guatemala are far more likely to make &gt;50K."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#capital-gaincapital-loss",
    "href": "content/projects/3-adult-income/AdultIncome.html#capital-gaincapital-loss",
    "title": "\nTable of Contents\n",
    "section": "Capital-Gain/Capital-Loss",
    "text": "Capital-Gain/Capital-Loss\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 4))\n\n# Plot Capital-Gains/Captial-Loss graph.\nsns.scatterplot(data = df, x = 'capital_gain', y = 'capital_loss', hue = 'income');\n\n\n\n\n\n\n\n\nConclusion: When people have zero capital gain, they have large capital-loss, and vis-versa. Perhaps these can be combined into a “capital-diff” difference score variable.\n\n# Make column \"capital_diff\" by taking the difference between \"capital_gain\" and \"capital_loss\".\ndf['capital_diff'] = df['capital_loss'] - df['capital_gain']\n\n# Drop columns \"capital_gain\" and \"capital_loss\".\ndf = df.drop(columns = ['capital_gain', 'capital_loss'])"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#age",
    "href": "content/projects/3-adult-income/AdultIncome.html#age",
    "title": "\nTable of Contents\n",
    "section": "Age",
    "text": "Age\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (12, 8))\n\n# Make indicator for \"income\" plot.\nmask = df['income'] == '&lt;=50K'\n\n# Split data by indicator.\nax = sns.distplot(df[mask].age, label = '&lt;=50K')\nax = sns.distplot(df[~mask].age,label = '&gt;50K')\n\n# Add legend.\nax.legend();\n\n\n\n\n\n\n\n\nConclusion: People making &lt;=50K are skewed towards being younger, however, these proportions become almost identical once people reach retirement.\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Display box and whisker plot for \"age\" column.\ndf['age'].plot(kind = 'box')\n\n# Display plot.\nplt.show()\n\n\n\n\n\n\n\n\nConclusion: Their appear to be some outliers, but that’s fine here. However, looking at the previous summary, the youngest person in the dataset is 17, yet there are people whose education stops at preschool. How is that possible? I’m going to seguay into a look at this here.\n\n# Print minimum age.\nprint('The minimum age is:', df['age'].min(), '\\n')\n\n# Print list of people with preschool-only education.\nprint('List of people with Preschool education: \\n')\ndf.loc[df['education'] == 'Preschool']\n\nThe minimum age is: 17 \n\nList of people with Preschool education: \n\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\nmarital_status\noccupation\nrelationship\nrace\ngender\nhours_per_week\nnative_country\nincome\ncapital_diff\n\n\n\n\n779\n64\nPrivate\n86837\nPreschool\nMarried-civ-spouse\nHandlers-cleaners\nHusband\nAsian-Pac-Islander\nMale\n40\nPhilippines\n&lt;=50K\n0\n\n\n818\n21\nPrivate\n243368\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n25\nMexico\n&lt;=50K\n0\n\n\n1029\n57\nPrivate\n274680\nPreschool\nSeparated\nProf-speciality\nNot-in-family\nWhite\nMale\n40\nUnited-States\n&lt;=50K\n0\n\n\n1059\n31\nPrivate\n25610\nPreschool\nNever-married\nHandlers-cleaners\nNot-in-family\nAmer-Indian-Eskimo\nMale\n25\nUnited-States\n&lt;=50K\n0\n\n\n1489\n19\nPrivate\n277695\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n36\nMexico\n&lt;=50K\n0\n\n\n1498\n37\nSelf-emp-not-inc\n227253\nPreschool\nMarried-civ-spouse\nSales\nHusband\nWhite\nMale\n30\nMexico\n&lt;=50K\n0\n\n\n2364\n21\nPrivate\n436431\nPreschool\nMarried-civ-spouse\nProf-speciality\nOther-relative\nWhite\nFemale\n40\nMexico\n&lt;=50K\n0\n\n\n2465\n24\nPrivate\n403107\nPreschool\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n3037\n54\nPrivate\n99208\nPreschool\nMarried-civ-spouse\nProf-speciality\nHusband\nWhite\nMale\n16\nUnited-States\n&lt;=50K\n0\n\n\n3540\n29\nPrivate\n565769\nPreschool\nNever-married\nProf-speciality\nNot-in-family\nBlack\nMale\n40\nSouth\n&lt;=50K\n0\n\n\n4426\n30\nPrivate\n408328\nPreschool\nMarried-spouse-absent\nHandlers-cleaners\nUnmarried\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n4629\n28\nPrivate\n203784\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n38\nMexico\n&lt;=50K\n0\n\n\n4729\n50\nPrivate\n176773\nPreschool\nMarried-civ-spouse\nFarming-fishing\nHusband\nBlack\nMale\n40\nHaiti\n&lt;=50K\n0\n\n\n5795\n22\nPrivate\n267412\nPreschool\nNever-married\nOther-service\nOwn-child\nBlack\nFemale\n20\nJamaica\n&lt;=50K\n-594\n\n\n7054\n77\nSelf-emp-not-inc\n161552\nPreschool\nWidowed\nExec-managerial\nNot-in-family\nWhite\nFemale\n60\nUnited-States\n&lt;=50K\n0\n\n\n7307\n60\nSelf-emp-not-inc\n269485\nPreschool\nDivorced\nOther-service\nUnmarried\nWhite\nFemale\n40\nMexico\n&lt;=50K\n0\n\n\n7438\n61\nSelf-emp-not-inc\n243019\nPreschool\nMarried-civ-spouse\nCraft-repair\nHusband\nWhite\nMale\n40\nUnited-States\n&lt;=50K\n0\n\n\n7485\n37\nPrivate\n216845\nPreschool\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n7736\n30\nPrivate\n90308\nPreschool\nNever-married\nOther-service\nUnmarried\nWhite\nMale\n28\nEl-Salvador\n&lt;=50K\n0\n\n\n7773\n19\nPrivate\n277695\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n50\nMexico\n&lt;=50K\n0\n\n\n10304\n50\nPrivate\n330543\nPreschool\nMarried-civ-spouse\nOther-service\nHusband\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n10721\n47\nPrivate\n98044\nPreschool\nNever-married\nOther-service\nNot-in-family\nWhite\nMale\n25\nEl-Salvador\n&lt;=50K\n0\n\n\n10777\n53\nPrivate\n308082\nPreschool\nNever-married\nOther-service\nNot-in-family\nWhite\nFemale\n15\nEl-Salvador\n&lt;=50K\n0\n\n\n10954\n33\nPrivate\n295591\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n11456\n50\nPrivate\n193081\nPreschool\nNever-married\nOther-service\nNot-in-family\nBlack\nFemale\n40\nHaiti\n&lt;=50K\n0\n\n\n11677\n47\nPrivate\n235431\nPreschool\nNever-married\nSales\nUnmarried\nBlack\nFemale\n40\nHaiti\n&lt;=50K\n0\n\n\n13568\n51\nPrivate\n186299\nPreschool\nNever-married\nMachine-op-inspct\nNot-in-family\nWhite\nMale\n30\nUnited-States\n&lt;=50K\n0\n\n\n13582\n43\nSelf-emp-not-inc\n245056\nPreschool\nMarried-civ-spouse\nTransport-moving\nHusband\nBlack\nMale\n40\nHaiti\n&lt;=50K\n0\n\n\n14153\n21\nPrivate\n243368\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n50\nMexico\n&lt;=50K\n0\n\n\n15513\n35\nPrivate\n290498\nPreschool\nMarried-civ-spouse\nCraft-repair\nHusband\nWhite\nMale\n38\nMexico\n&lt;=50K\n0\n\n\n15654\n60\nPrivate\n225894\nPreschool\nWidowed\nProf-speciality\nNot-in-family\nWhite\nFemale\n40\nGuatemala\n&lt;=50K\n0\n\n\n15964\n61\nPrivate\n194804\nPreschool\nSeparated\nTransport-moving\nNot-in-family\nBlack\nMale\n40\nUnited-States\n&gt;50K\n-14344\n\n\n16505\n53\nLocal-gov\n140359\nPreschool\nNever-married\nMachine-op-inspct\nNot-in-family\nWhite\nFemale\n35\nUnited-States\n&lt;=50K\n0\n\n\n17213\n51\nLocal-gov\n241843\nPreschool\nMarried-civ-spouse\nOther-service\nHusband\nWhite\nMale\n40\nUnited-States\n&lt;=50K\n0\n\n\n19165\n71\nPrivate\n235079\nPreschool\nWidowed\nCraft-repair\nUnmarried\nBlack\nMale\n10\nUnited-States\n&lt;=50K\n0\n\n\n19227\n31\nPrivate\n452405\nPreschool\nNever-married\nOther-service\nOther-relative\nWhite\nFemale\n35\nMexico\n&lt;=50K\n0\n\n\n19727\n33\nPrivate\n239781\nPreschool\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n19873\n39\nPrivate\n362685\nPreschool\nWidowed\nProf-speciality\nNot-in-family\nWhite\nFemale\n20\nEl-Salvador\n&lt;=50K\n0\n\n\n20388\n52\nPrivate\n416129\nPreschool\nMarried-civ-spouse\nOther-service\nNot-in-family\nWhite\nMale\n40\nEl-Salvador\n&lt;=50K\n0\n\n\n22714\n27\nPrivate\n211032\nPreschool\nMarried-civ-spouse\nFarming-fishing\nOther-relative\nWhite\nMale\n24\nMexico\n&lt;=50K\n-41310\n\n\n23145\n54\nPrivate\n286989\nPreschool\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n60\nUnited-States\n&lt;=50K\n0\n\n\n23351\n30\nPrivate\n193598\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n23454\n64\nPrivate\n140237\nPreschool\nMarried-civ-spouse\nProf-speciality\nHusband\nWhite\nMale\n40\nUnited-States\n&lt;=50K\n0\n\n\n24175\n26\nPrivate\n322614\nPreschool\nMarried-spouse-absent\nMachine-op-inspct\nNot-in-family\nWhite\nMale\n40\nMexico\n&lt;=50K\n1719\n\n\n24361\n21\nPrivate\n243368\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n50\nMexico\n&lt;=50K\n0\n\n\n24369\n54\nPrivate\n148657\nPreschool\nMarried-civ-spouse\nProf-speciality\nWife\nWhite\nFemale\n40\nMexico\n&lt;=50K\n0\n\n\n24377\n52\nPrivate\n248113\nPreschool\nMarried-spouse-absent\nProf-speciality\nOther-relative\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n25056\n20\nPrivate\n277700\nPreschool\nNever-married\nOther-service\nOwn-child\nWhite\nMale\n32\nUnited-States\n&lt;=50K\n0\n\n\n26591\n59\nPrivate\n157305\nPreschool\nNever-married\nMachine-op-inspct\nNot-in-family\nWhite\nMale\n40\nDominican-Republic\n&lt;=50K\n0\n\n\n27415\n32\nPrivate\n112137\nPreschool\nMarried-civ-spouse\nMachine-op-inspct\nWife\nAsian-Pac-Islander\nFemale\n40\nCambodia\n&lt;=50K\n-4508\n\n\n27641\n53\nPrivate\n188644\nPreschool\nMarried-civ-spouse\nOther-service\nHusband\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n28015\n65\nPrivate\n293385\nPreschool\nMarried-civ-spouse\nProf-speciality\nHusband\nBlack\nMale\n30\nUnited-States\n&lt;=50K\n0\n\n\n29529\n68\nPrivate\n168794\nPreschool\nNever-married\nMachine-op-inspct\nNot-in-family\nWhite\nMale\n10\nUnited-States\n&lt;=50K\n0\n\n\n31340\n21\nPrivate\n243368\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n50\nMexico\n&lt;=50K\n0\n\n\n32778\n75\nPrivate\n71898\nPreschool\nNever-married\nPriv-house-serv\nNot-in-family\nAsian-Pac-Islander\nFemale\n48\nPhilippines\n&lt;=50K\n0\n\n\n32843\n46\nPrivate\n225065\nPreschool\nMarried-civ-spouse\nMachine-op-inspct\nWife\nWhite\nFemale\n40\nMexico\n&lt;=50K\n0\n\n\n34696\n24\nPrivate\n243368\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n36\nMexico\n&lt;=50K\n0\n\n\n36721\n63\nPrivate\n440607\nPreschool\nMarried-civ-spouse\nProf-specialty\nHusband\nOther\nMale\n30\nMexico\n&lt;=50K\n0\n\n\n37651\n61\nPrivate\n98350\nPreschool\nMarried-spouse-absent\nOther-service\nNot-in-family\nAsian-Pac-Islander\nMale\n40\nChina\n&lt;=50K\n0\n\n\n37669\n24\nPrivate\n196678\nPreschool\nNever-married\nMachine-op-inspct\nOwn-child\nWhite\nFemale\n30\nUnited-States\n&lt;=50K\n0\n\n\n38003\n49\nPrivate\n149809\nPreschool\nMarried-civ-spouse\nOther-service\nHusband\nWhite\nMale\n40\nUnited-States\n&lt;=50K\n0\n\n\n38075\n41\nLocal-gov\n160893\nPreschool\nNever-married\nHandlers-cleaners\nOwn-child\nWhite\nFemale\n30\nUnited-States\n&lt;=50K\n0\n\n\n38448\n39\nPrivate\n341741\nPreschool\nNever-married\nOther-service\nNot-in-family\nWhite\nFemale\n12\nUnited-States\n&lt;=50K\n0\n\n\n38812\n40\nPrivate\n182268\nPreschool\nMarried-spouse-absent\nAdm-clerical\nOwn-child\nWhite\nMale\n40\nUnited-States\n&lt;=50K\n0\n\n\n39221\n25\nPrivate\n266820\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n35\nMexico\n&lt;=50K\n0\n\n\n40456\n54\nPrivate\n349340\nPreschool\nMarried-civ-spouse\nCraft-repair\nHusband\nAsian-Pac-Islander\nMale\n40\nIndia\n&lt;=50K\n0\n\n\n40839\n42\nPrivate\n572751\nPreschool\nMarried-civ-spouse\nCraft-repair\nHusband\nWhite\nMale\n40\nNicaragua\n&lt;=50K\n0\n\n\n40979\n32\nPrivate\n223212\nPreschool\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n40\nMexico\n&lt;=50K\n0\n\n\n41381\n48\nPrivate\n209182\nPreschool\nSeparated\nOther-service\nUnmarried\nWhite\nFemale\n40\nEl-Salvador\n&lt;=50K\n0\n\n\n41394\n23\nPrivate\n69911\nPreschool\nNever-married\nOther-service\nOwn-child\nWhite\nFemale\n15\nUnited-States\n&lt;=50K\n0\n\n\n41508\n23\nPrivate\n240049\nPreschool\nNever-married\nOther-service\nNot-in-family\nAsian-Pac-Islander\nFemale\n40\nLaos\n&lt;=50K\n0\n\n\n41933\n42\nPrivate\n144995\nPreschool\nNever-married\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n25\nUnited-States\n&lt;=50K\n0\n\n\n42224\n19\nPrivate\n277695\nPreschool\nNever-married\nFarming-fishing\nNot-in-family\nWhite\nMale\n36\nHong\n&lt;=50K\n0\n\n\n42782\n52\nPrivate\n370552\nPreschool\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n40\nEl-Salvador\n&lt;=50K\n0\n\n\n42887\n54\nPrivate\n175262\nPreschool\nMarried-civ-spouse\nCraft-repair\nHusband\nAsian-Pac-Islander\nMale\n40\nChina\n&lt;=50K\n0\n\n\n43433\n66\nPrivate\n236879\nPreschool\nWidowed\nPriv-house-serv\nOther-relative\nWhite\nFemale\n40\nGuatemala\n&lt;=50K\n0\n\n\n43520\n34\nLocal-gov\n144182\nPreschool\nNever-married\nAdm-clerical\nOwn-child\nBlack\nFemale\n25\nUnited-States\n&lt;=50K\n0\n\n\n44676\n36\nPrivate\n252231\nPreschool\nNever-married\nMachine-op-inspct\nNot-in-family\nBlack\nMale\n40\nPuerto-Rico\n&lt;=50K\n0\n\n\n48079\n31\nState-gov\n77634\nPreschool\nNever-married\nOther-service\nNot-in-family\nWhite\nMale\n24\nUnited-States\n&lt;=50K\n0\n\n\n48316\n40\nPrivate\n566537\nPreschool\nMarried-civ-spouse\nOther-service\nHusband\nWhite\nMale\n40\nMexico\n&lt;=50K\n1672\n\n\n48505\n40\nPrivate\n70645\nPreschool\nNever-married\nOther-service\nNot-in-family\nWhite\nFemale\n20\nUnited-States\n&lt;=50K\n0\n\n\n48640\n46\nPrivate\n139514\nPreschool\nMarried-civ-spouse\nMachine-op-inspct\nOther-relative\nBlack\nMale\n75\nDominican-Republic\n&lt;=50K\n0\n\n\n48713\n36\nPrivate\n208068\nPreschool\nDivorced\nOther-service\nNot-in-family\nOther\nMale\n72\nMexico\n&lt;=50K\n0\n\n\n\n\n\n\n\nPeople with only a pre-school education don’t seem to have any obvious trends. It is definitely possible for for someone in the United States to never go to school as a kid/teenager (I looked it up), so this seems entirely possible and I will keep them in the dataset."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#hours-per-week",
    "href": "content/projects/3-adult-income/AdultIncome.html#hours-per-week",
    "title": "\nTable of Contents\n",
    "section": "Hours-per-week",
    "text": "Hours-per-week\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (12, 8))\n\n# Make indicator for \"income\" plot.\nmask = df['income'] == '&lt;=50K'\n\n# Split data by indicator.\nax = sns.distplot(df[mask]['hours_per_week'], label = '&lt;=50K')\nax = sns.distplot(df[~mask]['hours_per_week'],label = '&gt;50K')\n\n# Add legend.\nax.legend();\n\n\n\n\n\n\n\n\nConclusion: Most people seem to work a standard 40 hour work week. People who work less than 40 hours per week typically make &lt;=50K, while people who work more than 40 hours per week typically make &gt;50K.\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Display box and whisker plots.\ndf['hours_per_week'].plot(kind = 'box')\n\n# Display plot.\nplt.show()\n\n\n\n\n\n\n\n\nConclusion: There are a lot’s of outliers for “hours_per_week”, but that is probably because the vast majority of people work a 40 hour workweek."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#capital_diff",
    "href": "content/projects/3-adult-income/AdultIncome.html#capital_diff",
    "title": "\nTable of Contents\n",
    "section": "Capital_diff",
    "text": "Capital_diff\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (12, 8))\n\n# Make indicator for \"income\" plot.\nmask = df['income'] == '&lt;=50K'\n\n# Split data by indicator.\nax = sns.distplot(df[mask]['capital_diff'], label = '&lt;=50K')\nax = sns.distplot(df[~mask]['capital_diff'],label = '&gt;50K')\n\n# Add legend.\nax.legend();\n\n\n\n\n\n\n\n\nConclusion: If people had a change in capital gain, those making &lt;=50K were more likely to have lost money, and those making &gt;50K were more likely to have earned money.\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Display box and whisker plots.\ndf['capital_diff'].plot(kind = 'box')\n\n# Display plot.\nplt.show()\n\n\n\n\n\n\n\n\nConclusion: Most people did not see any capital gain/loss. Therefore, any change in capital is an outlier."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#scaling-continuous-variables",
    "href": "content/projects/3-adult-income/AdultIncome.html#scaling-continuous-variables",
    "title": "\nTable of Contents\n",
    "section": "Scaling continuous variables",
    "text": "Scaling continuous variables\nIf continuous variable are on wildly different scales, the machine learning algorithm may run into problems. I’m going to see whether this is the case for my 3 continuous predictors.\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Set plot layout.\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot continuous predictors side-by-side.\nsns.histplot(data = df, x = 'age', kde = True, color = 'skyblue', ax = axs[0])\nsns.histplot(data = df, x = 'capital_diff', kde = True, color = 'gold', ax = axs[1])\nsns.histplot(data = df, x = 'hours_per_week', kde = True, color = 'teal', ax = axs[2])\n\n# Display plot.\nplt.show()\n\n\n\n\n\n\n\n\nConclusion: The continuous variable do not exactly fall within the same range. Standardisation will fix that.\n\n# List of variables I want to standardise.\ncol_names = ['age', 'capital_diff', 'hours_per_week']\n\n# Select variables to standardise from dataframe.\nfeatures = df[col_names]\n\n# Set StandardScaler instance.\nscaler = StandardScaler().fit(features.values)\n\n# Make array of standardised values corresponding to the columns in the dataframe.\nfeatures = scaler.transform(features.values)\n\n# Convert standardised array to pandas dataframe.\nscaled_features = pd.DataFrame(features, columns = col_names)\n\n# Glimps standardised dataframe.\nscaled_features.head()\n\n\n\n\n\n\n\n\nage\ncapital_diff\nhours_per_week\n\n\n\n\n0\n-0.995129\n0.132642\n-0.034087\n\n\n1\n-0.046942\n0.132642\n0.772930\n\n\n2\n-0.776316\n0.132642\n-0.034087\n\n\n3\n0.390683\n-0.895787\n-0.034087\n\n\n4\n-1.505691\n0.132642\n-0.841104\n\n\n\n\n\n\n\nNow the continuous predictors appear to be on a standardised scale. I’ll plot them again to see.\n\n# set a grey background.\nsns.set(style = 'darkgrid')\n\n# Set plot layout.\nfig, axs = plt.subplots(1, 3, figsize = (15, 5))\n\n# Plot continuous predictors side-by-side.\nsns.histplot(data = scaled_features, x = 'age', kde = True, color = 'skyblue', ax = axs[0])\nsns.histplot(data = scaled_features, x = 'capital_diff', kde = True, color = 'gold', ax = axs[1])\nsns.histplot(data = scaled_features, x = 'hours_per_week', kde = True, color = 'teal', ax = axs[2])\n\n# Display plot.\nplt.show()\n\n\n\n\n\n\n\n\nThey look standardised now, so I just need to replace the standardised values with the original values in the dataframe.\n\n# Replace non-standardised columns with standardised columns.\ndf = df.assign(age = scaled_features['age'], capital_diff = scaled_features['capital_diff'], hours_per_week = scaled_features['hours_per_week'])\n\n\n# Glimps dataframe.\ndf.head()\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\nmarital_status\noccupation\nrelationship\nrace\ngender\nhours_per_week\nnative_country\nincome\ncapital_diff\n\n\n\n\n0\n-0.995129\nPrivate\n226802\n11th\nNever-married\nMachine-op-inspct\nOwn-child\nBlack\nMale\n-0.034087\nUnited-States\n&lt;=50K\n0.132642\n\n\n1\n-0.046942\nPrivate\n89814\nHS-grad\nMarried-civ-spouse\nFarming-fishing\nHusband\nWhite\nMale\n0.772930\nUnited-States\n&lt;=50K\n0.132642\n\n\n2\n-0.776316\nLocal-gov\n336951\nAssoc-acdm\nMarried-civ-spouse\nProtective-serv\nHusband\nWhite\nMale\n-0.034087\nUnited-States\n&gt;50K\n0.132642\n\n\n3\n0.390683\nPrivate\n160323\nSome-college\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nBlack\nMale\n-0.034087\nUnited-States\n&gt;50K\n-0.895787\n\n\n4\n-1.505691\nPrivate\n103497\nSome-college\nNever-married\nProf-speciality\nOwn-child\nWhite\nFemale\n-0.841104\nUnited-States\n&lt;=50K\n0.132642"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#fnlwgt",
    "href": "content/projects/3-adult-income/AdultIncome.html#fnlwgt",
    "title": "\nTable of Contents\n",
    "section": "fnlwgt",
    "text": "fnlwgt\nThis variable is some sort of weighting unit. I want to see how many unique values it has.\n\n# Display number of unique \"fnlwgt\" labels.\ndf['fnlwgt'].nunique()\n\n28523\n\n\nConclusion: With so many unique labels, I’m not convinced they will be useful for machine learning, so I will drop this variable.\n\n# Drop \"fnlwgt\" from dataframe.\ndf = df.drop(columns = ['fnlwgt'])"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#dummy-coding",
    "href": "content/projects/3-adult-income/AdultIncome.html#dummy-coding",
    "title": "\nTable of Contents\n",
    "section": "Dummy Coding",
    "text": "Dummy Coding\nCategorical variables need to be numerically coded for most machine-learning algorithms. However, simply giving a unique value to each label of a predictor will imply a rank to each instance (which is not the case for any of these predictors). To get around that, each predictor will be given as many columns as instances, and will be designated 1 if that observation is an instance of it, and 0 otherwise. First, I want to see which predictors actually are categorical.\n\n# Make list displaying whether a column is continuous or object-based.\ns = (df.dtypes == 'object')\n\n# Drop income, since I want to save labels with a label encoder.\ns = s.drop(['income'])\n\n# Make list of column names with object instances.\nobject_cols = list(s[s].index)\n\n# Print names of all columns with categorical instances.\nprint('Categorical variables:', '\\n')\nprint(object_cols)\n\nCategorical variables: \n\n['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'native_country']\n\n\nEach of these variables needs to be dummy coded.\n\n# Dummy code categorical predictors. \ndf = pd.concat([df, pd.get_dummies(data = df, columns = list(s[s].index), drop_first = True)], axis = 1)\n\nNow, I can remove the none-dummy coded categorical predictors.\n\n# Remove non-dummy coded object columns.\ndf.drop(object_cols, axis = 1, inplace = True)\n\n\n# Recheck dataframe.\ndf.head()\n\n\n\n\n\n\n\n\nage\nhours_per_week\nincome\ncapital_diff\nage\nhours_per_week\nincome\ncapital_diff\nworkclass_Local-gov\nworkclass_Never-worked\nworkclass_Private\nworkclass_Self-emp-inc\nworkclass_Self-emp-not-inc\nworkclass_State-gov\nworkclass_Without-pay\neducation_11th\neducation_12th\neducation_1st-4th\neducation_5th-6th\neducation_7th-8th\neducation_9th\neducation_Assoc-acdm\neducation_Assoc-voc\neducation_Bachelors\neducation_Doctorate\neducation_HS-grad\neducation_Masters\neducation_Preschool\neducation_Prof-school\neducation_Some-college\nmarital_status_Married-AF-spouse\nmarital_status_Married-civ-spouse\nmarital_status_Married-spouse-absent\nmarital_status_Never-married\nmarital_status_Separated\nmarital_status_Widowed\noccupation_Armed-Forces\noccupation_Craft-repair\noccupation_Exec-managerial\noccupation_Farming-fishing\noccupation_Handlers-cleaners\noccupation_Machine-op-inspct\noccupation_Other-service\noccupation_Priv-house-serv\noccupation_Prof-speciality\noccupation_Prof-specialty\noccupation_Protective-serv\noccupation_Sales\noccupation_Tech-support\noccupation_Transport-moving\nrelationship_Not-in-family\nrelationship_Other-relative\nrelationship_Own-child\nrelationship_Unmarried\nrelationship_Wife\nrace_Asian-Pac-Islander\nrace_Black\nrace_Other\nrace_White\ngender_Male\nnative_country_Canada\nnative_country_China\nnative_country_Columbia\nnative_country_Cuba\nnative_country_Dominican-Republic\nnative_country_Ecuador\nnative_country_El-Salvador\nnative_country_England\nnative_country_France\nnative_country_Germany\nnative_country_Greece\nnative_country_Guatemala\nnative_country_Haiti\nnative_country_Holand-Netherlands\nnative_country_Honduras\nnative_country_Hong\nnative_country_Hungary\nnative_country_India\nnative_country_Iran\nnative_country_Ireland\nnative_country_Italy\nnative_country_Jamaica\nnative_country_Japan\nnative_country_Laos\nnative_country_Mexico\nnative_country_Nicaragua\nnative_country_Outlying-US(Guam-USVI-etc)\nnative_country_Peru\nnative_country_Philippines\nnative_country_Poland\nnative_country_Portugal\nnative_country_Puerto-Rico\nnative_country_Scotland\nnative_country_South\nnative_country_Taiwan\nnative_country_Thailand\nnative_country_Trinadad&Tobago\nnative_country_United-States\nnative_country_Vietnam\nnative_country_Yugoslavia\n\n\n\n\n0\n-0.995129\n-0.034087\n&lt;=50K\n0.132642\n-0.995129\n-0.034087\n&lt;=50K\n0.132642\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n1\n-0.046942\n0.772930\n&lt;=50K\n0.132642\n-0.046942\n0.772930\n&lt;=50K\n0.132642\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n-0.776316\n-0.034087\n&gt;50K\n0.132642\n-0.776316\n-0.034087\n&gt;50K\n0.132642\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n0.390683\n-0.034087\n&gt;50K\n-0.895787\n0.390683\n-0.034087\n&gt;50K\n-0.895787\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n-1.505691\n-0.841104\n&lt;=50K\n0.132642\n-1.505691\n-0.841104\n&lt;=50K\n0.132642\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\nNow there are far more columns than before, but they are properly coded."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#check-for-duplicate-columns",
    "href": "content/projects/3-adult-income/AdultIncome.html#check-for-duplicate-columns",
    "title": "\nTable of Contents\n",
    "section": "Check For Duplicate Columns",
    "text": "Check For Duplicate Columns\nWhen I change a lot of predictors in an analysis, mistakes can sometimes be made. I want to make sure no variables have been duplicated by mistake.\n\n# Get list of duplicate columns\nduplicateColumnNames = getDuplicateColumns(df)\nprint('Duplicate Columns are as follows:')\n\n# Loop that prints contents of duplicate list.\nfor col in duplicateColumnNames:\n    print('Column name : ', col)\n\nDuplicate Columns are as follows:\nColumn name :  age\nColumn name :  capital_diff\nColumn name :  income\nColumn name :  hours_per_week\n\n\nSomewhere along the way, these 4 predictors were duplicated. This would be redundent information for the machine-learning algorithm, so I will remove them.\n\n# Remove duplicate columns.\ndf = df.loc[:, ~df.columns.duplicated()]\n\n\n# Get list of duplicate columns\nduplicateColumnNames = getDuplicateColumns(df)\nprint('Duplicate Columns are as follows:')\n\n# Loop that prints contents of duplicate list.\nfor col in duplicateColumnNames:\n    print('Column name : ', col)\n\nDuplicate Columns are as follows:\n\n\nNow there are no duplicates, and I’, ready to prepare the machine-learning model!"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#define-predictors-and-response-variable",
    "href": "content/projects/3-adult-income/AdultIncome.html#define-predictors-and-response-variable",
    "title": "\nTable of Contents\n",
    "section": "Define Predictors And Response Variable",
    "text": "Define Predictors And Response Variable\n\n# Define response variable.\ny = df['income']\n\n# Define predictor variables.\nX = df.drop(columns = 'income')\n\n# Define label encoder.\nlab = preprocessing.LabelEncoder()\n\n# Transform response variable vector (y) to numeric.\ny = lab.fit_transform(y)\n\n# Invert numeric response variables (so that &lt;=50K is a positive class).\ny = np.where((y == 0)|(y == 1), y^1, y)\n\n# Print numerical y.\nprint('Numerical y:', y)\n\n# Invert numerical y and print label.\nprint('Labelled y:', lab.inverse_transform(y))\n\nNumerical y: [1 1 0 ... 1 1 0]\nLabelled y: ['&gt;50K' '&gt;50K' '&lt;=50K' ... '&gt;50K' '&gt;50K' '&lt;=50K']\n\n\nA cursory glance at the first and last few response variables show that “1” = “&gt;50K” and “0” = “&lt;=50K”."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#define-training-validation-and-testing-sets",
    "href": "content/projects/3-adult-income/AdultIncome.html#define-training-validation-and-testing-sets",
    "title": "\nTable of Contents\n",
    "section": "Define Training, Validation, And Testing Sets",
    "text": "Define Training, Validation, And Testing Sets\nI will add a validation set so that I can use grid search later withour contaminating the test set.\n\n# Split into train/test sets.\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2, random_state = 42,stratify = y)\n\n# Get validation for hyperparameter tuning.\nX_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size = 0.25, random_state = 1,stratify = y_train)\n\n# Print size of training, validation, and testing set for verification.\nprint('X_Train Shape =', round(X_train.shape[0]/48842 * 100),'%')\nprint('X_Validate Shape =', round(X_val.shape[0]/48842 * 100),'%')\nprint('X_Test Shape =', round(X_test.shape[0]/48842 * 100),'%')\nprint('') # Add space.\n\nX_Train Shape = 60 %\nX_Validate Shape = 20 %\nX_Test Shape = 20 %"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#testing-different-classifiers",
    "href": "content/projects/3-adult-income/AdultIncome.html#testing-different-classifiers",
    "title": "\nTable of Contents\n",
    "section": "Testing Different Classifiers",
    "text": "Testing Different Classifiers\nI’ll first test a dummy classifier, which randomly guesses which class each observation belongs to. This will be a soft benchmark that all machine learning classifiers I test must pass. Then I will test a series of different classes of classifier to get a rough benchmark for each of those. From there, I will pick the best one and further tune its parameters. Of note, the function I wrote to test each classifier tests against the validation set, and not the test set. That’s because I will further assess the winning classifier and so I don’t want to double dip.\nThe classifiers I will train are: a dummy classifier (guesses), logistic regression, decision tree, k nearest neighbor, gaussian naive bayes, random forest, and xgboost.\n\n# Create dataframe of classifier performance using testClassifier() function.\ntestClassifiers([DummyClassifier,\n                 LogisticRegression,\n                 DecisionTreeClassifier,\n                 KNeighborsClassifier,\n                 GaussianNB,\n                 RandomForestClassifier,\n                 xgboost.XGBClassifier],\n                X_train, y_train, X_val, y_val)\n\n[14:07:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\n\n\n\n\n\n\n\nClassifier\nAccuracy\nRecall\nPrecision\nf1\n\n\n\n\n0\nDummyClassifier\n0.760774\n1.000000\n0.760774\n0.864136\n\n\n1\nLogisticRegression\n0.846658\n0.934876\n0.872645\n0.902689\n\n\n2\nDecisionTreeClassifier\n0.819019\n0.874058\n0.886463\n0.880217\n\n\n3\nKNeighborsClassifier\n0.833760\n0.913213\n0.873938\n0.893144\n\n\n4\nGaussianNB\n0.562903\n0.445102\n0.957730\n0.607753\n\n\n5\nRandomForestClassifier\n0.848091\n0.920344\n0.884635\n0.902137\n\n\n6\nXGBClassifier\n0.869792\n0.947390\n0.888791\n0.917155\n\n\n\n\n\n\n\nFirst, total accuracy is not a good metric here. That’s because the classes are imbalanced, and so a classifier that guesses could have accuracy much higher than 50% if it guesses that every instance belongs to the most common class. Therefore, I will use the harmonic mean (aka f1) as my metric because it is a good measure for comparing classifiers. Additionally, the dummy classifier acts randomly, so it should be the benchmark such that all other classifiers should perform better than it does. Based on the above output, the xgboost classifier is both more accurate and has a higher f1 score than any other classifier. It also beats the dummy classifier."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#stacked-model",
    "href": "content/projects/3-adult-income/AdultIncome.html#stacked-model",
    "title": "\nTable of Contents\n",
    "section": "Stacked Model",
    "text": "Stacked Model\nStacked models combine the outputs of a bunch of previously tested models in an attempt to improve overall fit. Here I’ll take all of the previous model’s outputs and see if that can achieve better results than the xgboost algorithm.\n\n# Stacked Models for simultanious stacking.\nclfs = [x() for x in [LogisticRegression,\n                      DecisionTreeClassifier,\n                      KNeighborsClassifier,\n                      GaussianNB,\n                      RandomForestClassifier,\n                      xgboost.XGBClassifier]]\n\n# Specify stacking classifier..\nstack = StackingClassifier(classifiers = clfs,meta_classifier = LogisticRegression())\n\n# Use 10-fold cross-validation.\nkfold = model_selection.KFold(n_splits = 10)\n\n# Get stacked model score.\ns = model_selection.cross_val_score(stack, X_train, y_train, scoring = \"roc_auc\", cv = kfold)\n\n# Print stacked model accuracy and standard deviation.\nprint(f\"{stack.__class__.__name__} \" f\"AUC: {s.mean():.3f} STD: {s.std():.2f}\")\n\n[14:09:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:09:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:10:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:10:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:11:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:11:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:12:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:13:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:13:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n[14:14:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nStackingClassifier AUC: 0.855 STD: 0.01\n\n\nI am only using accuracy as a metric here, but it does not seem to perform better than the xgboost classifier. Therefore, the xgboost classifier is what I will use."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#xgboost-base-model",
    "href": "content/projects/3-adult-income/AdultIncome.html#xgboost-base-model",
    "title": "\nTable of Contents\n",
    "section": "XGBoost Base Model",
    "text": "XGBoost Base Model\nFirst, I will refit the xgboost algorithm and test it using the validation set.\n\n# Create dataframe of classifier performance using testClassifier() function.\nxgboost_baseline = testClassifiers([xgboost.XGBClassifier], X_train, y_train, X_val, y_val)\n\n# Display output.\nxgboost_baseline\n\n[14:29:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\n\n\n\n\n\n\n\nClassifier\nAccuracy\nRecall\nPrecision\nf1\n\n\n\n\n0\nXGBClassifier\n0.869792\n0.94739\n0.888791\n0.917155\n\n\n\n\n\n\n\nThis is the baseline output of the xgboost classifier. On the validation set, it assigned 87% of samples to the correct label. Of all the &lt;=50K samples it encountered, it correctly classified 95% of them. Finally, of all the times it classified an observation a &lt;=50K, it was correct 89% of the time. Now, I will see whether I can enhance these metrics."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#xgboost-grid-search",
    "href": "content/projects/3-adult-income/AdultIncome.html#xgboost-grid-search",
    "title": "\nTable of Contents\n",
    "section": "XGBoost Grid Search",
    "text": "XGBoost Grid Search\nI want to run a grid search, but xgboost has a ton of parameters. In an ideal world, I could tune the model to every iteration of parameters, but here, it would take too long. Therefore I’m only going to focus on a few important parameters.\nBelow is a parameter dictionary with the parameters I want to tune:\n\n# Parameter dictionary. I will update these based on the best parameters found during grid search.\nparams = {\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'objective':'binary:logistic',\n}\n\n\n# Tuning 'max_depth' and 'min_child_weight'.\n\n# Set first grid search model.\nxgb_grid1 = xgboost.XGBClassifier()\n\n# Set parameter range for grid search.\nparams = {'max_depth': np.arange(1, 9, 1),\n          'min_child_weight': np.arange(1, 9, 1)}\n\n# Fit training data to all parameter combinations.\ncv = model_selection.GridSearchCV(xgb_grid1, params, n_jobs = -1).fit(X_train, y_train)\n\n# Print parameters that result in highest prediction accuracy.\nprint(cv.best_params_)\n\n[14:41:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n{'max_depth': 4, 'min_child_weight': 1}\n\n\n\n# Update parameter list with gridsearch's best parameters.\nparams['max_depth'] = 4\nparams['min_child_weight'] = 1\n\n\n# Start tuning 'subsample' and 'colsample_bytree'.\n\n# Set second grid search model.\nxgb_grid2 = xgboost.XGBClassifier()\n\n# Set parameter range for grid search.\nparams = {'subsample': np.arange(0.1, 0.9, 0.1),\n          'colsample_bytree': np.arange(0.1, 0.9, 0.1)}\n\n# Fit training data to all parameter combinations.\ncv = model_selection.GridSearchCV(xgb_grid1, params, n_jobs = -1).fit(X_train, y_train)\n\n# Print parameters that result in highest prediction accuracy.\nprint(cv.best_params_)\n\n[14:51:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n{'colsample_bytree': 0.2, 'subsample': 0.8}\n\n\n\n# Update parameter list with gridsearch's best parameters.\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.2\n\n\n# Start tuning 'eta'.\n\n# Set second grid search model.\nxgb_grid3 = xgboost.XGBClassifier()\n\n# Set parameter range for grid search.\nparams = {'eta': [.3, .2, .1, .05, .01, .005]}\n\n# Fit training data to all parameter combinations.\ncv = model_selection.GridSearchCV(xgb_grid3, params, n_jobs = -1).fit(X_train, y_train)\n\n# Print parameters that result in highest prediction accuracy.\nprint(cv.best_params_)\n\n[14:52:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n{'eta': 0.1}\n\n\n\n# Update parameter list with gridsearch's best parameter.\nparams['eta'] = 0.1\n\n\n# Plot validation set accuracy.\nxgb_class.score(X_test, y_test)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-72-0b1ae4b17fcb&gt; in &lt;module&gt;\n      1 # Plot validation set accuracy.\n----&gt; 2 xgb_class.score(X_test, y_test)\n\nNameError: name 'xgb_class' is not defined\n\n\n\n\nxgboost_mod = testxgboost([xgboost.XGBClassifier],\n                X_train, y_train, X_val, y_val)\n\n\nprint('xgboost baseline')\nprint(xgboost_baseline)\nprint('')\nprint('xgboost grid search')\nprint(xgboost_mod)\n\nConclusion: While the gridsearch identified different parameter values than those selected by the inital model fit, their combined effect does not enhance or detract the total accuracy.\n\n# Change xgboost handle.\nimport xgboost as xgb\n\n# Specify xgboost classifier.\nxgb_class = xgb.XGBClassifier(random_state = 42,\n                              eta = 0.2,\n                              ubsample = 0.8,\n                              colsample_bytree = 0.3,\n                              max_depth = 4,\n                              min_child_weight = 2,\n                             use_label_encoder = False)\n\n# Fit xgboost model.\nxgb_class.fit(X_train, y_train, early_stopping_rounds = 10, eval_set = [(X_val, y_val)], verbose = False)\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.3, eta=0.2, gamma=0,\n              gpu_id=-1, importance_type='gain', interaction_constraints='',\n              learning_rate=0.200000003, max_delta_step=0, max_depth=4,\n              min_child_weight=2, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', ubsample=0.8, use_label_encoder=False,\n              validate_parameters=1, verbosity=None)"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#validation-curve",
    "href": "content/projects/3-adult-income/AdultIncome.html#validation-curve",
    "title": "\nTable of Contents\n",
    "section": "Validation Curve",
    "text": "Validation Curve\nThe validation curve can help show how different parameter levels affect classification accuracy. As an example, I will look at one of the more important parameters “max_depth”.\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 4))\n\n# Validation curve specificaions.\nvc_viz = ValidationCurve(xgboost.XGBClassifier(verbosity = 0),\n                         param_name = 'max_depth',\n                         param_range = np.arange(1, 11),\n                         cv = 10,\n                         scoring = 'accuracy',\n                         n_jobs = -1)\n\n# Run validation curve.\nvc_viz.fit(X_test, y_test)\n\n# Plot validation curve.\nvc_viz.poof();\n\n# Save validation curve plot.\nfig.savefig('images/ValidationCurve.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion: The cross-validation score is what I want to maximize here. It looks like a max tree depth of 2 or 3 would be optimal, altohugh it doesn’t really change the overall model accuracy. Max depth was tuned to 4 by the xgboost I used, but that still looks good based on the curve."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#learning-curve",
    "href": "content/projects/3-adult-income/AdultIncome.html#learning-curve",
    "title": "\nTable of Contents\n",
    "section": "Learning Curve",
    "text": "Learning Curve\nThis can show a number of things. First, it can indicate whether more data should be collected, and it can be informative as to whether the model is overfitting or underfitting to the data.\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 4))\n\n# Learning curve specificaions.\nlc3_viz = LearningCurve(xgboost.XGBClassifier(n_estimators = 100), cv = 10)\n\n# Run learning curve.\nlc3_viz.fit(X_test, y_test)\n\n# Plot learning curve.\nlc3_viz.poof();\n\n# Save learning curve plot.\nfig.savefig('images/LearningCurve.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion: First, it looks like collecting more data would lead to very mediocre improvements to the cross-validation score: it has essentially plateued. There is very little variability in the training score (the “cloud” around the line) so I know the model is not biased (i.e. no underfitting). There is some variability in the cross-validation score, indicating that the model may have some variance (i.e. some overfitting). Regularisation can sometimes be applied to reduce overfitting, but here I think this model looks decent."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#confusion-matrix",
    "href": "content/projects/3-adult-income/AdultIncome.html#confusion-matrix",
    "title": "\nTable of Contents\n",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe confusion matrix will give me a lot of important information related to accuacy, recall, and precision.\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 6))\n\n# Confusion matrix specificaions.\ncm_viz = ConfusionMatrix(xgb_class, classes = ['&gt;50K', '&lt;=50K'], label_encoder = {0: '&gt;50K', 1: '&lt;=50K'})\n\n# Get confusion matrix score.\ncm_viz.score(X_test, y_test)\n\n# Plot confusion matrix.\ncm_viz.poof();\n\n# Save confusion matrix plot.\nfig.savefig('images/ConfusionMatrix.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion: Since I defined &lt;=50K as being the positive class, I will consider the bottom right corner to be true positives, and the top left corner to be true negatives.\n\n# Defining true positives (tp), false positives (fp), false negatives (fn), and true negatives (tn).\ntp = 7028\nfp = 854\nfn = 403\ntn = 1484"
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#accuracy",
    "href": "content/projects/3-adult-income/AdultIncome.html#accuracy",
    "title": "\nTable of Contents\n",
    "section": "Accuracy",
    "text": "Accuracy\nFirst, Let’s see, overall, how well the classifier correctly predicts positive and negative classes.\n\nManual Calculation\n\n# Manually calculate accuracy.\n(tp + tn)/(tp + fp + fn + tn)\n\n0.8713276691575391\n\n\n\n\nScikitLearn Calculation\n\n# Make predictions on test set.\ny_predict = xgb_class.predict(X_test)\n\n# Compute test set accuracy.\naccuracy_score(y_test, y_predict)\n\n0.8713276691575391\n\n\nConclusion: Our model can accurately identify ~88% of people above and below 50K. But, classes were unbalanced (i.e. their are far more people making &lt;=50K). Thus, one could acheive relatively high overall accuracy by just predicting every sample belongs to the class &lt;=50K. In this situation, it’s better to look at how well the xgboost algorithm predicts specific classes on a few metrics."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#recallsensitivity-true-positive-rate",
    "href": "content/projects/3-adult-income/AdultIncome.html#recallsensitivity-true-positive-rate",
    "title": "\nTable of Contents\n",
    "section": "Recall/Sensitivity (True Positive Rate)",
    "text": "Recall/Sensitivity (True Positive Rate)\nRecall shows how many positive cases the classifier actually identified as positive.\n\nManual Calculation\n\n# Manually calculate recall/sensitivity.\ntp/(tp + fn)\n\n0.9457677297806486\n\n\n\n\nScikitLearn Calculation\n\n# Make predictions on test set.\ny_predict = xgb_class.predict(X_test)\n\n# Compute test set recall/sensitivity.\nrecall_score(y_test, y_predict)\n\n0.9457677297806486\n\n\nConclusion: 95% of people making &lt;=50K were correctly identified."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#precision",
    "href": "content/projects/3-adult-income/AdultIncome.html#precision",
    "title": "\nTable of Contents\n",
    "section": "Precision",
    "text": "Precision\nPrecision shows how many positive predictions were actually correct.\n\nManual Calculation\n\n# Manually calculate precision.\ntp/(tp + fp)\n\n0.8916518650088809\n\n\n\n\nScikitLearn Calculation\n\n# Make predictions on test set.\ny_predict = xgb_class.predict(X_test)\n\n# Compute test set precision.\nprecision_score(y_test, y_predict)\n\n0.8916518650088809\n\n\nConclusion: Of all the times our classifier predicted someone made &lt;=50K, it was correct 89% of the time."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#f1-harmonic-mean",
    "href": "content/projects/3-adult-income/AdultIncome.html#f1-harmonic-mean",
    "title": "\nTable of Contents\n",
    "section": "F1 (Harmonic Mean)",
    "text": "F1 (Harmonic Mean)\nThe F1 score is a combination of recall and precision scores. Importantly, one shouldn’t use the f1 score to assess the model’s accuracy. Rather, the f1 score can be useful for comparing different classifier models.\n\nManual Calculation\n\n# Manually calculate f1 score.\n(2 * (tp / (tp + fp)) * (tp / (tp + fn))) / ((tp / (tp + fp)) + (tp / (tp + fn)))\n\n0.9179128844772415\n\n\n\n\nScikitLearn Calculation\n\n# Make predictions on test set.\ny_predict = xgb_class.predict(X_test)\n\n# Compute test set f1 score.\nf1_score(y_test, y_predict)\n\n0.9179128844772415\n\n\nConclusion: The harmonic mean is 92%, and is a weighted average between precision and recall."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#classification-report",
    "href": "content/projects/3-adult-income/AdultIncome.html#classification-report",
    "title": "\nTable of Contents\n",
    "section": "Classification Report",
    "text": "Classification Report\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize=(6, 3))\n\n# Classification report specificaions.\ncm_viz = ClassificationReport(xgb_class, classes = ['&gt;50K', '&lt;=50K'], label_encoder = {0: '&gt;50K', 1: '&lt;=50K'}, support = True)\n\n# Get classification report score.\ncm_viz.score(X_test, y_test)\n\n# Plot classification report.\ncm_viz.poof();\n\n# Save classification report plot.\nfig.savefig('images/ClassificationReport.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion:\nFor the positive class, I have already interpreted the results.\nFor the negative class, the clssifier seems much less competant overall. First, of all negative predictions it made, it was right 79% of the time (Precision). Second, of all the negative cases the classifier encountered, only 64% were classified correctly (Recall).\nIf I only cared about identifying people making &lt;=50K, this might be a good classifier. However, because I am tasked with identifying people making &lt;=50K, this classifier is doing what I want. Later I will demonstrate how to manually change the decision threshold to affect these values."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#roc",
    "href": "content/projects/3-adult-income/AdultIncome.html#roc",
    "title": "\nTable of Contents\n",
    "section": "ROC",
    "text": "ROC\nAn ROC illustrates how the classifier performs by tracking the true positive rate (recall/sensitivity) as the false positive rate (inverted specificity) changes. The plot should bulge to the left, indicating a good balance between true and false positives.\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 6))\n\n# Setup ROC with xgboost model.\nroc_viz = ROCAUC(xgb_class)\n\n# Fit ROC.\nroc_viz.fit(X_train, y_train) \n\n# Get ROC scores.\nroc_viz.score(X_test, y_test)\n\n# Plot ROC.\nroc_viz.poof();\n\n# Save ROC plot.\nfig.savefig('images/ROC.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion: The plot bulges left, so I have a good classifier. If I were unhappy with the balance betwen true and false positives, the decision threshold could be tweaked to change this curve."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#precision-recall-curve",
    "href": "content/projects/3-adult-income/AdultIncome.html#precision-recall-curve",
    "title": "\nTable of Contents\n",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\nThe ROC curve may be overly optimistic for imbalanced classes, something this dataset suffers from. Another option for evaluating classifiers is using a precision-recall curve. Classification is a balancing act of finding everything you need (recall) while limiting the junk results (precision). This is typically a trade-off. As recall goes up, precision usually goes down and vice versa.\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 4))\n\n# Setup precision-recall curve with xgboost model.\nviz = PrecisionRecallCurve(xgb_class)\n\n# Fit precision-recall curve.\nviz.fit(X_train, y_train)\n\n# Get precision-recall curve scores.\nviz.score(X_test, y_test)\n\n# plot precision-recall curve.\nviz.poof();\n\n# Save precision-recall curve plot.\nfig.savefig('images/PrecisionRecallCurve.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion: This can give me some indication about how precision and recall change as a function of one another. If I wanted to improve recall, I would need to lower precision (and vis versa)."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#cumulative-gains-plot",
    "href": "content/projects/3-adult-income/AdultIncome.html#cumulative-gains-plot",
    "title": "\nTable of Contents\n",
    "section": "Cumulative Gains Plot",
    "text": "Cumulative Gains Plot\nThis plot visualising the gain in true positives (sensitivity) for a given fraction of the total population targeted by the classifier. As I sample more of the dataset, I can see how it affects the true positive rate. If, for instance, I wanted to find 80% of the population that made &lt;=50K, I could trace the plot below from 0.8 on the y-axis to the blue line, and the corresponding x-axis would tell me the fraction of the populaton I might need to sample to find 80%.\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 6))\n\n# Setup cumulative gains plot with xgboost model.\ny_probas = xgb_class.predict_proba(X_test)\n\n# plot cumulative gains plot.\nscikitplot.metrics.plot_cumulative_gain(y_test, y_probas, ax = ax);\n\n# Save cumulative gains plot.\nfig.savefig('images/CumulativeGains.png', dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n\n\n\nConclusion: If there is a cost to contacting people, this may give an idea of how much the cost is. For instance, if I sampled 40% of the dataset, I would already have over 90% of people making &lt;=50K. If this was all I cared about, it may be useful to know that I won’t need to contact 10,000 people in the future. However, at 40% I also have less than 50% of people making &gt;50K. If I wanted to find more than 90% of them, I would potentially need to contact 10,000+ people."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#lift-curve",
    "href": "content/projects/3-adult-income/AdultIncome.html#lift-curve",
    "title": "\nTable of Contents\n",
    "section": "Lift Curve",
    "text": "Lift Curve\nLift curve shows the ratio between the proportion of true positive instances in the selection and the proportion of people sampled.\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (6, 6))\n\n# Setup lift curve with xgboost model.\ny_probas = xgb_class.predict_proba(X_test)\n\n# plot lift curve.\nscikitplot.metrics.plot_lift_curve(y_test, y_probas, ax = ax);\n\n# Save lift curve plot.\nfig.savefig('images/Lift.png', dpi = 300, bbox_inches = 'tight')\n\n\n\n\n\n\n\n\nConclusion: The dashed line (baseline) shows how a random classifier would perform. I can see that, if I only sampled 20% of the dataset, I would already find nearly 3.5 times more people making &lt;=50K than a random classifier would find. I see much less improvement regarding the classification of people making &gt;50K, but also no real decrease to lift asmore peaople are sampled."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#class-prediction-error",
    "href": "content/projects/3-adult-income/AdultIncome.html#class-prediction-error",
    "title": "\nTable of Contents\n",
    "section": "Class Prediction Error",
    "text": "Class Prediction Error\nThis plot will display the same information as the confusion matrix, but also gives a better sense of class balance (which is unbalanced in this dataset).\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (10, 8))\n\n# Add class labels.\ncpe_viz = ClassPredictionError(xgb_class, classes = ['&gt;50K', '&lt;=50K'])\n\n# Fit class prediction error plot.\ncpe_viz.score(X_test, y_test)\n\n# Plot class prediction error plot.\ncpe_viz.poof();\n\n# Save class prediction error plot.\nfig.savefig('images/ClassPredictionError.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion: The top right bar shows the the true positives, the bottom right shows the false negatives, the top left shows the flase positives, and the bottom left shows the true negatives. THese have already been discussed previously."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#discrimination-threshold",
    "href": "content/projects/3-adult-income/AdultIncome.html#discrimination-threshold",
    "title": "\nTable of Contents\n",
    "section": "Discrimination Threshold",
    "text": "Discrimination Threshold\nHere I will look at the tradeoff between precision and recall, and figure out how to shift the classifier’s decision boundary should I decide to improve one metric over the other.\nIf I wanted to identify every person who makes &lt;=50K, I would enhance recall over precision, as it would decrease the false negative rate. Subsequently, however, the false positive rate would go up too (because I would be biasing the classifier towards making positive predictions).\nConversely, if I only want to identify people who make &lt;=50K if they actually make &lt;=50K, I would instead enhance precision over recall in order to decrease the false positive rate. However, the false negative rate would also go up (because the classifier would now be biased towards making negative predictions).\n\n# Set plot layout.\nfig, ax = plt.subplots(figsize = (9, 9));\n\n# Setup discrimination threshold plot with xgboost model.\ndt_viz = DiscriminationThreshold(xgb_class);\n\n# Fit discrimination threshold plot.\ndt_viz.fit(X_test, y_test);\n\n# Plot discrimination threshold plot.\ndt_viz.poof();\n\n# Save discrimination threshold plot.\nfig.savefig('images/DiscriminationThreshold.png', dpi = 300)\n\n\n\n\n\n\n\n\nConclusion:In terms of predicting people making &lt;=50K, the model has good overall precision and recall. However, I can tune the model such that one of these metrics is favored over the other. Below, I will do just that."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#identify-everyone-making-50k",
    "href": "content/projects/3-adult-income/AdultIncome.html#identify-everyone-making-50k",
    "title": "\nTable of Contents\n",
    "section": "Identify Everyone Making <=50K",
    "text": "Identify Everyone Making &lt;=50K\nIf the goal is to simply maximize the identification people making &lt;=50K, recall should be increased.\n\n# Lower decision threshold to 0.25.\ndiscrimination_threshold = 0.25\n\n# Get predictions for confusion matrix.\npredictions = xgb_class.predict_proba(X_test)\n\n# Adjust confusion matrix to account for new decision threshold.\npredictions = (predictions[::,1] &gt; discrimination_threshold ) * 1\n\n# Print: precision, recall, accuracy, and f1 score.\nprint('The precision score is: %.2f' % precision_score( y_test, predictions))\nprint('The recall score is: %.2f' % recall_score( y_test, predictions), \"\\n\")\nprint('Accuracy score is: %.2f' % accuracy_score( y_test, predictions))\nprint('The F1 score is: %.2f' % f1_score( y_test, predictions))\n\n# Fit model for confusion matrix.\ncm = confusion_matrix( y_test , predictions)\n\n# Set plot layout.\nplt.figure(figsize = (3, 3))\n\n# Set confusion matrix specifications.\nsns.heatmap(cm, annot = True, annot_kws = {'size': 25}, fmt = 'd', cmap = 'viridis', cbar = False)\n\n# Plot confusion matrix.\nplt.show()\n\nThe precision score is: 0.83\nThe recall score is: 0.99 \n\nAccuracy score is: 0.84\nThe F1 score is: 0.90\n\n\n\n\n\n\n\n\n\nConclusion: The confusion matrix shows that almost everyone who makes &lt;=50K is correctly identified. However, many more people who make &gt;50K are now classified as making &lt;=50K. If the goal was to find everyone who make &lt;=50K, this might be a better model than the one currently being used."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#correctly-identify-everyone-making-50k",
    "href": "content/projects/3-adult-income/AdultIncome.html#correctly-identify-everyone-making-50k",
    "title": "\nTable of Contents\n",
    "section": "Correctly Identify Everyone Making <=50K",
    "text": "Correctly Identify Everyone Making &lt;=50K\nIf I don’t want incorrectly identify people making &lt;=50K, precision should be increased.\n\n# Raise decision threshold to 0.95.\ndiscrimination_threshold = 0.95\n\n# Get predictions for confusion matrix.\npredictions = xgb_class.predict_proba(X_test)\n\n# Adjust confusion matrix to account for new decision threshold.\npredictions = (predictions[::, 1] &gt; discrimination_threshold ) * 1\n\n# Print: precision, recall, accuracy, and f1 score.\nprint('The precision score is: %.2f' % precision_score( y_test, predictions))\nprint('The recall score is: %.2f' % recall_score( y_test, predictions))\nprint('Accuracy score is: %.2f' % accuracy_score( y_test, predictions))\nprint('The F1 score is: %.2f' % f1_score( y_test, predictions))\n\n# Fit model for confusion matrix.\ncm = confusion_matrix( y_test , predictions)\n\n# Set plot layout.\nplt.figure(figsize = (3, 3))\n\n# Set confusion matrix specifications.\nsns.heatmap(cm, annot = True, annot_kws = {'size': 25}, fmt = 'd', cmap = 'viridis', cbar = False)\n\n# Plot confusion matrix.\nplt.show()\n\nThe precision score is: 0.99\nThe recall score is: 0.54\nAccuracy score is: 0.65\nThe F1 score is: 0.70\n\n\n\n\n\n\n\n\n\nConclusion: Virtually every person predicted to make &lt;=50K actually made &lt;=50K. However, there are now many more false negatives. But, if I want to make sure every person I identify as making &lt;=50K is actually in that class, this might be a better model."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#shapley-additive-explanations-shap",
    "href": "content/projects/3-adult-income/AdultIncome.html#shapley-additive-explanations-shap",
    "title": "\nTable of Contents\n",
    "section": "Shapley Additive Explanations (SHAP)",
    "text": "Shapley Additive Explanations (SHAP)\nA SHAP plot shows how features each contribute to pushing a model output from the base value (the average model output over the training dataset I passed) to the model output. Features pushing the prediction higher are shown in red, while those pushing the prediction lower are in blue. As an example, I will look at participant 4 (who was predicted to make &lt;=50K).\n\n# Explain the xgboost model's predictions using SHAP.\nexplainer = shap.Explainer(xgb_class)\n\n# Fit SHAP model.\nshap_values = explainer(X_test)\n\n# visualize a single prediction's prediction's explanation\nshap.plots.waterfall(shap_values[3])\n\n\n\n\n\n\n\n\nConclusion: Factors such as the person being young, working less than 40 hours per week, and having a low status job all contribute greatly to the person being predicted to make &lt;=50K. Interestingly, because the individual is male, this was the only contributing factor to them being pushed towards a prediction of &gt;50K."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#force-plot",
    "href": "content/projects/3-adult-income/AdultIncome.html#force-plot",
    "title": "\nTable of Contents\n",
    "section": "Force Plot",
    "text": "Force Plot\nThe above graph can also be more concisly displayed using a force plot, which plots the same information as above, but takes less visual space.\n\n&lt;=50K Force Plot\n\n# Set javascript display.\n#shap.initjs()\n\n# Assign force plot to xgboost model.\ns = shap.TreeExplainer(xgb_class)\n\n# Fit force plot.\nshap_vals = s.shap_values(X_test)\n\n# Plot force plot.\nshap.force_plot(s.expected_value, shap_vals[3, :], feature_names = X_test.columns, matplotlib=True)\n\n\n\n\n\n\n\n\n\n# Print predicted probability for [&gt;50K, &lt;=50K].\nprint(xgb_class.predict_proba(X_test.iloc[[3]]))\n\n[[6.300807e-04 9.993699e-01]]\n\n\nThis person was classified as making &lt;=50K. Again, I can see that their age, hours worked per week, and their job, all contributed the most to them being predicted as making &lt;=50K. Additionally, the predicted probability of them making &lt;=50K is 99.9%.\n\n\n&gt;50K Force Plot\n\n# Set javascript display.\n#shap.initjs()\n\n# Plot force plot.\nshap.force_plot(s.expected_value, shap_vals[7, :], feature_names = X_test.columns,  matplotlib=True)\n\n\n\n\n\n\n\n\n\n# Plot class prediction interval for force plot observation.\nxgb_class.predict_proba(X_test.iloc[[7]])\n\narray([[0.9964505 , 0.00354952]], dtype=float32)\n\n\nThis person was predicted to make &gt;50K. I can see that their capital_diff score, education level, and hours worked per week, all contributed the most at pushing them towards a high probability of making &gt;50K.\n\n# visualize a single prediction's prediction's explanation\nshap.plots.waterfall(shap_values[7])\n\n\n\n\n\n\n\n\nIndeed, this person has a large negative captial_diff score (meaning they made money), and they work almost 1 standard deviation above the average.\nConclusion: Force plots a great way to determine why someone might be an outlier, and if their are interesting followups based on group differences. For example, is there something about people with only a preschool education that uniquely contributes to &lt;=50K being a common prediction that does not include education level? These won’t be followed up on here, but they certainly could be interesting avenues to explore."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#bee-swarm-plot",
    "href": "content/projects/3-adult-income/AdultIncome.html#bee-swarm-plot",
    "title": "\nTable of Contents\n",
    "section": "Bee Swarm Plot",
    "text": "Bee Swarm Plot\nTo get an overview of which features are most important for the model’s predictions, I can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output.\n\n# summarize the effects of all the features.\nshap.plots.beeswarm(shap_values)\n\n\n\n\n\n\n\n\nConclusion: There are some very clear and interesting (albeit obvious) patterns. For instance, almost everyone who is married is predicted to make &gt;50K, and almost everyone who is not married is predicted to make &lt;=50K. For capital_diff, there is no clear amount that helps predict loses, and only when people see capital gains exceeding 4 standard deviations from the mean are they guarenteed to be classified as making &gt;50K. There is also a clear divide amongst those working more or less than 40 hours per week, with those working more making more, and those working less making less. Finally, having a bachelors degree seems to almost guarantee the person will be classified as making &gt;50K."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#shap-bar-plot",
    "href": "content/projects/3-adult-income/AdultIncome.html#shap-bar-plot",
    "title": "\nTable of Contents\n",
    "section": "SHAP Bar Plot",
    "text": "SHAP Bar Plot\nI can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot.\n\n# Summarise overall effects for most important features.\nshap.plots.bar(shap_values)\n\n\n\n\n\n\n\n\nConclusion: Here I can see the most important features determining what class someone will be predicted to be in. If I wanted to send out some surveys that targeted people making &lt;=50K, I might want to focus more strongly on those who are younger, unmarried, high school dropouts, and are female. If information like education was more easy to obtain, I could instead find people who did not graduate high school, or those who did but did not attend university."
  },
  {
    "objectID": "content/projects/3-adult-income/AdultIncome.html#shap-dot-plot",
    "href": "content/projects/3-adult-income/AdultIncome.html#shap-dot-plot",
    "title": "\nTable of Contents\n",
    "section": "SHAP Dot Plot",
    "text": "SHAP Dot Plot\nTo understand how a single feature affects the output of the model I can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Vertical dispersion at a single value of a predictor could represent interaction effects with other features. To help reveal these interactions I can color by another feature. If I pass the whole explanation tensor to the color argument the scatter plot will pick the best feature to color by. I will pick a few of the more interesting ones I have found below.\n\nSHAP Dot Plot: gender_Male\n\n#Plot 2 predictors against each other.\nshap.plots.scatter(shap_values[:, 'gender_Male'], color = shap_values)\n\n\n\n\n\n\n\n\nHere, it is clear that males are more likely to make &gt;50K than females. however, if either sex has a child, a very strong interaction is observed. Females are more likely to make &gt;50K if they have a child, but males are more likely to make &lt;=50K if they have a child. Thus, having a child affects earnings very differently for either sex. One possibility is child support payments being more likely to be made if you are male, and more likely to be recieved if you are female. If I wanted to target people by sex, it might be useful to contact females without children, or males with children.\n\n\nSHAP Dot Plot: gender_Male\n\n#Plot 2 predictors against each other.\nshap.plots.scatter(shap_values[:, 'hours_per_week'], color = shap_values)\n\n\n\n\n\n\n\n\nThis one is a bit trickier to interpret, but I can try. First, there is a clear divide at 40 hours per week (o on the plot),where people working more than 40 hours making more, and people working less making less. However, if someone has a spouse, this causes an interaction. People working less than 40 hours who have a spouse are more likely to make &gt;50K than those without a spouse, and people working more than 40 hours are less likely to make &gt;50K if they have a spouse. THis second interaction holds no matter how many standard deviations someone is above 40 hours."
  }
]